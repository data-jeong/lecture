# í”„ë¡œì íŠ¸ 6: ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Web Scraping)

ë‹¤ì–‘í•œ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬(Selenium, requests, curl, Scrapy, httpx)ë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

- ë‹¤ì–‘í•œ ì›¹ ìŠ¤í¬ë˜í•‘ ë„êµ¬ì˜ íŠ¹ì§•ê³¼ ì‚¬ìš©ë²•
- ë™ì /ì •ì  ì›¹ í˜ì´ì§€ í¬ë¡¤ë§
- ë°˜í¬ë¡¤ë§ íšŒí”¼ ê¸°ë²•
- ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„¤ê³„
- ë°ì´í„° ì¶”ì¶œ ë° ì •ì œ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
06_web_scraping/
â”œâ”€â”€ news_crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_crawler.py      # í¬ë¡¤ëŸ¬ ê¸°ë³¸ í´ë˜ìŠ¤
â”‚   â”‚   â”œâ”€â”€ requests_crawler.py  # requests ê¸°ë°˜ í¬ë¡¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ selenium_crawler.py  # Selenium ê¸°ë°˜ í¬ë¡¤ëŸ¬
â”‚   â”‚   â”œâ”€â”€ scrapy_crawler.py    # Scrapy ê¸°ë°˜ í¬ë¡¤ëŸ¬
â”‚   â”‚   â””â”€â”€ httpx_crawler.py     # httpx ë¹„ë™ê¸° í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parser.py            # HTML íŒŒì‹± ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py     # í”„ë¡ì‹œ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ user_agent.py        # User-Agent ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ anti_bot.py          # ë°˜í¬ë¡¤ë§ íšŒí”¼
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ news_extractor.py    # ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ
â”‚   â”‚   â”œâ”€â”€ comment_extractor.py # ëŒ“ê¸€ ì¶”ì¶œ
â”‚   â”‚   â””â”€â”€ metadata_extractor.py # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_storage.py      # íŒŒì¼ ì €ì¥
â”‚   â”‚   â”œâ”€â”€ database.py          # DB ì €ì¥
â”‚   â”‚   â””â”€â”€ cache.py             # ìºì‹±
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sentiment.py         # ê°ì • ë¶„ì„
â”‚   â”‚   â”œâ”€â”€ keyword.py           # í‚¤ì›Œë“œ ì¶”ì¶œ
â”‚   â”‚   â””â”€â”€ trend.py             # íŠ¸ë Œë“œ ë¶„ì„
â”‚   â”œâ”€â”€ main.py                  # CLI ì¸í„°í˜ì´ìŠ¤
â”‚   â””â”€â”€ examples.py              # ì‚¬ìš© ì˜ˆì œ
â”œâ”€â”€ scrapy_project/             # Scrapy í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ scrapy.cfg
â”‚   â””â”€â”€ news_spider/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ items.py
â”‚       â”œâ”€â”€ pipelines.py
â”‚       â”œâ”€â”€ settings.py
â”‚       â””â”€â”€ spiders/
â”‚           â””â”€â”€ news.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_crawlers.py
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â””â”€â”€ test_analyzers.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### ì„¤ì¹˜
```bash
cd 06_web_scraping
pip install -r requirements.txt

# Scrapy í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
cd scrapy_project
scrapy crawl news
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

#### 1. Requests í¬ë¡¤ëŸ¬
```python
from news_crawler.core import RequestsCrawler

crawler = RequestsCrawler()
articles = crawler.crawl_news("https://news.ycombinator.com")
```

#### 2. Selenium í¬ë¡¤ëŸ¬ (ë™ì  í˜ì´ì§€)
```python
from news_crawler.core import SeleniumCrawler

crawler = SeleniumCrawler(headless=True)
articles = crawler.crawl_news("https://dynamic-news-site.com")
```

#### 3. Scrapy í¬ë¡¤ëŸ¬ (ëŒ€ê·œëª¨)
```bash
cd scrapy_project
scrapy crawl news -a start_urls=https://example.com
```

#### 4. HTTPX ë¹„ë™ê¸° í¬ë¡¤ëŸ¬
```python
import asyncio
from news_crawler.core import HttpxCrawler

async def main():
    crawler = HttpxCrawler()
    articles = await crawler.crawl_multiple_async(urls)

asyncio.run(main())
```

## ğŸ“š ì£¼ìš” ê¸°ëŠ¥

### 1. ë‹¤ì–‘í•œ í¬ë¡¤ë§ ë°©ì‹

#### Requests + BeautifulSoup
```python
# ì •ì  í˜ì´ì§€ì— ìµœì 
response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')
```

#### Selenium WebDriver
```python
# JavaScript ë Œë”ë§ í˜ì´ì§€
driver = webdriver.Chrome()
driver.get(url)
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "article"))
)
```

#### Scrapy Framework
```python
# ëŒ€ê·œëª¨ í¬ë¡¤ë§
class NewsSpider(scrapy.Spider):
    name = 'news'
    
    def parse(self, response):
        for article in response.css('article'):
            yield {
                'title': article.css('h2::text').get(),
                'content': article.css('.content::text').getall()
            }
```

#### HTTPX ë¹„ë™ê¸°
```python
# ê³ ì„±ëŠ¥ ë¹„ë™ê¸° í¬ë¡¤ë§
async with httpx.AsyncClient() as client:
    tasks = [client.get(url) for url in urls]
    responses = await asyncio.gather(*tasks)
```

### 2. ë°˜í¬ë¡¤ë§ íšŒí”¼

- User-Agent ë¡œí…Œì´ì…˜
- í”„ë¡ì‹œ ì„œë²„ ì‚¬ìš©
- ìš”ì²­ ê°„ê²© ì¡°ì ˆ
- ì¿ í‚¤/ì„¸ì…˜ ê´€ë¦¬
- JavaScript ì‹¤í–‰

### 3. ë°ì´í„° ì¶”ì¶œ ë° ë¶„ì„

- ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
- ë©”íƒ€ë°ì´í„° íŒŒì‹±
- ëŒ“ê¸€ ìˆ˜ì§‘
- ê°ì • ë¶„ì„
- í‚¤ì›Œë“œ ì¶”ì¶œ
- íŠ¸ë Œë“œ ë¶„ì„

## ğŸ”§ CLI ì‚¬ìš©ë²•

```bash
# ë‹¨ì¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
python -m news_crawler.main crawl --url https://example.com --type requests

# ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ í¬ë¡¤ë§
python -m news_crawler.main crawl-multiple --urls-file sites.txt --concurrent 5

# ë™ì  í˜ì´ì§€ í¬ë¡¤ë§
python -m news_crawler.main crawl --url https://spa-site.com --type selenium

# ë¶„ì„ ì‹¤í–‰
python -m news_crawler.main analyze --input crawled_data.json --sentiment --keywords

# ëª¨ë‹ˆí„°ë§ ëª¨ë“œ
python -m news_crawler.main monitor --site https://news.com --interval 3600
```

## ğŸ“Š í¬ë¡¤ëŸ¬ ë¹„êµ

| ë„êµ¬ | ì¥ì  | ë‹¨ì  | ì í•©í•œ ìƒí™© |
|------|------|------|------------|
| Requests | ë¹ ë¥´ê³  ê°„ë‹¨ | JS ë Œë”ë§ ë¶ˆê°€ | ì •ì  í˜ì´ì§€ |
| Selenium | JS ì™„ë²½ ì§€ì› | ëŠë¦¼, ë¦¬ì†ŒìŠ¤ ë§ì´ ì‚¬ìš© | ë™ì  í˜ì´ì§€ |
| Scrapy | ëŒ€ê·œëª¨ í¬ë¡¤ë§ ìµœì í™” | í•™ìŠµ ê³¡ì„  | ëŒ€ëŸ‰ ë°ì´í„° |
| HTTPX | ë¹„ë™ê¸°, ë¹ ë¦„ | JS ë¯¸ì§€ì› | ë‹¤ìˆ˜ í˜ì´ì§€ ë™ì‹œ |

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest

# íŠ¹ì • í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
pytest tests/test_crawlers.py::TestRequestsCrawler -v

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest --cov=news_crawler
```

## ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸

1. **í¬ë¡¤ëŸ¬ ì„ íƒ**: ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ë„êµ¬ ì„ íƒ
2. **ìœ¤ë¦¬ì  í¬ë¡¤ë§**: robots.txt ì¤€ìˆ˜, ì„œë²„ ë¶€í•˜ ê³ ë ¤
3. **ì—ëŸ¬ ì²˜ë¦¬**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, íŒŒì‹± ì˜¤ë¥˜ ëŒ€ì‘
4. **í™•ì¥ì„±**: ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì„¤ê³„

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

- í”„ë¡œì íŠ¸ 7: API ê¸°ì´ˆ (ë‚ ì”¨ API í´ë¼ì´ì–¸íŠ¸)
- í”„ë¡œì íŠ¸ 8: ë°ì´í„°ë² ì´ìŠ¤ (í•™ìƒ ì„±ì  ê´€ë¦¬)
- í”„ë¡œì íŠ¸ 9: FastAPI ë°±ì—”ë“œ