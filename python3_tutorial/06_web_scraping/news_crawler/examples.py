#!/usr/bin/env python3
"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‚¬ìš© ì˜ˆì œ
ë‹¤ì–‘í•œ í¬ë¡¤ë§ ì‹œë‚˜ë¦¬ì˜¤ ì‹œì—°
"""

import asyncio
import time
from pathlib import Path

from core import (
    RequestsCrawler,
    SeleniumCrawler,
    HttpxCrawler
)
from extractors import NewsExtractor, CommentExtractor
from utils import UserAgentManager, ProxyManager
from storage import FileStorage


async def example_basic_crawling():
    """ê¸°ë³¸ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("ğŸ•·ï¸  ê¸°ë³¸ í¬ë¡¤ë§ ì˜ˆì œ")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ URLë“¤
    test_urls = [
        "https://httpbin.org/html",
        "https://example.com",
    ]
    
    # 1. Requests í¬ë¡¤ëŸ¬
    print("\n1. Requests í¬ë¡¤ëŸ¬")
    crawler = RequestsCrawler(delay=0.5)
    
    for url in test_urls:
        print(f"  í¬ë¡¤ë§: {url}")
        result = crawler.crawl(url)
        
        if result.success:
            print(f"    âœ… ì„±ê³µ - ì œëª©: {result.title}")
            print(f"    ğŸ“„ ì»¨í…ì¸  í¬ê¸°: {len(result.content or '')} ê¸€ì")
        else:
            print(f"    âŒ ì‹¤íŒ¨: {result.error}")
    
    crawler.close()
    
    # 2. HTTPX ë¹„ë™ê¸° í¬ë¡¤ëŸ¬
    print("\n2. HTTPX ë¹„ë™ê¸° í¬ë¡¤ëŸ¬")
    httpx_crawler = HttpxCrawler()
    
    # ë™ì‹œ í¬ë¡¤ë§
    start_time = time.time()
    results = await httpx_crawler.crawl_multiple_async(test_urls)
    duration = time.time() - start_time
    
    print(f"  âš¡ {len(results)}ê°œ í˜ì´ì§€ë¥¼ {duration:.2f}ì´ˆì— ì™„ë£Œ")
    
    for result in results:
        if result.success:
            print(f"    âœ… {result.url}: {result.title}")
        else:
            print(f"    âŒ {result.url}: {result.error}")


async def example_news_extraction():
    """ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ ì˜ˆì œ"""
    print("\n\nğŸ“° ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ ì˜ˆì œ")
    print("=" * 60)
    
    # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URL (ì‹¤ì œë¡œëŠ” í—ˆê°€ëœ ì‚¬ì´íŠ¸ë§Œ ì‚¬ìš©)
    news_urls = [
        "https://httpbin.org/html",  # í…ŒìŠ¤íŠ¸ìš©
    ]
    
    crawler = RequestsCrawler()
    extractor = NewsExtractor()
    
    for url in news_urls:
        print(f"\nğŸ“– ë¶„ì„ ì¤‘: {url}")
        
        # í¬ë¡¤ë§
        result = crawler.crawl(url)
        
        if result.success:
            # ë‰´ìŠ¤ ë°ì´í„° ì¶”ì¶œ
            news_data = extractor.extract(result.raw_html, url)
            
            print(f"  ì œëª©: {news_data.get('title', 'N/A')}")
            print(f"  ì‘ì„±ì: {news_data.get('author', 'N/A')}")
            print(f"  ë‚ ì§œ: {news_data.get('published_date', 'N/A')}")
            print(f"  ì¹´í…Œê³ ë¦¬: {news_data.get('category', 'N/A')}")
            print(f"  ë‹¨ì–´ ìˆ˜: {news_data.get('word_count', 0)}")
            print(f"  ì˜ˆìƒ ì½ê¸° ì‹œê°„: {news_data.get('reading_time', 0)}ë¶„")
            print(f"  ì–¸ì–´: {news_data.get('language', 'N/A')}")
            print(f"  ì´ë¯¸ì§€ ìˆ˜: {len(news_data.get('images', []))}")
            print(f"  ê´€ë ¨ ê¸°ì‚¬: {len(news_data.get('related_articles', []))}")
    
    crawler.close()


def example_user_agent_rotation():
    """User-Agent ë¡œí…Œì´ì…˜ ì˜ˆì œ"""
    print("\n\nğŸ­ User-Agent ë¡œí…Œì´ì…˜ ì˜ˆì œ")
    print("=" * 60)
    
    ua_manager = UserAgentManager()
    
    # ë‹¤ì–‘í•œ User-Agent ìƒì„±
    print("ìƒì„±ëœ User-Agentë“¤:")
    for i in range(5):
        ua = ua_manager.get_random_user_agent()
        print(f"  {i+1}. {ua}")
    
    # ë¸Œë¼ìš°ì €ë³„ User-Agent
    print("\në¸Œë¼ìš°ì €ë³„ User-Agent:")
    browsers = ['chrome', 'firefox', 'safari', 'edge']
    
    for browser in browsers:
        ua = ua_manager.get_random_user_agent(browser=browser)
        print(f"  {browser.capitalize()}: {ua}")
    
    # ëª¨ë°”ì¼ User-Agent
    print("\nëª¨ë°”ì¼ User-Agent:")
    mobile_ua = ua_manager.get_random_user_agent(mobile=True)
    print(f"  Mobile: {mobile_ua}")


async def example_rate_limiting():
    """ì†ë„ ì œí•œ ì˜ˆì œ"""
    print("\n\nğŸš¦ ì†ë„ ì œí•œ ì˜ˆì œ")
    print("=" * 60)
    
    from utils.rate_limiter import AsyncRateLimiter
    
    # ì†ë„ ì œí•œê¸° ì„¤ì • (2 requests/second)
    rate_limiter = AsyncRateLimiter(rate=2, per=1.0)
    
    # ì œí•œëœ í¬ë¡¤ë§ í•¨ìˆ˜
    @rate_limiter
    async def limited_crawl(url):
        print(f"  ğŸ• {time.strftime('%H:%M:%S')} - í¬ë¡¤ë§: {url}")
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
        await asyncio.sleep(0.1)
        return f"Result from {url}"
    
    # 5ê°œ ìš”ì²­ ì‹¤í–‰
    urls = [f"https://example.com/page/{i}" for i in range(5)]
    
    print("ì†ë„ ì œí•œ ì ìš© (2 requests/second):")
    start_time = time.time()
    
    tasks = [limited_crawl(url) for url in urls]
    results = await asyncio.gather(*tasks)
    
    duration = time.time() - start_time
    print(f"\nâ±ï¸  ì´ ì‹œê°„: {duration:.2f}ì´ˆ (ì˜ˆìƒ: ~2ì´ˆ)")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ìš”ì²­: {len(results)}ê°œ")


def example_proxy_management():
    """í”„ë¡ì‹œ ê´€ë¦¬ ì˜ˆì œ"""
    print("\n\nğŸŒ í”„ë¡ì‹œ ê´€ë¦¬ ì˜ˆì œ")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© í”„ë¡ì‹œ ëª©ë¡ (ì‹¤ì œë¡œëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    test_proxies = [
        "http://proxy1.example.com:8080",
        "http://proxy2.example.com:8080",
        "socks5://proxy3.example.com:1080",
    ]
    
    proxy_manager = ProxyManager(test_proxies)
    
    print(f"ë“±ë¡ëœ í”„ë¡ì‹œ: {len(proxy_manager.proxies)}ê°œ")
    
    # í”„ë¡ì‹œ ì„ íƒ
    for i in range(3):
        proxy = proxy_manager.get_proxy()
        if proxy:
            print(f"  {i+1}. ì„ íƒëœ í”„ë¡ì‹œ: {proxy.address}")
        else:
            print(f"  {i+1}. ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ì—†ìŒ")
    
    # í†µê³„ ì¶œë ¥
    stats = proxy_manager.get_statistics()
    print(f"\nğŸ“Š í”„ë¡ì‹œ í†µê³„:")
    print(f"  ì´ í”„ë¡ì‹œ: {stats['total_proxies']}")
    print(f"  ì‚¬ìš© ê°€ëŠ¥: {stats['available_proxies']}")
    print(f"  ë¸”ë™ë¦¬ìŠ¤íŠ¸: {stats['blacklisted_proxies']}")


async def example_batch_crawling():
    """ë°°ì¹˜ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n\nğŸ“¦ ë°°ì¹˜ í¬ë¡¤ë§ ì˜ˆì œ")
    print("=" * 60)
    
    # URL ëª©ë¡
    urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml",
        "https://example.com",
    ]
    
    # ë¹„ë™ê¸° ë°°ì¹˜ í¬ë¡¤ë§
    crawler = HttpxCrawler(max_connections=2)
    
    print(f"ğŸš€ {len(urls)}ê°œ URL ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
    start_time = time.time()
    
    results = await crawler.crawl_multiple_async(urls)
    
    duration = time.time() - start_time
    successful = sum(1 for r in results if r.success)
    
    print(f"\nâœ… ì™„ë£Œ:")
    print(f"  ì„±ê³µ: {successful}/{len(results)}")
    print(f"  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    print(f"  í‰ê·  ì†ë„: {len(results)/duration:.2f} requests/sec")
    
    # ê²°ê³¼ ìƒì„¸
    for result in results:
        status = "âœ…" if result.success else "âŒ"
        print(f"  {status} {result.url}")


def example_data_storage():
    """ë°ì´í„° ì €ì¥ ì˜ˆì œ"""
    print("\n\nğŸ’¾ ë°ì´í„° ì €ì¥ ì˜ˆì œ")
    print("=" * 60)
    
    from core.base_crawler import CrawlResult
    from datetime import datetime
    
    # ì €ì¥ì†Œ ì´ˆê¸°í™”
    storage = FileStorage("example_data")
    
    # ê°€ìƒ í¬ë¡¤ë§ ê²°ê³¼ ìƒì„±
    sample_results = []
    for i in range(3):
        result = CrawlResult(
            url=f"https://example.com/article/{i+1}",
            title=f"í…ŒìŠ¤íŠ¸ ê¸°ì‚¬ {i+1}",
            content=f"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ê¸°ì‚¬ {i+1}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. " * 20,
            author=f"ê¸°ì{i+1}",
            published_date=datetime.now(),
            success=True
        )
        sample_results.append(result)
    
    # ê°œë³„ ì €ì¥
    print("ê°œë³„ ê¸°ì‚¬ ì €ì¥:")
    for result in sample_results:
        filepath = storage.save_article(result)
        print(f"  ì €ì¥ë¨: {Path(filepath).name}")
    
    # ë°°ì¹˜ ì €ì¥
    batch_file = storage.save_batch(sample_results)
    print(f"\në°°ì¹˜ íŒŒì¼ ì €ì¥: {Path(batch_file).name}")
    
    # CSV ë‚´ë³´ë‚´ê¸°
    articles_data = [r.to_dict() for r in sample_results]
    csv_file = storage.export_to_csv(articles_data, "sample_articles.csv")
    print(f"CSV ë‚´ë³´ë‚´ê¸°: {Path(csv_file).name}")
    
    # ì €ì¥ì†Œ í†µê³„
    stats = storage.get_statistics()
    print(f"\nğŸ“Š ì €ì¥ì†Œ í†µê³„:")
    print(f"  ê¸°ì‚¬ ìˆ˜: {stats['articles_count']}")
    print(f"  ì´ ìš©ëŸ‰: {stats['total_size_mb']:.2f} MB")


async def example_selenium_crawling():
    """Selenium í¬ë¡¤ë§ ì˜ˆì œ (JavaScript í˜ì´ì§€)"""
    print("\n\nğŸ–¥ï¸  Selenium í¬ë¡¤ë§ ì˜ˆì œ")
    print("=" * 60)
    
    # Selenium í¬ë¡¤ëŸ¬ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
    try:
        crawler = SeleniumCrawler(headless=True)
        
        # JavaScriptê°€ ë§ì€ í˜ì´ì§€ (SPA ë“±)
        spa_url = "https://httpbin.org/html"  # í…ŒìŠ¤íŠ¸ìš©
        
        print(f"JavaScript í˜ì´ì§€ í¬ë¡¤ë§: {spa_url}")
        result = crawler.crawl(spa_url)
        
        if result.success:
            print(f"  âœ… ì„±ê³µ")
            print(f"  ğŸ“„ ì œëª©: {result.title}")
            print(f"  ğŸ“Š ì»¨í…ì¸  í¬ê¸°: {len(result.content or '')} ê¸€ì")
        else:
            print(f"  âŒ ì‹¤íŒ¨: {result.error}")
        
        crawler.close()
        
    except Exception as e:
        print(f"  âš ï¸  Selenium í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("  (Chrome ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")


async def main():
    """ëª¨ë“  ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸš€ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì˜ˆì œ ì‹œì—°")
    print("=" * 80)
    
    examples = [
        example_basic_crawling,
        example_news_extraction,
        example_user_agent_rotation,
        example_rate_limiting,
        example_proxy_management,
        example_batch_crawling,
        example_data_storage,
        example_selenium_crawling,
    ]
    
    for i, example in enumerate(examples, 1):
        try:
            if asyncio.iscoroutinefunction(example):
                await example()
            else:
                example()
        except Exception as e:
            print(f"\nâŒ ì˜ˆì œ {i} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if i < len(examples):
            print("\n" + "-" * 80)
            await asyncio.sleep(1)  # ì˜ˆì œ ê°„ ì ì‹œ ëŒ€ê¸°
    
    print("\n\nâœ¨ ëª¨ë“  ì˜ˆì œ ì™„ë£Œ!")
    print("\nğŸ’¡ ë” ë§ì€ ì‚¬ìš©ë²•:")
    print("  - python -m news_crawler.main --help")
    print("  - python -m news_crawler.main crawl https://example.com")
    print("  - python -m news_crawler.main crawl-multiple urls.txt")


if __name__ == "__main__":
    asyncio.run(main())