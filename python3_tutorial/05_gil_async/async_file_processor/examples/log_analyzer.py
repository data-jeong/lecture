"""
ë¡œê·¸ ë¶„ì„ ì˜ˆì œ
ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ì˜ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
"""

import asyncio
import time
import random
from pathlib import Path
from typing import Dict, List, AsyncIterator, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json
import re
from collections import defaultdict, Counter

from ..core.async_processor import AsyncProcessor
from ..patterns.batch_processor import AsyncBatchProcessor, AdaptiveBatchProcessor
from ..patterns.producer_consumer import AsyncProducerConsumer
from ..utils.benchmark import measure_time_async


@dataclass
class LogEntry:
    """ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: datetime
    level: str  # DEBUG, INFO, WARNING, ERROR, CRITICAL
    source: str
    message: str
    ip: Optional[str] = None
    user_id: Optional[int] = None
    response_time: Optional[float] = None
    status_code: Optional[int] = None
    
    def to_dict(self) -> dict:
        return {
            "timestamp": self.timestamp.isoformat(),
            "level": self.level,
            "source": self.source,
            "message": self.message,
            "ip": self.ip,
            "user_id": self.user_id,
            "response_time": self.response_time,
            "status_code": self.status_code
        }


@dataclass
class LogAnalysisResult:
    """ë¡œê·¸ ë¶„ì„ ê²°ê³¼"""
    total_entries: int = 0
    level_counts: Dict[str, int] = field(default_factory=dict)
    error_messages: List[str] = field(default_factory=list)
    average_response_time: float = 0
    status_code_distribution: Dict[int, int] = field(default_factory=dict)
    top_ips: List[tuple] = field(default_factory=list)
    hourly_distribution: Dict[int, int] = field(default_factory=dict)
    anomalies: List[dict] = field(default_factory=list)


class LogAnalyzerExample:
    """ë¡œê·¸ ë¶„ì„ ì˜ˆì œ"""
    
    def __init__(self):
        self.log_pattern = re.compile(
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(\w+)\] \[([^\]]+)\] (.+)'
        )
        self.ip_pattern = re.compile(r'\d+\.\d+\.\d+\.\d+')
        self.response_time_pattern = re.compile(r'response_time=(\d+\.?\d*)ms')
        self.status_pattern = re.compile(r'status=(\d+)')
        self.user_pattern = re.compile(r'user_id=(\d+)')
    
    async def generate_sample_logs(self, output_file: Path, num_entries: int = 10000):
        """ìƒ˜í”Œ ë¡œê·¸ íŒŒì¼ ìƒì„±"""
        print(f"ğŸ“ {num_entries:,}ê°œ ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„± ì¤‘...")
        
        levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        sources = ["web", "api", "database", "auth", "payment"]
        ips = [f"192.168.1.{i}" for i in range(1, 21)]
        
        start_time = datetime.now() - timedelta(hours=24)
        
        with open(output_file, 'w') as f:
            for i in range(num_entries):
                # ì‹œê°„ ì¦ê°€
                timestamp = start_time + timedelta(seconds=i * 86400 / num_entries)
                
                # ë ˆë²¨ ì„ íƒ (ê°€ì¤‘ì¹˜)
                level = random.choices(
                    levels,
                    weights=[10, 60, 20, 8, 2],
                    k=1
                )[0]
                
                source = random.choice(sources)
                ip = random.choice(ips)
                user_id = random.randint(1000, 9999) if random.random() > 0.3 else None
                
                # ë©”ì‹œì§€ ìƒì„±
                if source == "web":
                    status = random.choices(
                        [200, 201, 400, 404, 500],
                        weights=[70, 10, 10, 8, 2],
                        k=1
                    )[0]
                    response_time = random.gauss(50, 20) if status < 400 else random.gauss(200, 50)
                    response_time = max(1, response_time)
                    
                    message = f"Request from {ip}"
                    if user_id:
                        message += f" user_id={user_id}"
                    message += f" status={status} response_time={response_time:.1f}ms"
                    
                elif source == "database":
                    query_time = random.gauss(10, 5)
                    message = f"Query executed in {query_time:.2f}ms"
                    
                elif source == "auth":
                    action = random.choice(["login", "logout", "token_refresh"])
                    message = f"User {action}"
                    if user_id:
                        message += f" user_id={user_id}"
                    message += f" from {ip}"
                    
                else:
                    message = f"Operation completed from {ip}"
                
                # ì—ëŸ¬ ë©”ì‹œì§€ ì¶”ê°€
                if level in ["ERROR", "CRITICAL"]:
                    errors = [
                        "Connection timeout",
                        "Database connection failed",
                        "Authentication failed",
                        "Internal server error",
                        "Memory allocation error"
                    ]
                    message += f" - {random.choice(errors)}"
                
                # ë¡œê·¸ ë¼ì¸ ì‘ì„±
                log_line = f"{timestamp.strftime('%Y-%m-%d %H:%M:%S')} [{level}] [{source}] {message}\n"
                f.write(log_line)
        
        print(f"âœ… ë¡œê·¸ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_file}")
    
    def parse_log_entry(self, line: str) -> Optional[LogEntry]:
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        match = self.log_pattern.match(line.strip())
        if not match:
            return None
        
        timestamp_str, level, source, message = match.groups()
        timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
        
        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        ip_match = self.ip_pattern.search(message)
        ip = ip_match.group() if ip_match else None
        
        response_time_match = self.response_time_pattern.search(message)
        response_time = float(response_time_match.group(1)) if response_time_match else None
        
        status_match = self.status_pattern.search(message)
        status_code = int(status_match.group(1)) if status_match else None
        
        user_match = self.user_pattern.search(message)
        user_id = int(user_match.group(1)) if user_match else None
        
        return LogEntry(
            timestamp=timestamp,
            level=level,
            source=source,
            message=message,
            ip=ip,
            user_id=user_id,
            response_time=response_time,
            status_code=status_code
        )
    
    async def analyze_log_batch(self, entries: List[LogEntry]) -> LogAnalysisResult:
        """ë¡œê·¸ ë°°ì¹˜ ë¶„ì„"""
        result = LogAnalysisResult()
        
        result.total_entries = len(entries)
        
        # ë ˆë²¨ë³„ ì¹´ìš´íŠ¸
        level_counter = Counter(entry.level for entry in entries)
        result.level_counts = dict(level_counter)
        
        # ì—ëŸ¬ ë©”ì‹œì§€ ìˆ˜ì§‘
        for entry in entries:
            if entry.level in ["ERROR", "CRITICAL"]:
                result.error_messages.append(entry.message)
        
        # ì‘ë‹µ ì‹œê°„ í‰ê· 
        response_times = [e.response_time for e in entries if e.response_time]
        if response_times:
            result.average_response_time = sum(response_times) / len(response_times)
        
        # ìƒíƒœ ì½”ë“œ ë¶„í¬
        status_codes = [e.status_code for e in entries if e.status_code]
        result.status_code_distribution = dict(Counter(status_codes))
        
        # ìƒìœ„ IP
        ip_counter = Counter(e.ip for e in entries if e.ip)
        result.top_ips = ip_counter.most_common(5)
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        hour_counter = Counter(e.timestamp.hour for e in entries)
        result.hourly_distribution = dict(hour_counter)
        
        # ì´ìƒ íƒì§€ (ëŠë¦° ì‘ë‹µ)
        for entry in entries:
            if entry.response_time and entry.response_time > 500:
                result.anomalies.append({
                    "type": "slow_response",
                    "timestamp": entry.timestamp.isoformat(),
                    "response_time": entry.response_time,
                    "message": entry.message
                })
        
        return result
    
    async def stream_analyze_logs(self, log_file: Path) -> LogAnalysisResult:
        """ë¡œê·¸ íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ë¶„ì„"""
        print("\nğŸ”„ ë¡œê·¸ íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ë¶„ì„ ì‹œì‘...")
        
        # ì „ì²´ ê²°ê³¼ ì§‘ê³„
        total_result = LogAnalysisResult()
        
        # ë°°ì¹˜ ì²˜ë¦¬ê¸°
        batch_processor = AdaptiveBatchProcessor[LogEntry, LogAnalysisResult](
            initial_batch_size=500,
            min_batch_size=100,
            max_batch_size=2000,
            target_duration=0.5
        )
        
        # ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´
        pc = AsyncProducerConsumer[str, LogEntry](
            max_queue_size=1000,
            num_consumers=4
        )
        
        # ë¡œê·¸ ë¼ì¸ ìƒì„±ê¸°
        async def log_line_generator():
            with open(log_file, 'r') as f:
                for line in f:
                    yield line
                    # ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
                    if random.random() < 0.001:  # 0.1% í™•ë¥ ë¡œ ì§€ì—°
                        await asyncio.sleep(0.01)
        
        # ë¡œê·¸ íŒŒì‹± í•¨ìˆ˜
        def parse_line(line: str) -> Optional[LogEntry]:
            return self.parse_log_entry(line)
        
        # íŒŒì‹± ì‹¤í–‰
        print("  1ï¸âƒ£ ë¡œê·¸ íŒŒì‹± ì¤‘...")
        parse_results = await pc.run(
            source=log_line_generator(),
            processor=parse_line
        )
        
        # íŒŒì‹±ëœ ì—”íŠ¸ë¦¬ ì¶”ì¶œ
        entries = []
        for result in parse_results:
            if "result" in result and result["result"]:
                entries.append(result["result"])
        
        print(f"  âœ… {len(entries):,}ê°œ ì—”íŠ¸ë¦¬ íŒŒì‹± ì™„ë£Œ")
        
        # ë°°ì¹˜ ë¶„ì„
        print("  2ï¸âƒ£ ë°°ì¹˜ ë¶„ì„ ì¤‘...")
        
        @measure_time_async
        async def analyze_entries():
            batch_results = await batch_processor.add_many(entries, self.analyze_log_batch)
            return batch_results
        
        batch_results, analysis_time = await analyze_entries()
        
        print(f"  âœ… {len(batch_results)}ê°œ ë°°ì¹˜ë¡œ ë¶„ì„ ì™„ë£Œ ({analysis_time:.2f}ì´ˆ)")
        
        # ê²°ê³¼ ì§‘ê³„
        for batch_result in batch_results:
            if batch_result.results:
                for result in batch_result.results:
                    # ê²°ê³¼ ë³‘í•©
                    total_result.total_entries += result.total_entries
                    
                    for level, count in result.level_counts.items():
                        total_result.level_counts[level] = \
                            total_result.level_counts.get(level, 0) + count
                    
                    total_result.error_messages.extend(result.error_messages)
                    
                    # ì‘ë‹µ ì‹œê°„ í‰ê·  ì¬ê³„ì‚°ì€ ë‹¨ìˆœí™”
                    if result.average_response_time > 0:
                        total_result.average_response_time = result.average_response_time
                    
                    for code, count in result.status_code_distribution.items():
                        total_result.status_code_distribution[code] = \
                            total_result.status_code_distribution.get(code, 0) + count
                    
                    for hour, count in result.hourly_distribution.items():
                        total_result.hourly_distribution[hour] = \
                            total_result.hourly_distribution.get(hour, 0) + count
                    
                    total_result.anomalies.extend(result.anomalies)
        
        # ìƒìœ„ IP ì¬ê³„ì‚°
        all_ips = []
        for entry in entries:
            if entry.ip:
                all_ips.append(entry.ip)
        ip_counter = Counter(all_ips)
        total_result.top_ips = ip_counter.most_common(10)
        
        # ë°°ì¹˜ ì²˜ë¦¬ í†µê³„
        batch_stats = batch_processor.get_statistics()
        print("\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í†µê³„:")
        for key, value in batch_stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        
        return total_result
    
    def print_analysis_report(self, result: LogAnalysisResult):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\nğŸ“Š ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        print(f"\nì´ ë¡œê·¸ ì—”íŠ¸ë¦¬: {result.total_entries:,}ê°œ")
        
        print("\në ˆë²¨ë³„ ë¶„í¬:")
        for level in ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]:
            count = result.level_counts.get(level, 0)
            percentage = count / result.total_entries * 100 if result.total_entries > 0 else 0
            bar = "â–ˆ" * int(percentage / 2)
            print(f"  {level:8} {count:6,} ({percentage:5.1f}%) {bar}")
        
        print(f"\ní‰ê·  ì‘ë‹µ ì‹œê°„: {result.average_response_time:.2f}ms")
        
        print("\nìƒíƒœ ì½”ë“œ ë¶„í¬:")
        for code, count in sorted(result.status_code_distribution.items()):
            percentage = count / sum(result.status_code_distribution.values()) * 100
            print(f"  {code}: {count:,} ({percentage:.1f}%)")
        
        print("\nìƒìœ„ IP ì£¼ì†Œ:")
        for ip, count in result.top_ips[:5]:
            print(f"  {ip}: {count:,}íšŒ")
        
        print("\nì‹œê°„ëŒ€ë³„ í™œë™:")
        for hour in range(24):
            count = result.hourly_distribution.get(hour, 0)
            bar = "â–ˆ" * int(count / max(result.hourly_distribution.values()) * 20)
            print(f"  {hour:02d}ì‹œ: {bar} {count:,}")
        
        print(f"\nâš ï¸  ê°ì§€ëœ ì´ìƒ: {len(result.anomalies)}ê°œ")
        if result.anomalies:
            print("  ìµœê·¼ 5ê°œ:")
            for anomaly in result.anomalies[:5]:
                print(f"    - [{anomaly['type']}] {anomaly['timestamp']} "
                      f"ì‘ë‹µì‹œê°„: {anomaly['response_time']}ms")
    
    async def run(self):
        """ì˜ˆì œ ì‹¤í–‰"""
        print("ğŸ“‹ ë¡œê·¸ ë¶„ì„ ì˜ˆì œ")
        print("=" * 60)
        
        # 1. ìƒ˜í”Œ ë¡œê·¸ ìƒì„±
        log_file = Path("sample_logs.log")
        await self.generate_sample_logs(log_file, num_entries=50000)
        
        # 2. ìŠ¤íŠ¸ë¦¼ ë¶„ì„
        result = await self.stream_analyze_logs(log_file)
        
        # 3. ë¦¬í¬íŠ¸ ì¶œë ¥
        self.print_analysis_report(result)
        
        # 4. ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥...")
        
        report_data = {
            "analysis_timestamp": datetime.now().isoformat(),
            "total_entries": result.total_entries,
            "level_distribution": result.level_counts,
            "average_response_time": result.average_response_time,
            "status_code_distribution": result.status_code_distribution,
            "top_ips": result.top_ips,
            "hourly_distribution": result.hourly_distribution,
            "anomaly_count": len(result.anomalies),
            "sample_anomalies": result.anomalies[:10]
        }
        
        with open("log_analysis_report.json", 'w') as f:
            json.dump(report_data, f, indent=2)
        
        print("  âœ… log_analysis_report.json ì €ì¥ ì™„ë£Œ")
        
        # ì—ëŸ¬ ë¡œê·¸ ë³„ë„ ì €ì¥
        if result.error_messages:
            with open("error_logs.txt", 'w') as f:
                for error in result.error_messages[:100]:  # ìµœëŒ€ 100ê°œ
                    f.write(error + '\n')
            print("  âœ… error_logs.txt ì €ì¥ ì™„ë£Œ")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    example = LogAnalyzerExample()
    await example.run()


if __name__ == "__main__":
    asyncio.run(main())