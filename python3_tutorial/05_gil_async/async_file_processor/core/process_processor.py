"""
í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬ê¸°
multiprocessingì„ í™œìš©í•œ ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬
"""

import multiprocessing as mp
import concurrent.futures
from pathlib import Path
from typing import List, Dict, Optional, Callable, Any, Tuple
from dataclasses import dataclass
import time
import hashlib
import json
import pickle
import os
from functools import partial


@dataclass
class ProcessResult:
    """í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬ ê²°ê³¼"""
    process_id: int
    file_path: str
    success: bool
    result: Any = None
    error: Optional[str] = None
    duration: float = 0.0


def process_file_worker(args: Tuple[str, bytes]) -> ProcessResult:
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜"""
    file_path, processor_bytes = args
    process_id = os.getpid()
    start_time = time.perf_counter()
    
    try:
        # í”„ë¡œì„¸ì„œ ì—­ì§ë ¬í™”
        processor = pickle.loads(processor_bytes)
        
        # íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ì²˜ë¦¬
        result = processor(content)
        
        duration = time.perf_counter() - start_time
        
        return ProcessResult(
            process_id=process_id,
            file_path=file_path,
            success=True,
            result=result,
            duration=duration
        )
        
    except Exception as e:
        duration = time.perf_counter() - start_time
        return ProcessResult(
            process_id=process_id,
            file_path=file_path,
            success=False,
            error=str(e),
            duration=duration
        )


class ProcessProcessor:
    """í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.results: List[ProcessResult] = []
        self.manager = mp.Manager()
    
    def process_directory_pool(
        self,
        directory: str,
        pattern: str = "*",
        processor: Callable[[str], Any] = None,
        recursive: bool = True,
        chunksize: int = 1
    ) -> List[ProcessResult]:
        """í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë””ë ‰í† ë¦¬ ì²˜ë¦¬"""
        path = Path(directory)
        
        if recursive:
            files = list(path.rglob(pattern))
        else:
            files = list(path.glob(pattern))
        
        # íŒŒì¼ë§Œ í•„í„°ë§
        files = [f for f in files if f.is_file()]
        print(f"ğŸ“ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        if processor is None:
            processor = self._default_processor
        
        # í”„ë¡œì„¸ì„œ ì§ë ¬í™”
        processor_bytes = pickle.dumps(processor)
        
        # ì‘ì—… ì¤€ë¹„
        tasks = [(str(f), processor_bytes) for f in files]
        
        results = []
        
        # í”„ë¡œì„¸ìŠ¤ í’€ ì‚¬ìš©
        with mp.Pool(processes=self.max_workers) as pool:
            # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ë¹„ë™ê¸° ì²˜ë¦¬
            async_results = pool.map_async(
                process_file_worker, 
                tasks, 
                chunksize=chunksize
            )
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            while not async_results.ready():
                time.sleep(0.1)
            
            results = async_results.get()
        
        self.results.extend(results)
        
        # ê²°ê³¼ ìš”ì•½
        success_count = sum(1 for r in results if r.success)
        print(f"\nâœ… ì™„ë£Œ: {success_count}/{len(results)} ì„±ê³µ")
        
        return results
    
    def process_directory_executor(
        self,
        directory: str,
        pattern: str = "*",
        processor: Callable[[str], Any] = None,
        recursive: bool = True
    ) -> List[ProcessResult]:
        """ProcessPoolExecutorë¥¼ ì‚¬ìš©í•œ ë””ë ‰í† ë¦¬ ì²˜ë¦¬"""
        path = Path(directory)
        
        if recursive:
            files = list(path.rglob(pattern))
        else:
            files = list(path.glob(pattern))
        
        # íŒŒì¼ë§Œ í•„í„°ë§
        files = [f for f in files if f.is_file()]
        print(f"ğŸ“ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        if processor is None:
            processor = self._default_processor
        
        # í”„ë¡œì„¸ì„œ ì§ë ¬í™”
        processor_bytes = pickle.dumps(processor)
        
        results = []
        
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            futures = {
                executor.submit(process_file_worker, (str(f), processor_bytes)): f
                for f in files
            }
            
            # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                completed += 1
                result = future.result()
                results.append(result)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                success_count = sum(1 for r in results if r.success)
                print(f"\rì§„í–‰: {completed}/{len(files)} | "
                      f"ì„±ê³µ: {success_count} | "
                      f"ì‹¤íŒ¨: {completed - success_count}", end='')
        
        print()  # ì¤„ë°”ê¿ˆ
        self.results.extend(results)
        return results
    
    def _default_processor(self, content: str) -> dict:
        """ê¸°ë³¸ ì²˜ë¦¬ê¸°"""
        return {
            "size": len(content),
            "lines": content.count('\n'),
            "words": len(content.split()),
            "hash": hashlib.md5(content.encode()).hexdigest()
        }
    
    def parallel_map(
        self,
        func: Callable[[Any], Any],
        items: List[Any],
        chunksize: int = 1
    ) -> List[Any]:
        """í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ë³‘ë ¬ map"""
        with mp.Pool(processes=self.max_workers) as pool:
            return pool.map(func, items, chunksize=chunksize)
    
    def parallel_starmap(
        self,
        func: Callable[..., Any],
        items: List[Tuple],
        chunksize: int = 1
    ) -> List[Any]:
        """í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ë³‘ë ¬ starmap (ì—¬ëŸ¬ ì¸ì)"""
        with mp.Pool(processes=self.max_workers) as pool:
            return pool.starmap(func, items, chunksize=chunksize)
    
    def producer_consumer_pattern(
        self,
        producer_func: Callable[[], Any],
        consumer_func: Callable[[Any], Any],
        num_consumers: int = None,
        max_queue_size: int = 100
    ) -> List[Any]:
        """í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´"""
        if num_consumers is None:
            num_consumers = self.max_workers
        
        # ê³µìœ  í
        task_queue = self.manager.Queue(maxsize=max_queue_size)
        result_queue = self.manager.Queue()
        
        def producer_process():
            """ìƒì‚°ì í”„ë¡œì„¸ìŠ¤"""
            try:
                while True:
                    item = producer_func()
                    if item is None:
                        break
                    task_queue.put(item)
            finally:
                # ì¢…ë£Œ ì‹ í˜¸
                for _ in range(num_consumers):
                    task_queue.put(None)
        
        def consumer_process():
            """ì†Œë¹„ì í”„ë¡œì„¸ìŠ¤"""
            while True:
                try:
                    item = task_queue.get(timeout=1)
                    if item is None:
                        break
                    
                    result = consumer_func(item)
                    result_queue.put(result)
                except Exception as e:
                    result_queue.put({"error": str(e)})
        
        # í”„ë¡œì„¸ìŠ¤ ì‹œì‘
        producer = mp.Process(target=producer_process)
        consumers = [
            mp.Process(target=consumer_process)
            for _ in range(num_consumers)
        ]
        
        producer.start()
        for c in consumers:
            c.start()
        
        # ì™„ë£Œ ëŒ€ê¸°
        producer.join()
        for c in consumers:
            c.join()
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        while not result_queue.empty():
            results.append(result_queue.get())
        
        return results
    
    def map_reduce(
        self,
        map_func: Callable[[Any], Any],
        reduce_func: Callable[[Any, Any], Any],
        items: List[Any],
        initial_value: Any = None
    ) -> Any:
        """ë§µ-ë¦¬ë“€ìŠ¤ íŒ¨í„´"""
        # Map ë‹¨ê³„
        with mp.Pool(processes=self.max_workers) as pool:
            mapped_results = pool.map(map_func, items)
        
        # Reduce ë‹¨ê³„
        if initial_value is not None:
            result = initial_value
        else:
            result = mapped_results[0]
            mapped_results = mapped_results[1:]
        
        for item in mapped_results:
            result = reduce_func(result, item)
        
        return result
    
    def batch_process(
        self,
        items: List[Any],
        processor: Callable[[List[Any]], Any],
        batch_size: int = 100
    ) -> List[Any]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        batches = [
            items[i:i + batch_size]
            for i in range(0, len(items), batch_size)
        ]
        
        with mp.Pool(processes=self.max_workers) as pool:
            batch_results = pool.map(processor, batches)
        
        # ê²°ê³¼ í‰íƒ„í™”
        results = []
        for batch_result in batch_results:
            if isinstance(batch_result, list):
                results.extend(batch_result)
            else:
                results.append(batch_result)
        
        return results
    
    def get_statistics(self) -> dict:
        """ì²˜ë¦¬ í†µê³„"""
        if not self.results:
            return {"message": "No results available"}
        
        success_count = sum(1 for r in self.results if r.success)
        failed_count = len(self.results) - success_count
        total_duration = sum(r.duration for r in self.results)
        
        # í”„ë¡œì„¸ìŠ¤ë³„ í†µê³„
        process_stats = {}
        for result in self.results:
            pid = result.process_id
            if pid not in process_stats:
                process_stats[pid] = {"count": 0, "duration": 0}
            process_stats[pid]["count"] += 1
            process_stats[pid]["duration"] += result.duration
        
        return {
            "total_files": len(self.results),
            "successful": success_count,
            "failed": failed_count,
            "total_duration": f"{total_duration:.2f}s",
            "average_duration": f"{total_duration / len(self.results):.2f}s",
            "throughput": f"{len(self.results) / total_duration:.2f} files/s",
            "processes_used": len(process_stats),
            "max_workers": self.max_workers,
            "process_statistics": process_stats
        }
    
    def save_results(self, output_file: str = "process_results.json"):
        """ê²°ê³¼ ì €ì¥"""
        data = {
            "statistics": self.get_statistics(),
            "results": [
                {
                    "process_id": r.process_id,
                    "file_path": r.file_path,
                    "success": r.success,
                    "result": str(r.result) if r.result else None,
                    "error": r.error,
                    "duration": r.duration
                }
                for r in self.results
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def cpu_intensive_processor(content: str) -> dict:
    """CPU ì§‘ì•½ì  ì²˜ë¦¬ ì˜ˆì œ"""
    # í•´ì‹œ ê³„ì‚° (CPU ì§‘ì•½ì )
    hash_md5 = hashlib.md5(content.encode()).hexdigest()
    hash_sha256 = hashlib.sha256(content.encode()).hexdigest()
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    words = content.lower().split()
    word_freq = {}
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ 10ê°œ
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "size": len(content),
        "lines": content.count('\n'),
        "words": len(words),
        "unique_words": len(word_freq),
        "hash_md5": hash_md5,
        "hash_sha256": hash_sha256,
        "top_words": top_words
    }


def example_usage():
    """ì‚¬ìš© ì˜ˆì œ"""
    print("ğŸš€ í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬ê¸° ì˜ˆì œ")
    print("=" * 60)
    
    processor = ProcessProcessor(max_workers=4)
    
    # 1. í”„ë¡œì„¸ìŠ¤ í’€ ë°©ì‹
    print(f"\n1. ProcessPool ë°©ì‹ (ì›Œì»¤: {processor.max_workers}):")
    results = processor.process_directory_pool(
        directory=".",
        pattern="*.py",
        processor=cpu_intensive_processor,
        recursive=False,
        chunksize=2
    )
    
    # 2. í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    stats = processor.get_statistics()
    for key, value in stats.items():
        if key != "process_statistics":
            print(f"  {key}: {value}")
    
    # 3. ë§µ-ë¦¬ë“€ìŠ¤ ì˜ˆì œ
    print("\n2. ë§µ-ë¦¬ë“€ìŠ¤ ì˜ˆì œ:")
    
    # ìˆ«ì ë¦¬ìŠ¤íŠ¸ì˜ ì œê³± í•© ê³„ì‚°
    numbers = list(range(1, 101))
    
    def square(x):
        return x ** 2
    
    def add(x, y):
        return x + y
    
    result = processor.map_reduce(
        map_func=square,
        reduce_func=add,
        items=numbers,
        initial_value=0
    )
    
    print(f"1ë¶€í„° 100ê¹Œì§€ì˜ ì œê³± í•©: {result}")
    
    # 4. ê²°ê³¼ ì €ì¥
    processor.save_results()


if __name__ == "__main__":
    # Windowsì—ì„œ í•„ìš”
    mp.freeze_support()
    example_usage()