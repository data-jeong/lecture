"""
ë°°ì¹˜ ì²˜ë¦¬ íŒ¨í„´
ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ íŒ¨í„´
"""

import asyncio
import time
from typing import List, Any, Callable, Optional, TypeVar, Generic, Union
from dataclasses import dataclass, field
from collections import defaultdict
import logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor


T = TypeVar('T')
R = TypeVar('R')


@dataclass
class BatchResult(Generic[T, R]):
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼"""
    batch_id: int
    items: List[T]
    results: Optional[List[R]] = None
    error: Optional[str] = None
    start_time: float = 0
    end_time: float = 0
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time
    
    @property
    def success(self) -> bool:
        return self.error is None
    
    @property
    def items_per_second(self) -> float:
        if self.duration > 0:
            return len(self.items) / self.duration
        return 0


class BatchProcessor(Generic[T, R]):
    """ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(
        self,
        batch_size: int = 100,
        max_workers: Optional[int] = None,
        use_processes: bool = False
    ):
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.use_processes = use_processes
        self.results: List[BatchResult[T, R]] = []
        self._batch_counter = 0
    
    def process(
        self,
        items: List[T],
        processor: Callable[[List[T]], List[R]],
        parallel: bool = True
    ) -> List[BatchResult[T, R]]:
        """ì•„ì´í…œë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        batches = self._create_batches(items)
        results = []
        
        if parallel and len(batches) > 1:
            # ë³‘ë ¬ ì²˜ë¦¬
            Executor = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
            
            with Executor(max_workers=self.max_workers) as executor:
                futures = []
                
                for batch in batches:
                    self._batch_counter += 1
                    batch_result = BatchResult(
                        batch_id=self._batch_counter,
                        items=batch,
                        start_time=time.time()
                    )
                    
                    future = executor.submit(self._process_batch, batch, processor)
                    futures.append((future, batch_result))
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future, batch_result in futures:
                    try:
                        batch_result.results = future.result()
                        batch_result.end_time = time.time()
                    except Exception as e:
                        batch_result.error = str(e)
                        batch_result.end_time = time.time()
                    
                    results.append(batch_result)
                    self.results.append(batch_result)
        else:
            # ìˆœì°¨ ì²˜ë¦¬
            for batch in batches:
                self._batch_counter += 1
                batch_result = self._process_batch_with_result(batch, processor)
                results.append(batch_result)
                self.results.append(batch_result)
        
        return results
    
    def _create_batches(self, items: List[T]) -> List[List[T]]:
        """ì•„ì´í…œë“¤ì„ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            batches.append(batch)
        return batches
    
    def _process_batch(self, batch: List[T], processor: Callable[[List[T]], List[R]]) -> List[R]:
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        return processor(batch)
    
    def _process_batch_with_result(
        self,
        batch: List[T],
        processor: Callable[[List[T]], List[R]]
    ) -> BatchResult[T, R]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë° ê²°ê³¼ ìƒì„±"""
        batch_result = BatchResult(
            batch_id=self._batch_counter,
            items=batch,
            start_time=time.time()
        )
        
        try:
            batch_result.results = processor(batch)
        except Exception as e:
            batch_result.error = str(e)
        finally:
            batch_result.end_time = time.time()
        
        return batch_result
    
    def get_statistics(self) -> dict:
        """ì²˜ë¦¬ í†µê³„"""
        if not self.results:
            return {}
        
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        total_items = sum(len(r.items) for r in self.results)
        total_duration = sum(r.duration for r in self.results)
        
        return {
            "total_batches": len(self.results),
            "successful_batches": len(successful),
            "failed_batches": len(failed),
            "total_items": total_items,
            "total_duration": total_duration,
            "average_batch_duration": total_duration / len(self.results),
            "items_per_second": total_items / total_duration if total_duration > 0 else 0,
            "batch_size": self.batch_size
        }


class AsyncBatchProcessor(Generic[T, R]):
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(
        self,
        batch_size: int = 100,
        max_concurrent_batches: int = 10,
        batch_timeout: Optional[float] = None,
        auto_flush_interval: Optional[float] = None
    ):
        self.batch_size = batch_size
        self.max_concurrent_batches = max_concurrent_batches
        self.batch_timeout = batch_timeout
        self.auto_flush_interval = auto_flush_interval
        
        self.current_batch: List[T] = []
        self.batch_lock = asyncio.Lock()
        self.semaphore = asyncio.Semaphore(max_concurrent_batches)
        self.results: List[BatchResult[T, R]] = []
        self._batch_counter = 0
        self._auto_flush_task: Optional[asyncio.Task] = None
    
    async def add(self, item: T, processor: Callable[[List[T]], List[R]]) -> Optional[BatchResult[T, R]]:
        """ì•„ì´í…œ ì¶”ê°€ (ë°°ì¹˜ê°€ ê°€ë“ ì°¨ë©´ ìë™ ì²˜ë¦¬)"""
        async with self.batch_lock:
            self.current_batch.append(item)
            
            if len(self.current_batch) >= self.batch_size:
                return await self._flush_batch(processor)
        
        return None
    
    async def add_many(
        self,
        items: List[T],
        processor: Callable[[List[T]], List[R]]
    ) -> List[BatchResult[T, R]]:
        """ì—¬ëŸ¬ ì•„ì´í…œ ì¶”ê°€"""
        results = []
        
        for item in items:
            result = await self.add(item, processor)
            if result:
                results.append(result)
        
        # ë‚¨ì€ ì•„ì´í…œ ì²˜ë¦¬
        if self.current_batch:
            result = await self._flush_batch(processor)
            if result:
                results.append(result)
        
        return results
    
    async def _flush_batch(self, processor: Callable[[List[T]], List[R]]) -> Optional[BatchResult[T, R]]:
        """í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬"""
        if not self.current_batch:
            return None
        
        batch = self.current_batch.copy()
        self.current_batch.clear()
        self._batch_counter += 1
        
        # ë°°ì¹˜ ì²˜ë¦¬
        async with self.semaphore:
            batch_result = await self._process_batch_async(batch, processor)
        
        self.results.append(batch_result)
        return batch_result
    
    async def _process_batch_async(
        self,
        batch: List[T],
        processor: Callable[[List[T]], List[R]]
    ) -> BatchResult[T, R]:
        """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬"""
        batch_result = BatchResult(
            batch_id=self._batch_counter,
            items=batch,
            start_time=time.time()
        )
        
        try:
            if asyncio.iscoroutinefunction(processor):
                batch_result.results = await processor(batch)
            else:
                # ë™ê¸° í•¨ìˆ˜ëŠ” executorì—ì„œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                batch_result.results = await loop.run_in_executor(
                    None, processor, batch
                )
            
            if self.batch_timeout:
                # íƒ€ì„ì•„ì›ƒ ì ìš©
                batch_result.results = await asyncio.wait_for(
                    processor(batch) if asyncio.iscoroutinefunction(processor) 
                    else loop.run_in_executor(None, processor, batch),
                    timeout=self.batch_timeout
                )
        
        except asyncio.TimeoutError:
            batch_result.error = f"Batch processing timeout ({self.batch_timeout}s)"
        except Exception as e:
            batch_result.error = str(e)
        finally:
            batch_result.end_time = time.time()
        
        return batch_result
    
    async def process_stream(
        self,
        stream: asyncio.Queue,
        processor: Callable[[List[T]], List[R]]
    ) -> None:
        """ìŠ¤íŠ¸ë¦¼ì—ì„œ ì•„ì´í…œì„ ë°›ì•„ ë°°ì¹˜ ì²˜ë¦¬"""
        if self.auto_flush_interval:
            self._auto_flush_task = asyncio.create_task(
                self._auto_flush_loop(processor)
            )
        
        try:
            while True:
                try:
                    item = await asyncio.wait_for(stream.get(), timeout=1.0)
                    if item is None:  # ì¢…ë£Œ ì‹ í˜¸
                        break
                    
                    await self.add(item, processor)
                    
                except asyncio.TimeoutError:
                    continue
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if self.current_batch:
                await self._flush_batch(processor)
        
        finally:
            if self._auto_flush_task:
                self._auto_flush_task.cancel()
    
    async def _auto_flush_loop(self, processor: Callable[[List[T]], List[R]]):
        """ìë™ í”ŒëŸ¬ì‹œ ë£¨í”„"""
        while True:
            await asyncio.sleep(self.auto_flush_interval)
            async with self.batch_lock:
                if self.current_batch:
                    await self._flush_batch(processor)
    
    def get_statistics(self) -> dict:
        """ì²˜ë¦¬ í†µê³„"""
        if not self.results:
            return {}
        
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        total_items = sum(len(r.items) for r in self.results)
        total_duration = sum(r.duration for r in self.results)
        
        batch_sizes = [len(r.items) for r in self.results]
        avg_batch_size = sum(batch_sizes) / len(batch_sizes) if batch_sizes else 0
        
        return {
            "total_batches": len(self.results),
            "successful_batches": len(successful),
            "failed_batches": len(failed),
            "total_items": total_items,
            "total_duration": total_duration,
            "average_batch_duration": total_duration / len(self.results) if self.results else 0,
            "average_batch_size": avg_batch_size,
            "items_per_second": total_items / total_duration if total_duration > 0 else 0,
            "configured_batch_size": self.batch_size
        }


class AdaptiveBatchProcessor(AsyncBatchProcessor[T, R]):
    """ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬ê¸° - ì²˜ë¦¬ ì†ë„ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •"""
    
    def __init__(
        self,
        initial_batch_size: int = 100,
        min_batch_size: int = 10,
        max_batch_size: int = 1000,
        target_duration: float = 1.0,
        adjustment_factor: float = 0.2
    ):
        super().__init__(batch_size=initial_batch_size)
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.target_duration = target_duration
        self.adjustment_factor = adjustment_factor
        
        self.performance_history = defaultdict(list)
    
    async def _process_batch_async(
        self,
        batch: List[T],
        processor: Callable[[List[T]], List[R]]
    ) -> BatchResult[T, R]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ê¸°ë¡"""
        result = await super()._process_batch_async(batch, processor)
        
        if result.success:
            # ì„±ëŠ¥ ê¸°ë¡
            batch_size = len(batch)
            self.performance_history[batch_size].append(result.duration)
            
            # ë°°ì¹˜ í¬ê¸° ì¡°ì •
            self._adjust_batch_size(result)
        
        return result
    
    def _adjust_batch_size(self, result: BatchResult[T, R]):
        """ì²˜ë¦¬ ì‹œê°„ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¡°ì •"""
        if result.duration < self.target_duration * 0.8:
            # ì²˜ë¦¬ê°€ ë„ˆë¬´ ë¹ ë¦„ - ë°°ì¹˜ í¬ê¸° ì¦ê°€
            new_size = int(self.batch_size * (1 + self.adjustment_factor))
            self.batch_size = min(new_size, self.max_batch_size)
        
        elif result.duration > self.target_duration * 1.2:
            # ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë¦¼ - ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            new_size = int(self.batch_size * (1 - self.adjustment_factor))
            self.batch_size = max(new_size, self.min_batch_size)
        
        logging.info(f"Batch size adjusted to {self.batch_size} (duration: {result.duration:.2f}s)")


async def example_batch_processing():
    """ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ íŒ¨í„´ ì˜ˆì œ")
    print("=" * 60)
    
    # 1. ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬
    print("\n1. ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬")
    
    # ì²˜ë¦¬ í•¨ìˆ˜ (ì‹œë®¬ë ˆì´ì…˜)
    async def process_batch(items: List[int]) -> List[int]:
        await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        return [item * 2 for item in items]
    
    processor = AsyncBatchProcessor[int, int](
        batch_size=5,
        max_concurrent_batches=3
    )
    
    # ì•„ì´í…œ ì¶”ê°€
    items = list(range(20))
    results = await processor.add_many(items, process_batch)
    
    print(f"ì²˜ë¦¬ëœ ë°°ì¹˜ ìˆ˜: {len(results)}")
    for result in results:
        print(f"  ë°°ì¹˜ {result.batch_id}: {len(result.items)}ê°œ ì•„ì´í…œ, "
              f"{result.duration:.3f}ì´ˆ")
    
    # 2. ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
    print("\n2. ìŠ¤íŠ¸ë¦¼ ë°°ì¹˜ ì²˜ë¦¬")
    
    stream_processor = AsyncBatchProcessor[str, str](
        batch_size=3,
        auto_flush_interval=1.0  # 1ì´ˆë§ˆë‹¤ ìë™ í”ŒëŸ¬ì‹œ
    )
    
    # ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
    stream_queue = asyncio.Queue()
    
    async def producer():
        for i in range(10):
            await stream_queue.put(f"Item {i}")
            await asyncio.sleep(0.3)
        await stream_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸
    
    async def string_processor(items: List[str]) -> List[str]:
        return [f"Processed: {item}" for item in items]
    
    # ìƒì‚°ìì™€ ë°°ì¹˜ ì²˜ë¦¬ ë™ì‹œ ì‹¤í–‰
    await asyncio.gather(
        producer(),
        stream_processor.process_stream(stream_queue, string_processor)
    )
    
    print(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì™„ë£Œ: {len(stream_processor.results)}ê°œ ë°°ì¹˜")
    
    # 3. ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬
    print("\n3. ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬")
    
    adaptive_processor = AdaptiveBatchProcessor[int, int](
        initial_batch_size=10,
        min_batch_size=5,
        max_batch_size=50,
        target_duration=0.5
    )
    
    # ê°€ë³€ ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
    async def variable_processor(items: List[int]) -> List[int]:
        # ì•„ì´í…œ ìˆ˜ì— ë”°ë¼ ì²˜ë¦¬ ì‹œê°„ ì¦ê°€
        process_time = len(items) * 0.05
        await asyncio.sleep(process_time)
        return [item ** 2 for item in items]
    
    # ì—¬ëŸ¬ ë²ˆ ì²˜ë¦¬í•˜ì—¬ ì ì‘ í™•ì¸
    for round in range(5):
        items = list(range(20))
        results = await adaptive_processor.add_many(items, variable_processor)
        
        print(f"\në¼ìš´ë“œ {round + 1}:")
        print(f"  í˜„ì¬ ë°°ì¹˜ í¬ê¸°: {adaptive_processor.batch_size}")
        print(f"  ì²˜ë¦¬ëœ ë°°ì¹˜: {len(results)}")
        
        for result in results:
            print(f"    ë°°ì¹˜ {result.batch_id}: {len(result.items)}ê°œ, "
                  f"{result.duration:.3f}ì´ˆ")
    
    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ì „ì²´ í†µê³„:")
    stats = adaptive_processor.get_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")


def example_sync_batch_processing():
    """ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
    print("\n\nâš™ï¸  ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ")
    print("=" * 60)
    
    # ì²˜ë¦¬ í•¨ìˆ˜
    def cpu_intensive_processor(items: List[int]) -> List[int]:
        # CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        results = []
        for item in items:
            result = sum(i ** 2 for i in range(item * 100))
            results.append(result)
        return results
    
    # í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬
    processor = BatchProcessor[int, int](
        batch_size=10,
        max_workers=4,
        use_processes=True
    )
    
    # ë°ì´í„° ì²˜ë¦¬
    data = list(range(50))
    results = processor.process(data, cpu_intensive_processor, parallel=True)
    
    print(f"ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ë°°ì¹˜")
    
    # í†µê³„
    stats = processor.get_statistics()
    print("\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì˜ˆì œ
    asyncio.run(example_batch_processing())
    
    # ë™ê¸° ì˜ˆì œ
    example_sync_batch_processing()