# 18. NLP Text Processing - ìì—°ì–´ ì²˜ë¦¬

## ğŸ“š ê³¼ì • ì†Œê°œ
ë§ˆì¼€íŒ… ë° ê´‘ê³  ë„ë©”ì¸ì— íŠ¹í™”ëœ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê´‘ê³  ì¹´í”¼ ë¶„ì„, ê°ì • ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ, ìë™ ì¹´í”¼ ìƒì„±ê¹Œì§€ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê´‘ê³  í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¶„ì„
- ê°ì • ë¶„ì„ìœ¼ë¡œ ë¸Œëœë“œ ëª¨ë‹ˆí„°ë§
- í‚¤ì›Œë“œ ì¶”ì¶œ ë° í† í”½ ëª¨ë¸ë§
- ìë™ ê´‘ê³  ì¹´í”¼ ìƒì„±

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê´‘ê³  í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
```python
import re
import nltk
from konlpy.tag import Okt, Mecab
from typing import List, Dict, Tuple
import numpy as np
from collections import Counter

class AdTextPreprocessor:
    """ê´‘ê³  í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, language='ko'):
        self.language = language
        if language == 'ko':
            self.tokenizer = Okt()
        else:
            nltk.download('punkt')
            nltk.download('stopwords')
            from nltk.corpus import stopwords
            self.stop_words = set(stopwords.words('english'))
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # URL ì œê±°
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # ì´ë©”ì¼ ì œê±°
        text = re.sub(r'\S+@\S+', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        if self.language == 'ko':
            text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
        else:
            text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """í† í°í™”"""
        if self.language == 'ko':
            # ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬ë§Œ ì¶”ì¶œ
            tokens = self.tokenizer.pos(text, stem=True)
            return [word for word, pos in tokens if pos in ['Noun', 'Adjective', 'Verb'] and len(word) > 1]
        else:
            from nltk.tokenize import word_tokenize
            tokens = word_tokenize(text.lower())
            return [word for word in tokens if word not in self.stop_words and len(word) > 2]
    
    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:
        """N-gram ì¶”ì¶œ"""
        if len(tokens) < n:
            return []
        return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
    
    def preprocess_ad_copy(self, text: str) -> Dict:
        """ê´‘ê³  ì¹´í”¼ ì „ì²˜ë¦¬ (ì „ì²´ íŒŒì´í”„ë¼ì¸)"""
        # ì •ì œ
        cleaned = self.clean_text(text)
        
        # í† í°í™”
        tokens = self.tokenize(cleaned)
        
        # N-gram ì¶”ì¶œ
        bigrams = self.extract_ngrams(tokens, 2)
        trigrams = self.extract_ngrams(tokens, 3)
        
        return {
            'original': text,
            'cleaned': cleaned,
            'tokens': tokens,
            'bigrams': bigrams,
            'trigrams': trigrams,
            'token_count': len(tokens),
            'unique_tokens': len(set(tokens))
        }
```

### ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.nn.functional import softmax

class SentimentAnalyzer:
    """ê°ì • ë¶„ì„ê¸° (ë¸Œëœë“œ ëª¨ë‹ˆí„°ë§ìš©)"""
    
    def __init__(self, model_name='klue/roberta-base'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # ê°ì • ë ˆì´ë¸”
        self.sentiment_labels = ['ë§¤ìš° ë¶€ì •', 'ë¶€ì •', 'ì¤‘ë¦½', 'ê¸ì •', 'ë§¤ìš° ê¸ì •']
    
    def analyze_sentiment(self, text: str) -> Dict:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = softmax(outputs.logits, dim=-1)
            
        pred_label_idx = torch.argmax(probabilities, dim=-1).item()
        confidence = probabilities[0][pred_label_idx].item()
        
        return {
            'sentiment': self.sentiment_labels[pred_label_idx],
            'confidence': confidence,
            'probabilities': {
                label: prob.item() 
                for label, prob in zip(self.sentiment_labels, probabilities[0])
            }
        }
    
    def batch_analyze(self, texts: List[str]) -> List[Dict]:
        """ë°°ì¹˜ ê°ì • ë¶„ì„"""
        results = []
        for text in texts:
            results.append(self.analyze_sentiment(text))
        return results
    
    def analyze_brand_mentions(self, texts: List[str], brand_keywords: List[str]) -> Dict:
        """ë¸Œëœë“œ ì–¸ê¸‰ ê°ì • ë¶„ì„"""
        brand_mentions = []
        
        for text in texts:
            # ë¸Œëœë“œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
            if any(keyword.lower() in text.lower() for keyword in brand_keywords):
                sentiment = self.analyze_sentiment(text)
                brand_mentions.append({
                    'text': text,
                    'sentiment': sentiment['sentiment'],
                    'confidence': sentiment['confidence']
                })
        
        if not brand_mentions:
            return {'message': 'ë¸Œëœë“œ ì–¸ê¸‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
        
        # ê°ì • ë¶„í¬ ê³„ì‚°
        sentiment_counts = Counter([mention['sentiment'] for mention in brand_mentions])
        total_mentions = len(brand_mentions)
        
        return {
            'total_mentions': total_mentions,
            'sentiment_distribution': {
                sentiment: count / total_mentions * 100
                for sentiment, count in sentiment_counts.items()
            },
            'detailed_mentions': brand_mentions[:10],  # ìƒìœ„ 10ê°œë§Œ
            'overall_sentiment_score': self._calculate_sentiment_score(sentiment_counts)
        }
    
    def _calculate_sentiment_score(self, sentiment_counts: Counter) -> float:
        """ì „ì²´ ê°ì • ì ìˆ˜ ê³„ì‚° (-2 ~ +2)"""
        weights = {'ë§¤ìš° ë¶€ì •': -2, 'ë¶€ì •': -1, 'ì¤‘ë¦½': 0, 'ê¸ì •': 1, 'ë§¤ìš° ê¸ì •': 2}
        total_weighted = sum(weights[sentiment] * count for sentiment, count in sentiment_counts.items())
        total_count = sum(sentiment_counts.values())
        return total_weighted / total_count if total_count > 0 else 0
```

### í‚¤ì›Œë“œ ì¶”ì¶œ ë° í† í”½ ëª¨ë¸ë§
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx
from textrank import TextRank4Keyword

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.preprocessor = AdTextPreprocessor()
        
    def extract_tfidf_keywords(self, documents: List[str], top_k: int = 10) -> Dict:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì „ì²˜ë¦¬
        processed_docs = []
        for doc in documents:
            tokens = self.preprocessor.tokenize(self.preprocessor.clean_text(doc))
            processed_docs.append(' '.join(tokens))
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        tfidf_matrix = vectorizer.fit_transform(processed_docs)
        feature_names = vectorizer.get_feature_names_out()
        
        # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
        keyword_scores = list(zip(feature_names, mean_scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'keywords': keyword_scores[:top_k],
            'total_documents': len(documents),
            'vocabulary_size': len(feature_names)
        }
    
    def extract_textrank_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """TextRank ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tr4w = TextRank4Keyword()
        tr4w.analyze(text, candidate_pos=['NOUN', 'PROPN'], window_size=4, lower=False)
        
        keywords = tr4w.get_keywords(top_k)
        return [(kw, score) for kw, score in keywords]
    
    def extract_campaign_keywords(self, campaign_texts: List[str], 
                                 performance_data: List[Dict]) -> Dict:
        """ìº í˜ì¸ë³„ ì„±ê³¼ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì„±ê³¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê·¸ë£¹í™”
        high_performance = []
        low_performance = []
        
        for text, perf in zip(campaign_texts, performance_data):
            if perf['ctr'] > np.median([p['ctr'] for p in performance_data]):
                high_performance.append(text)
            else:
                low_performance.append(text)
        
        # ê° ê·¸ë£¹ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
        high_perf_keywords = self.extract_tfidf_keywords(high_performance)
        low_perf_keywords = self.extract_tfidf_keywords(low_performance)
        
        # ì„±ê³¼ ì°¨ì´ ë¶„ì„
        high_kw_set = set([kw[0] for kw in high_perf_keywords['keywords']])
        low_kw_set = set([kw[0] for kw in low_perf_keywords['keywords']])
        
        return {
            'high_performance_unique': list(high_kw_set - low_kw_set),
            'low_performance_unique': list(low_kw_set - high_kw_set),
            'common_keywords': list(high_kw_set & low_kw_set),
            'high_performance_keywords': high_perf_keywords['keywords'],
            'low_performance_keywords': low_perf_keywords['keywords']
        }

class TopicModeler:
    """í† í”½ ëª¨ë¸ë§"""
    
    def __init__(self, n_topics: int = 5):
        self.n_topics = n_topics
        self.preprocessor = AdTextPreprocessor()
        
    def fit_lda(self, documents: List[str]) -> Dict:
        """LDA í† í”½ ëª¨ë¸ë§"""
        # ì „ì²˜ë¦¬
        processed_docs = []
        for doc in documents:
            tokens = self.preprocessor.tokenize(self.preprocessor.clean_text(doc))
            processed_docs.append(' '.join(tokens))
        
        # ë²¡í„°í™”
        vectorizer = TfidfVectorizer(
            max_features=1000,
            min_df=2,
            max_df=0.8,
            stop_words=None
        )
        
        doc_term_matrix = vectorizer.fit_transform(processed_docs)
        
        # LDA í•™ìŠµ
        lda = LatentDirichletAllocation(
            n_components=self.n_topics,
            random_state=42,
            max_iter=100
        )
        
        lda.fit(doc_term_matrix)
        
        # í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            top_scores = [topic[i] for i in top_words_idx]
            
            topics.append({
                'topic_id': topic_idx,
                'words': list(zip(top_words, top_scores)),
                'weight': topic.sum()
            })
        
        # ë¬¸ì„œë³„ í† í”½ ë¶„í¬
        doc_topic_matrix = lda.transform(doc_term_matrix)
        
        return {
            'topics': topics,
            'document_topic_distribution': doc_topic_matrix,
            'perplexity': lda.perplexity(doc_term_matrix),
            'log_likelihood': lda.score(doc_term_matrix)
        }
```

### ìë™ ê´‘ê³  ì¹´í”¼ ìƒì„±
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline

class AdCopyGenerator:
    """ê´‘ê³  ì¹´í”¼ ìë™ ìƒì„±ê¸°"""
    
    def __init__(self, model_name='skt/kogpt2-base-v2'):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.generator = pipeline('text-generation', 
                                 model=self.model,
                                 tokenizer=self.tokenizer)
        
        # ê´‘ê³  ì¹´í”¼ í…œí”Œë¦¿
        self.templates = [
            "{product}ë¡œ {benefit}ì„ ê²½í—˜í•˜ì„¸ìš”!",
            "ì§€ê¸ˆ {product} {discount}% í• ì¸!",
            "{target_audience}ë¥¼ ìœ„í•œ íŠ¹ë³„í•œ {product}",
            "{problem}ì— ì§€ì¹œ ë‹¹ì‹ , {product}ê°€ í•´ë‹µì…ë‹ˆë‹¤",
            "í•œì • íŠ¹ê°€! {product} {action} ê¸°íšŒ"
        ]
    
    def generate_copy_variants(self, product: str, keywords: List[str], 
                              target_audience: str = "", num_variants: int = 5) -> List[Dict]:
        """ì¹´í”¼ ë³€í˜• ìƒì„±"""
        base_prompt = f"{product} ê´‘ê³  ì¹´í”¼: "
        if target_audience:
            base_prompt += f"{target_audience} ëŒ€ìƒ, "
        base_prompt += f"í‚¤ì›Œë“œ: {', '.join(keywords[:3])}"
        
        variants = []
        
        # GPT ê¸°ë°˜ ìƒì„±
        for i in range(num_variants):
            generated = self.generator(
                base_prompt,
                max_length=50,
                num_return_sequences=1,
                temperature=0.8,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # í›„ì²˜ë¦¬
            text = generated[0]['generated_text'].replace(base_prompt, '').strip()
            text = self._clean_generated_text(text)
            
            variants.append({
                'copy': text,
                'method': 'gpt_generation',
                'score': self._score_copy(text, keywords)
            })
        
        # í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±
        for template in self.templates[:3]:
            try:
                filled_template = self._fill_template(template, product, keywords, target_audience)
                variants.append({
                    'copy': filled_template,
                    'method': 'template_based',
                    'score': self._score_copy(filled_template, keywords)
                })
            except:
                continue
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        variants.sort(key=lambda x: x['score'], reverse=True)
        return variants[:num_variants]
    
    def _clean_generated_text(self, text: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ì œ"""
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
        sentences = text.split('.')
        if len(sentences) > 1:
            text = sentences[0] + '.'
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\s!?.,()%]', '', text)
        return text.strip()
    
    def _fill_template(self, template: str, product: str, 
                      keywords: List[str], target_audience: str) -> str:
        """í…œí”Œë¦¿ ì±„ìš°ê¸°"""
        replacements = {
            'product': product,
            'benefit': keywords[0] if keywords else 'ë§Œì¡±',
            'discount': np.random.choice([10, 20, 30, 50]),
            'target_audience': target_audience or 'ê³ ê°',
            'problem': keywords[1] if len(keywords) > 1 else 'ê³ ë¯¼',
            'action': np.random.choice(['êµ¬ë§¤', 'ì²´í—˜', 'ë¬¸ì˜'])
        }
        
        for key, value in replacements.items():
            template = template.replace(f'{{{key}}}', str(value))
        
        return template
    
    def _score_copy(self, copy: str, keywords: List[str]) -> float:
        """ì¹´í”¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        
        # í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜
        for keyword in keywords:
            if keyword.lower() in copy.lower():
                score += 1
        
        # ê¸¸ì´ ì ìˆ˜ (15-50ì ì ì •)
        length = len(copy)
        if 15 <= length <= 50:
            score += 2
        elif 10 <= length <= 60:
            score += 1
        
        # ê°ì • í‘œí˜„ ì ìˆ˜
        emotional_words = ['íŠ¹ë³„í•œ', 'ë†€ë¼ìš´', 'í˜ì‹ ì ì¸', 'ìµœê³ ì˜', 'ì™„ë²½í•œ', 'ìƒˆë¡œìš´']
        for word in emotional_words:
            if word in copy:
                score += 0.5
        
        # CTA í¬í•¨ ì ìˆ˜
        cta_words = ['ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ë°”ë¡œ', 'ì¦‰ì‹œ', 'í´ë¦­', 'êµ¬ë§¤', 'ì²´í—˜']
        for word in cta_words:
            if word in copy:
                score += 1
                break
        
        return score
    
    def optimize_for_platform(self, copy: str, platform: str) -> str:
        """í”Œë«í¼ë³„ ìµœì í™”"""
        if platform.lower() == 'facebook':
            # Facebook: ê°œì¸ì ì´ê³  ìŠ¤í† ë¦¬í…”ë§
            return f"ì¹œêµ¬ë“¤ì•„! {copy} ì§„ì§œ ì¶”ì²œí•´!"
        elif platform.lower() == 'google':
            # Google: ì§ì ‘ì ì´ê³  ê²€ìƒ‰ ì¹œí™”ì 
            return copy.replace('!', '').replace('?', '') + ' | í• ì¸ê°€ê²© í™•ì¸'
        elif platform.lower() == 'instagram':
            # Instagram: í•´ì‹œíƒœê·¸ ì¶”ê°€
            hashtags = ['#íŠ¹ê°€', '#í• ì¸', '#ì¶”ì²œ', '#ê´‘ê³ ']
            return f"{copy} {' '.join(hashtags[:2])}"
        else:
            return copy
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ë¸Œëœë“œ ëª¨ë‹ˆí„°ë§ ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ**
2. **ìë™ ê´‘ê³  ì¹´í”¼ ìƒì„± í”Œë«í¼**
3. **ê²½ìŸì‚¬ í‚¤ì›Œë“œ ë¶„ì„ ë„êµ¬**
4. **ë‹¤êµ­ì–´ ê´‘ê³  ë²ˆì—­ ë° í˜„ì§€í™”**