# 30. A/B Testing - A/B í…ŒìŠ¤íŠ¸

## ğŸ“š ê³¼ì • ì†Œê°œ
ê³¼í•™ì  ê´‘ê³  ìµœì í™”ë¥¼ ìœ„í•œ A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„, ì‹¤í–‰, ë¶„ì„ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. ë² ì´ì§€ì•ˆ A/B í…ŒìŠ¤íŠ¸, ë‹¤ë³€ëŸ‰ í…ŒìŠ¤íŠ¸, ìˆœì°¨ì  í…ŒìŠ¤íŠ¸ê¹Œì§€ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê³¼í•™ì  ì‹¤í—˜ ì„¤ê³„ ë°©ë²•ë¡ 
- í†µê³„ì  ìœ ì˜ì„±ê³¼ ì‹¤ìš©ì  ìœ ì˜ì„±
- ë² ì´ì§€ì•ˆ A/B í…ŒìŠ¤íŠ¸ êµ¬í˜„
- ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ì‹¤í—˜ ì„¤ê³„ í”„ë ˆì„ì›Œí¬
```python
import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from datetime import datetime, timedelta
import hashlib

@dataclass
class ExperimentConfig:
    """ì‹¤í—˜ ì„¤ì • í´ë˜ìŠ¤"""
    name: str
    hypothesis: str
    primary_metric: str
    secondary_metrics: List[str]
    minimum_detectable_effect: float
    baseline_conversion_rate: float
    alpha: float = 0.05
    power: float = 0.80
    one_sided: bool = False
    
class ExperimentDesigner:
    """ì‹¤í—˜ ì„¤ê³„ì"""
    
    def __init__(self):
        self.experiment_registry = {}
        
    def design_experiment(self, config: ExperimentConfig) -> Dict:
        """ì‹¤í—˜ ì„¤ê³„"""
        # í‘œë³¸ í¬ê¸° ê³„ì‚°
        sample_size = self._calculate_sample_size(
            baseline_rate=config.baseline_conversion_rate,
            mde=config.minimum_detectable_effect,
            alpha=config.alpha,
            power=config.power,
            one_sided=config.one_sided
        )
        
        # ì‹¤í—˜ ê¸°ê°„ ì¶”ì •
        duration_info = self._estimate_duration(sample_size)
        
        # ë¬´ì‘ìœ„ ë°°ì • ì „ëµ
        randomization_config = self._setup_randomization()
        
        # ì‹¤í—˜ ì„¤ì •
        experiment_design = {
            'config': config,
            'sample_size_per_variant': sample_size,
            'total_sample_size': sample_size * 2,
            'duration_estimates': duration_info,
            'randomization': randomization_config,
            'success_criteria': self._define_success_criteria(config),
            'monitoring_plan': self._create_monitoring_plan(config),
            'statistical_plan': self._create_statistical_plan(config)
        }
        
        # ì‹¤í—˜ ë“±ë¡
        self.experiment_registry[config.name] = experiment_design
        
        return experiment_design
    
    def _calculate_sample_size(self, baseline_rate: float, mde: float,
                              alpha: float, power: float, one_sided: bool) -> int:
        """í‘œë³¸ í¬ê¸° ê³„ì‚°"""
        from statsmodels.stats.power import tt_solve_power
        from statsmodels.stats.proportion import proportion_effectsize
        
        # íš¨ê³¼ í¬ê¸° ê³„ì‚°
        treatment_rate = baseline_rate * (1 + mde)
        effect_size = proportion_effectsize(treatment_rate, baseline_rate)
        
        # ë‹¨ì¸¡/ì–‘ì¸¡ ê²€ì • ì¡°ì •
        alpha_adjusted = alpha if one_sided else alpha / 2
        
        # í‘œë³¸ í¬ê¸° ê³„ì‚°
        n = tt_solve_power(
            effect_size=effect_size,
            power=power,
            alpha=alpha_adjusted,
            alternative='larger' if one_sided else 'two-sided'
        )
        
        return int(np.ceil(n))
    
    def _estimate_duration(self, sample_size: int) -> Dict:
        """ì‹¤í—˜ ê¸°ê°„ ì¶”ì •"""
        return {
            'sample_size_per_variant': sample_size,
            'total_needed': sample_size * 2,
            'duration_calculator': lambda daily_traffic: max(1, int(np.ceil(sample_size * 2 / daily_traffic))),
            'recommendations': {
                'min_duration_days': 7,  # ìµœì†Œ 1ì£¼ì¼
                'max_duration_days': 30,  # ìµœëŒ€ 1ê°œì›”
                'weekday_effect_consideration': True
            }
        }
    
    def _setup_randomization(self) -> Dict:
        """ë¬´ì‘ìœ„ ë°°ì • ì„¤ì •"""
        return {
            'method': 'hash_based',
            'traffic_split': 0.5,
            'hash_function': 'md5',
            'salt': f"experiment_{datetime.now().strftime('%Y%m%d')}",
            'consistency_check': True
        }
    
    def _define_success_criteria(self, config: ExperimentConfig) -> Dict:
        """ì„±ê³µ ê¸°ì¤€ ì •ì˜"""
        return {
            'primary_metric': {
                'metric': config.primary_metric,
                'minimum_lift': config.minimum_detectable_effect,
                'significance_level': config.alpha,
                'decision_threshold': 'statistical_significance'
            },
            'secondary_metrics': [
                {
                    'metric': metric,
                    'monitoring_purpose': 'guardrail',
                    'alert_threshold': 0.05  # 5% í•˜ë½ ì‹œ ê²½ê³ 
                }
                for metric in config.secondary_metrics
            ],
            'business_criteria': {
                'min_practical_significance': config.minimum_detectable_effect * 0.5,
                'cost_benefit_threshold': 1.2  # ìµœì†Œ 20% ROI
            }
        }

class ABTestRunner:
    """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.active_experiments = {}
        self.experiment_data = {}
        
    def start_experiment(self, experiment_design: Dict, 
                        traffic_allocation: float = 1.0) -> str:
        """ì‹¤í—˜ ì‹œì‘"""
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        experiment_runtime = {
            'id': experiment_id,
            'design': experiment_design,
            'start_time': datetime.now(),
            'traffic_allocation': traffic_allocation,
            'status': 'running',
            'participants': {'control': [], 'treatment': []},
            'metrics_data': {'control': {}, 'treatment': {}}
        }
        
        self.active_experiments[experiment_id] = experiment_runtime
        self.experiment_data[experiment_id] = pd.DataFrame()
        
        return experiment_id
    
    def assign_variant(self, experiment_id: str, user_id: str) -> str:
        """ì‚¬ìš©ì ë³€í˜• ë°°ì •"""
        if experiment_id not in self.active_experiments:
            return 'control'  # ê¸°ë³¸ê°’
        
        experiment = self.active_experiments[experiment_id]
        
        # í•´ì‹œ ê¸°ë°˜ ì¼ê´€ëœ ë°°ì •
        hash_input = f"{user_id}_{experiment['design']['randomization']['salt']}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
        
        # 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
        assignment_score = (hash_value % 10000) / 10000
        
        # íŠ¸ë˜í”½ í• ë‹¹ ê³ ë ¤
        if assignment_score > experiment['traffic_allocation']:
            return 'control'  # ì‹¤í—˜ ëŒ€ìƒ ì™¸
        
        # ë³€í˜• ë°°ì •
        split_point = experiment['design']['randomization']['traffic_split']
        variant = 'treatment' if assignment_score < split_point else 'control'
        
        # ì°¸ê°€ì ê¸°ë¡
        experiment['participants'][variant].append({
            'user_id': user_id,
            'assigned_at': datetime.now(),
            'assignment_score': assignment_score
        })
        
        return variant
    
    def record_event(self, experiment_id: str, user_id: str, 
                    event_type: str, value: float = 1.0, 
                    metadata: Dict = None) -> None:
        """ì´ë²¤íŠ¸ ê¸°ë¡"""
        if experiment_id not in self.active_experiments:
            return
        
        # ì‚¬ìš©ì ë³€í˜• í™•ì¸
        variant = self._get_user_variant(experiment_id, user_id)
        if not variant:
            return
        
        # ì´ë²¤íŠ¸ ë°ì´í„° êµ¬ì„±
        event_data = {
            'experiment_id': experiment_id,
            'user_id': user_id,
            'variant': variant,
            'event_type': event_type,
            'value': value,
            'timestamp': datetime.now(),
            'metadata': metadata or {}
        }
        
        # ë°ì´í„° ì €ì¥
        new_row = pd.DataFrame([event_data])
        self.experiment_data[experiment_id] = pd.concat([
            self.experiment_data[experiment_id], new_row
        ], ignore_index=True)
    
    def _get_user_variant(self, experiment_id: str, user_id: str) -> Optional[str]:
        """ì‚¬ìš©ì ë³€í˜• ì¡°íšŒ"""
        experiment = self.active_experiments[experiment_id]
        
        for variant in ['control', 'treatment']:
            for participant in experiment['participants'][variant]:
                if participant['user_id'] == user_id:
                    return variant
        
        return None

class BayesianABTest:
    """ë² ì´ì§€ì•ˆ A/B í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, prior_alpha: float = 1, prior_beta: float = 1):
        self.prior_alpha = prior_alpha
        self.prior_beta = prior_beta
        
    def analyze_experiment(self, control_conversions: int, control_visitors: int,
                          treatment_conversions: int, treatment_visitors: int,
                          credible_interval: float = 0.95) -> Dict:
        """ë² ì´ì§€ì•ˆ ë¶„ì„"""
        # ì‚¬í›„ë¶„í¬ ê³„ì‚°
        control_posterior = self._calculate_posterior(control_conversions, 
                                                    control_visitors - control_conversions)
        treatment_posterior = self._calculate_posterior(treatment_conversions,
                                                       treatment_visitors - treatment_conversions)
        
        # ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        n_samples = 100000
        control_samples = control_posterior.rvs(n_samples)
        treatment_samples = treatment_posterior.rvs(n_samples)
        
        # í™•ë¥  ê³„ì‚°
        prob_treatment_better = np.mean(treatment_samples > control_samples)
        
        # ë¦¬í”„íŠ¸ ë¶„ì„
        lift_samples = (treatment_samples - control_samples) / control_samples
        lift_mean = np.mean(lift_samples)
        lift_credible_interval = np.percentile(lift_samples, 
                                             [50 * (1 - credible_interval), 
                                              50 * (1 + credible_interval)])
        
        # ê¸°ëŒ€ ì†ì‹¤ ê³„ì‚°
        expected_loss_control = np.mean(np.maximum(0, treatment_samples - control_samples))
        expected_loss_treatment = np.mean(np.maximum(0, control_samples - treatment_samples))
        
        return {
            'control': {
                'conversions': control_conversions,
                'visitors': control_visitors,
                'rate': control_conversions / control_visitors,
                'posterior_mean': control_posterior.mean(),
                'credible_interval': control_posterior.interval(credible_interval)
            },
            'treatment': {
                'conversions': treatment_conversions,
                'visitors': treatment_visitors, 
                'rate': treatment_conversions / treatment_visitors,
                'posterior_mean': treatment_posterior.mean(),
                'credible_interval': treatment_posterior.interval(credible_interval)
            },
            'probability_treatment_better': prob_treatment_better,
            'probability_control_better': 1 - prob_treatment_better,
            'expected_lift': lift_mean,
            'lift_credible_interval': lift_credible_interval,
            'expected_loss': {
                'control': expected_loss_control,
                'treatment': expected_loss_treatment
            },
            'decision_recommendation': self._make_bayesian_decision(
                prob_treatment_better, expected_loss_control, expected_loss_treatment
            )
        }
    
    def _calculate_posterior(self, successes: int, failures: int) -> stats.beta:
        """ë² íƒ€ ì‚¬í›„ë¶„í¬ ê³„ì‚°"""
        alpha_post = self.prior_alpha + successes
        beta_post = self.prior_beta + failures
        return stats.beta(alpha_post, beta_post)
    
    def _make_bayesian_decision(self, prob_better: float, 
                               loss_control: float, loss_treatment: float) -> Dict:
        """ë² ì´ì§€ì•ˆ ì˜ì‚¬ê²°ì •"""
        if prob_better > 0.95 and loss_control < 0.01:
            decision = "treatment_winner"
            confidence = "high"
        elif prob_better < 0.05 and loss_treatment < 0.01:
            decision = "control_winner"
            confidence = "high"
        elif 0.8 <= prob_better <= 0.95:
            decision = "treatment_likely_better"
            confidence = "medium"
        elif 0.05 <= prob_better <= 0.2:
            decision = "control_likely_better"
            confidence = "medium"
        else:
            decision = "inconclusive"
            confidence = "low"
        
        return {
            'decision': decision,
            'confidence': confidence,
            'probability_better': prob_better,
            'recommendation': self._generate_recommendation(decision, confidence)
        }
    
    def _generate_recommendation(self, decision: str, confidence: str) -> str:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = {
            'treatment_winner': "Treatment ë³€í˜•ì„ ì „ì²´ íŠ¸ë˜í”½ì— ì ìš©í•˜ì„¸ìš”.",
            'control_winner': "Control ë³€í˜•ì„ ìœ ì§€í•˜ê³  Treatmentë¥¼ ì¤‘ë‹¨í•˜ì„¸ìš”.", 
            'treatment_likely_better': "ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ Treatment ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.",
            'control_likely_better': "ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì‹¤í—˜ ì¤‘ë‹¨ì„ ê³ ë ¤í•˜ì„¸ìš”.",
            'inconclusive': "ì‹¤í—˜ì„ ê³„ì†í•˜ê±°ë‚˜ ë” í° íš¨ê³¼ë¥¼ ê°€ì§„ ìƒˆë¡œìš´ ë³€í˜•ì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”."
        }
        
        return recommendations.get(decision, "ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")

class SequentialTesting:
    """ìˆœì°¨ì  í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, alpha: float = 0.05, beta: float = 0.2):
        self.alpha = alpha
        self.beta = beta
        
    def setup_sprt(self, h0_rate: float, h1_rate: float) -> Dict:
        """Sequential Probability Ratio Test ì„¤ì •"""
        # ê°€ì„¤ ì„¤ì •
        # H0: conversion_rate = h0_rate
        # H1: conversion_rate = h1_rate
        
        # ì„ê³„ê°’ ê³„ì‚°
        A = (1 - self.beta) / self.alpha  # ìƒí•œ
        B = self.beta / (1 - self.alpha)  # í•˜í•œ
        
        return {
            'h0_rate': h0_rate,
            'h1_rate': h1_rate,
            'upper_threshold': A,
            'lower_threshold': B,
            'alpha': self.alpha,
            'beta': self.beta,
            'decision_boundaries': self._calculate_decision_boundaries(h0_rate, h1_rate, A, B)
        }
    
    def analyze_sequential_data(self, sprt_config: Dict, 
                              conversions: int, visitors: int) -> Dict:
        """ìˆœì°¨ ë°ì´í„° ë¶„ì„"""
        h0_rate = sprt_config['h0_rate']
        h1_rate = sprt_config['h1_rate']
        
        # ìš°ë„ë¹„ ê³„ì‚°
        if visitors == 0:
            likelihood_ratio = 1
        else:
            failures = visitors - conversions
            
            # ìš°ë„ ê³„ì‚°
            likelihood_h1 = (h1_rate ** conversions) * ((1 - h1_rate) ** failures)
            likelihood_h0 = (h0_rate ** conversions) * ((1 - h0_rate) ** failures)
            
            likelihood_ratio = likelihood_h1 / likelihood_h0 if likelihood_h0 > 0 else float('inf')
        
        # ì˜ì‚¬ê²°ì •
        decision = self._make_sequential_decision(likelihood_ratio, sprt_config)
        
        return {
            'conversions': conversions,
            'visitors': visitors,
            'conversion_rate': conversions / visitors if visitors > 0 else 0,
            'likelihood_ratio': likelihood_ratio,
            'log_likelihood_ratio': np.log(likelihood_ratio) if likelihood_ratio > 0 else float('-inf'),
            'decision': decision,
            'continue_sampling': decision['action'] == 'continue'
        }
    
    def _make_sequential_decision(self, likelihood_ratio: float, 
                                sprt_config: Dict) -> Dict:
        """ìˆœì°¨ì  ì˜ì‚¬ê²°ì •"""
        A = sprt_config['upper_threshold']
        B = sprt_config['lower_threshold']
        
        if likelihood_ratio >= A:
            return {
                'action': 'reject_h0',
                'conclusion': f'H1 ì±„íƒ: ì „í™˜ìœ¨ì´ {sprt_config["h1_rate"]:.3f}',
                'confidence': 1 - sprt_config['alpha']
            }
        elif likelihood_ratio <= B:
            return {
                'action': 'accept_h0', 
                'conclusion': f'H0 ì±„íƒ: ì „í™˜ìœ¨ì´ {sprt_config["h0_rate"]:.3f}',
                'confidence': 1 - sprt_config['beta']
            }
        else:
            return {
                'action': 'continue',
                'conclusion': 'ë” ë§ì€ ë°ì´í„° í•„ìš”',
                'confidence': None
            }

class MultiVariateTest:
    """ë‹¤ë³€ëŸ‰ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.factorial_designs = {}
        
    def setup_factorial_design(self, factors: Dict[str, List]) -> Dict:
        """íŒ©í† ë¦¬ì–¼ ì„¤ê³„ ì„¤ì •"""
        import itertools
        
        # ëª¨ë“  ì¡°í•© ìƒì„±
        factor_names = list(factors.keys())
        factor_levels = list(factors.values())
        
        combinations = list(itertools.product(*factor_levels))
        
        # ë³€í˜• ìƒì„±
        variants = []
        for i, combo in enumerate(combinations):
            variant = {
                'id': f'variant_{i}',
                'factors': dict(zip(factor_names, combo))
            }
            variants.append(variant)
        
        design = {
            'factors': factors,
            'variants': variants,
            'total_variants': len(variants),
            'sample_size_adjustment': self._calculate_multivariate_sample_size(len(variants))
        }
        
        return design
    
    def _calculate_multivariate_sample_size(self, num_variants: int) -> Dict:
        """ë‹¤ë³€ëŸ‰ í…ŒìŠ¤íŠ¸ í‘œë³¸ í¬ê¸° ì¡°ì •"""
        # Bonferroni ë³´ì •
        bonferroni_alpha = 0.05 / (num_variants - 1)  # ëŒ€ì¡°êµ° ì œì™¸
        
        return {
            'bonferroni_corrected_alpha': bonferroni_alpha,
            'sample_size_multiplier': np.sqrt(num_variants),
            'recommendation': f'{num_variants}ê°œ ë³€í˜•ì— ëŒ€í•´ í‘œë³¸ í¬ê¸°ë¥¼ {np.sqrt(num_variants):.2f}ë°° ì¦ê°€ ê¶Œì¥'
        }
    
    def analyze_factorial_results(self, results_data: pd.DataFrame,
                                 factors: List[str], outcome: str) -> Dict:
        """íŒ©í† ë¦¬ì–¼ ê²°ê³¼ ë¶„ì„"""
        from sklearn.linear_model import LinearRegression
        from itertools import combinations
        
        # ì£¼íš¨ê³¼ ë¶„ì„
        main_effects = {}
        for factor in factors:
            factor_means = results_data.groupby(factor)[outcome].mean()
            main_effects[factor] = {
                'means_by_level': factor_means.to_dict(),
                'effect_size': factor_means.max() - factor_means.min()
            }
        
        # êµí˜¸ì‘ìš© ë¶„ì„
        interaction_effects = {}
        for factor_pair in combinations(factors, 2):
            interaction_means = results_data.groupby(list(factor_pair))[outcome].mean()
            interaction_effects[f'{factor_pair[0]}_x_{factor_pair[1]}'] = {
                'means': interaction_means.to_dict(),
                'interaction_plot_data': interaction_means.reset_index()
            }
        
        # íšŒê·€ë¶„ì„ìœ¼ë¡œ íš¨ê³¼ í¬ê¸° ì¶”ì •
        X = pd.get_dummies(results_data[factors])
        y = results_data[outcome]
        
        model = LinearRegression()
        model.fit(X, y)
        
        return {
            'main_effects': main_effects,
            'interaction_effects': interaction_effects,
            'model_summary': {
                'r_squared': model.score(X, y),
                'coefficients': dict(zip(X.columns, model.coef_)),
                'intercept': model.intercept_
            },
            'recommendations': self._generate_factorial_recommendations(main_effects, interaction_effects)
        }
    
    def _generate_factorial_recommendations(self, main_effects: Dict, 
                                          interaction_effects: Dict) -> List[str]:
        """íŒ©í† ë¦¬ì–¼ ì¶”ì²œì‚¬í•­"""
        recommendations = []
        
        # ê°€ì¥ í° ì£¼íš¨ê³¼ ì°¾ê¸°
        best_factor = max(main_effects.keys(), 
                         key=lambda x: main_effects[x]['effect_size'])
        recommendations.append(f"{best_factor} ìš”ì¸ì´ ê°€ì¥ í° íš¨ê³¼ë¥¼ ë³´ì„")
        
        # ìµœì  ìˆ˜ì¤€ ì¡°í•© ì°¾ê¸°
        best_levels = {}
        for factor, effects in main_effects.items():
            best_level = max(effects['means_by_level'].keys(),
                           key=lambda x: effects['means_by_level'][x])
            best_levels[factor] = best_level
        
        recommendations.append(f"ìµœì  ì¡°í•©: {best_levels}")
        
        return recommendations

# ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
class ExperimentMonitor:
    """ì‹¤í—˜ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.monitoring_alerts = []
        
    def create_monitoring_dashboard(self, experiment_data: pd.DataFrame) -> Dict:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        # ì‹¤ì‹œê°„ ì§€í‘œ ê³„ì‚°
        current_metrics = self._calculate_current_metrics(experiment_data)
        
        # í†µê³„ì  íŒŒì›Œ ëª¨ë‹ˆí„°ë§
        power_analysis = self._monitor_statistical_power(experiment_data)
        
        # í’ˆì§ˆ ì²´í¬
        quality_checks = self._run_quality_checks(experiment_data)
        
        # ì¡°ê¸° ì¤‘ë‹¨ ì‹ í˜¸ ì²´í¬
        early_stopping_signals = self._check_early_stopping(experiment_data)
        
        return {
            'current_metrics': current_metrics,
            'power_analysis': power_analysis,
            'quality_checks': quality_checks,
            'early_stopping': early_stopping_signals,
            'alerts': self.monitoring_alerts,
            'recommendations': self._generate_monitoring_recommendations(
                current_metrics, quality_checks, early_stopping_signals
            )
        }
    
    def _calculate_current_metrics(self, data: pd.DataFrame) -> Dict:
        """í˜„ì¬ ì§€í‘œ ê³„ì‚°"""
        if data.empty:
            return {}
        
        metrics_by_variant = {}
        
        for variant in data['variant'].unique():
            variant_data = data[data['variant'] == variant]
            
            # ì „í™˜ ê´€ë ¨ ì§€í‘œ
            conversions = len(variant_data[variant_data['event_type'] == 'conversion'])
            visitors = variant_data['user_id'].nunique()
            
            metrics_by_variant[variant] = {
                'visitors': visitors,
                'conversions': conversions,
                'conversion_rate': conversions / visitors if visitors > 0 else 0,
                'daily_trend': self._calculate_daily_trend(variant_data)
            }
        
        return metrics_by_variant
    
    def _run_quality_checks(self, data: pd.DataFrame) -> Dict:
        """í’ˆì§ˆ ì²´í¬"""
        checks = {}
        
        # ìƒ˜í”Œ ë¹„ìœ¨ ì²´í¬ (SRM - Sample Ratio Mismatch)
        if not data.empty:
            variant_counts = data['variant'].value_counts()
            expected_ratio = 0.5  # 50:50 ë¶„í•  ê°€ì •
            
            if len(variant_counts) == 2:
                total = variant_counts.sum()
                observed_ratio = variant_counts.iloc[0] / total
                
                # ì¹´ì´ì œê³± ê²€ì •
                expected_counts = [total * expected_ratio, total * (1 - expected_ratio)]
                observed_counts = variant_counts.values
                
                chi2_stat, p_value = stats.chisquare(observed_counts, expected_counts)
                
                checks['sample_ratio_mismatch'] = {
                    'expected_ratio': expected_ratio,
                    'observed_ratio': observed_ratio,
                    'chi2_statistic': chi2_stat,
                    'p_value': p_value,
                    'is_significant': p_value < 0.01,  # ì—„ê²©í•œ ê¸°ì¤€
                    'status': 'FAIL' if p_value < 0.01 else 'PASS'
                }
        
        return checks
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ì™„ì „ ìë™í™” A/B í…ŒìŠ¤íŠ¸ í”Œë«í¼**
2. **ë² ì´ì§€ì•ˆ ì‹¤í—˜ ìµœì í™” ì‹œìŠ¤í…œ**
3. **ë‹¤ë³€ëŸ‰ í…ŒìŠ¤íŠ¸ ê´€ë¦¬ ë„êµ¬**
4. **ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**