# 19. Web Scraping - ì›¹ ìŠ¤í¬ë˜í•‘

## ğŸ“š ê³¼ì • ì†Œê°œ
ê²½ìŸì‚¬ ê´‘ê³  ëª¨ë‹ˆí„°ë§, ì‹œì¥ ì¡°ì‚¬, ê°€ê²© ì¶”ì ì„ ìœ„í•œ ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ìˆ ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. BeautifulSoup, Scrapy, Seleniumì„ í™œìš©í•œ ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê²½ìŸì‚¬ ê´‘ê³  ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ë™ì  ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
- ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
- ì•ˆí‹° ìŠ¤í¬ë˜í•‘ ìš°íšŒ ê¸°ë²•

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê¸°ë³¸ ì›¹ ìŠ¤í¬ë˜í•‘
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import logging
from dataclasses import dataclass
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

@dataclass
class AdData:
    """ê´‘ê³  ë°ì´í„° êµ¬ì¡°"""
    title: str
    description: str
    url: str
    image_url: Optional[str]
    platform: str
    advertiser: str
    timestamp: str
    price: Optional[float] = None
    category: Optional[str] = None

class BaseWebScraper:
    """ê¸°ë³¸ ì›¹ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, delay_range: tuple = (1, 3)):
        self.session = requests.Session()
        self.delay_range = delay_range
        self.setup_session()
        
    def setup_session(self):
        """ì„¸ì…˜ ì„¤ì •"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(headers)
        
    def get_page(self, url: str, **kwargs) -> Optional[requests.Response]:
        """í˜ì´ì§€ ìš”ì²­"""
        try:
            response = self.session.get(url, **kwargs)
            response.raise_for_status()
            
            # ìš”ì²­ ê°„ ì§€ì—°
            time.sleep(random.uniform(*self.delay_range))
            
            return response
        except requests.RequestException as e:
            logging.error(f"Error fetching {url}: {e}")
            return None
    
    def parse_html(self, html: str) -> BeautifulSoup:
        """HTML íŒŒì‹±"""
        return BeautifulSoup(html, 'html.parser')

class CompetitorAdMonitor(BaseWebScraper):
    """ê²½ìŸì‚¬ ê´‘ê³  ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        super().__init__()
        self.monitored_sites = {
            'naver': {
                'search_url': 'https://search.naver.com/search.naver',
                'ad_selector': '.ad_area',
                'title_selector': '.ad_tit',
                'desc_selector': '.ad_desc',
                'url_selector': '.ad_tit a'
            },
            'google': {
                'search_url': 'https://www.google.com/search',
                'ad_selector': '[data-text-ad]',
                'title_selector': 'h3',
                'desc_selector': '.VwiC3b',
                'url_selector': 'h3 a'
            }
        }
    
    def monitor_search_ads(self, keywords: List[str], 
                          platforms: List[str] = ['naver', 'google']) -> List[AdData]:
        """ê²€ìƒ‰ ê´‘ê³  ëª¨ë‹ˆí„°ë§"""
        all_ads = []
        
        for keyword in keywords:
            for platform in platforms:
                if platform in self.monitored_sites:
                    ads = self._scrape_search_ads(keyword, platform)
                    all_ads.extend(ads)
                    
        return all_ads
    
    def _scrape_search_ads(self, keyword: str, platform: str) -> List[AdData]:
        """íŠ¹ì • í”Œë«í¼ì˜ ê²€ìƒ‰ ê´‘ê³  ìŠ¤í¬ë˜í•‘"""
        config = self.monitored_sites[platform]
        
        # ê²€ìƒ‰ URL êµ¬ì„±
        params = {'query': keyword} if platform == 'naver' else {'q': keyword}
        response = self.get_page(config['search_url'], params=params)
        
        if not response:
            return []
        
        soup = self.parse_html(response.text)
        ads = []
        
        # ê´‘ê³  ì˜ì—­ ì°¾ê¸°
        ad_elements = soup.select(config['ad_selector'])
        
        for ad_element in ad_elements:
            try:
                # ê´‘ê³  ì •ë³´ ì¶”ì¶œ
                title_elem = ad_element.select_one(config['title_selector'])
                desc_elem = ad_element.select_one(config['desc_selector'])
                url_elem = ad_element.select_one(config['url_selector'])
                
                if title_elem and url_elem:
                    ad_data = AdData(
                        title=title_elem.get_text(strip=True),
                        description=desc_elem.get_text(strip=True) if desc_elem else '',
                        url=url_elem.get('href', ''),
                        image_url=None,
                        platform=platform,
                        advertiser=self._extract_advertiser(url_elem.get('href', '')),
                        timestamp=pd.Timestamp.now().isoformat()
                    )
                    ads.append(ad_data)
                    
            except Exception as e:
                logging.error(f"Error parsing ad element: {e}")
                continue
                
        return ads
    
    def _extract_advertiser(self, url: str) -> str:
        """URLì—ì„œ ê´‘ê³ ì£¼ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            return parsed.netloc
        except:
            return 'unknown'
    
    def monitor_shopping_ads(self, product_keywords: List[str]) -> List[AdData]:
        """ì‡¼í•‘ ê´‘ê³  ëª¨ë‹ˆí„°ë§"""
        shopping_sites = {
            'naver_shopping': 'https://shopping.naver.com/search/all',
            'coupang': 'https://www.coupang.com/np/search',
            'gmarket': 'http://browse.gmarket.co.kr/search'
        }
        
        all_ads = []
        
        for keyword in product_keywords:
            for site_name, base_url in shopping_sites.items():
                ads = self._scrape_shopping_site(keyword, site_name, base_url)
                all_ads.extend(ads)
                
        return all_ads
    
    def _scrape_shopping_site(self, keyword: str, site_name: str, base_url: str) -> List[AdData]:
        """ì‡¼í•‘ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        params = {'query': keyword}
        response = self.get_page(base_url, params=params)
        
        if not response:
            return []
        
        soup = self.parse_html(response.text)
        ads = []
        
        # ì‚¬ì´íŠ¸ë³„ ì„ íƒì (ê°„ì†Œí™”)
        selectors = {
            'naver_shopping': {
                'container': '.basicList_item__2XT81',
                'title': '.basicList_title__3P9Q7',
                'price': '.price_num__2WUXn',
                'link': 'a'
            },
            'coupang': {
                'container': '.search-product',
                'title': '.name',
                'price': '.price-value',
                'link': 'a'
            }
        }
        
        if site_name in selectors:
            config = selectors[site_name]
            items = soup.select(config['container'])
            
            for item in items[:10]:  # ìƒìœ„ 10ê°œë§Œ
                try:
                    title_elem = item.select_one(config['title'])
                    price_elem = item.select_one(config['price'])
                    link_elem = item.select_one(config['link'])
                    
                    if title_elem and link_elem:
                        ad_data = AdData(
                            title=title_elem.get_text(strip=True),
                            description='',
                            url=urljoin(base_url, link_elem.get('href', '')),
                            image_url=None,
                            platform=site_name,
                            advertiser=site_name,
                            timestamp=pd.Timestamp.now().isoformat(),
                            price=self._extract_price(price_elem.get_text() if price_elem else '')
                        )
                        ads.append(ad_data)
                        
                except Exception as e:
                    logging.error(f"Error parsing shopping item: {e}")
                    continue
        
        return ads
    
    def _extract_price(self, price_text: str) -> Optional[float]:
        """ê°€ê²© í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        import re
        if not price_text:
            return None
        # ìˆ«ìë§Œ ì¶”ì¶œ
        numbers = re.findall(r'[\d,]+', price_text.replace(',', ''))
        return float(numbers[0]) if numbers else None

class DynamicWebScraper:
    """ë™ì  ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ (Selenium)"""
    
    def __init__(self, headless: bool = True):
        self.headless = headless
        self.driver = None
        
    def setup_driver(self):
        """WebDriver ì„¤ì •"""
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        # User-Agent ì„¤ì •
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(10)
        
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
    
    def scrape_instagram_ads(self, hashtags: List[str]) -> List[AdData]:
        """Instagram ê´‘ê³  ìŠ¤í¬ë˜í•‘"""
        if not self.driver:
            self.setup_driver()
            
        all_ads = []
        
        for hashtag in hashtags:
            try:
                # Instagram í•´ì‹œíƒœê·¸ í˜ì´ì§€ ì´ë™
                url = f"https://www.instagram.com/explore/tags/{hashtag}/"
                self.driver.get(url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                time.sleep(3)
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ
                self._scroll_page(3)
                
                # ê´‘ê³  í¬ìŠ¤íŠ¸ ì°¾ê¸° (Sponsored í‘œì‹œê°€ ìˆëŠ” ê²ƒë“¤)
                sponsored_posts = self.driver.find_elements(
                    By.XPATH, "//article[contains(.//text(), 'Sponsored')]"
                )
                
                for post in sponsored_posts[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    try:
                        # í¬ìŠ¤íŠ¸ í´ë¦­
                        post.click()
                        time.sleep(2)
                        
                        # ê´‘ê³  ì •ë³´ ì¶”ì¶œ
                        ad_data = self._extract_instagram_ad_info()
                        if ad_data:
                            all_ads.append(ad_data)
                        
                        # ëª¨ë‹¬ ë‹«ê¸°
                        self.driver.find_element(By.XPATH, "//button[contains(@aria-label, 'Close')]").click()
                        time.sleep(1)
                        
                    except Exception as e:
                        logging.error(f"Error processing Instagram post: {e}")
                        continue
                        
            except Exception as e:
                logging.error(f"Error scraping Instagram hashtag {hashtag}: {e}")
                continue
                
        return all_ads
    
    def _scroll_page(self, times: int):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤"""
        for _ in range(times):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
    
    def _extract_instagram_ad_info(self) -> Optional[AdData]:
        """Instagram ê´‘ê³  ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê´‘ê³  í…ìŠ¤íŠ¸
            caption_elem = self.driver.find_element(
                By.XPATH, "//article//span[contains(@class, 'caption')]"
            )
            caption = caption_elem.text if caption_elem else ''
            
            # ê´‘ê³ ì£¼ ì •ë³´
            advertiser_elem = self.driver.find_element(
                By.XPATH, "//header//a[contains(@class, 'username')]"
            )
            advertiser = advertiser_elem.text if advertiser_elem else 'unknown'
            
            # ì´ë¯¸ì§€ URL
            img_elem = self.driver.find_element(By.TAG_NAME, "img")
            image_url = img_elem.get_attribute('src') if img_elem else None
            
            return AdData(
                title=caption[:100],  # ì²˜ìŒ 100ì
                description=caption,
                url=self.driver.current_url,
                image_url=image_url,
                platform='instagram',
                advertiser=advertiser,
                timestamp=pd.Timestamp.now().isoformat()
            )
            
        except Exception as e:
            logging.error(f"Error extracting Instagram ad info: {e}")
            return None
    
    def scrape_youtube_ads(self, search_terms: List[str]) -> List[AdData]:
        """YouTube ê´‘ê³  ìŠ¤í¬ë˜í•‘"""
        if not self.driver:
            self.setup_driver()
            
        all_ads = []
        
        for term in search_terms:
            try:
                # YouTube ê²€ìƒ‰
                search_url = f"https://www.youtube.com/results?search_query={term}"
                self.driver.get(search_url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.ID, "contents"))
                )
                
                # ê´‘ê³  ë¹„ë””ì˜¤ ì°¾ê¸°
                ad_videos = self.driver.find_elements(
                    By.XPATH, "//span[contains(text(), 'Ad')]/ancestor::div[contains(@class, 'ytd-video-renderer')]"
                )
                
                for ad_video in ad_videos[:3]:  # ìƒìœ„ 3ê°œë§Œ
                    try:
                        title_elem = ad_video.find_element(By.ID, "video-title")
                        channel_elem = ad_video.find_element(By.XPATH, ".//a[contains(@class, 'channel-name')]")
                        
                        ad_data = AdData(
                            title=title_elem.text,
                            description='',
                            url=title_elem.get_attribute('href'),
                            image_url=None,
                            platform='youtube',
                            advertiser=channel_elem.text,
                            timestamp=pd.Timestamp.now().isoformat()
                        )
                        all_ads.append(ad_data)
                        
                    except Exception as e:
                        logging.error(f"Error processing YouTube ad: {e}")
                        continue
                        
            except Exception as e:
                logging.error(f"Error scraping YouTube for {term}: {e}")
                continue
                
        return all_ads

class ScrapingPipeline:
    """ìŠ¤í¬ë˜í•‘ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.results = []
        self.scrapers = {
            'competitor': CompetitorAdMonitor(),
            'dynamic': DynamicWebScraper()
        }
        
    def run_monitoring_campaign(self, config: Dict) -> pd.DataFrame:
        """ëª¨ë‹ˆí„°ë§ ìº í˜ì¸ ì‹¤í–‰"""
        # ê²€ìƒ‰ ê´‘ê³  ëª¨ë‹ˆí„°ë§
        if 'search_keywords' in config:
            search_ads = self.scrapers['competitor'].monitor_search_ads(
                config['search_keywords'],
                config.get('platforms', ['naver', 'google'])
            )
            self.results.extend(search_ads)
        
        # ì‡¼í•‘ ê´‘ê³  ëª¨ë‹ˆí„°ë§
        if 'product_keywords' in config:
            shopping_ads = self.scrapers['competitor'].monitor_shopping_ads(
                config['product_keywords']
            )
            self.results.extend(shopping_ads)
        
        # ì†Œì…œ ë¯¸ë””ì–´ ê´‘ê³  ëª¨ë‹ˆí„°ë§
        if 'social_hashtags' in config:
            social_ads = self.scrapers['dynamic'].scrape_instagram_ads(
                config['social_hashtags']
            )
            self.results.extend(social_ads)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        return self._results_to_dataframe()
    
    def _results_to_dataframe(self) -> pd.DataFrame:
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.results:
            return pd.DataFrame()
        
        data = []
        for ad in self.results:
            data.append({
                'title': ad.title,
                'description': ad.description,
                'url': ad.url,
                'image_url': ad.image_url,
                'platform': ad.platform,
                'advertiser': ad.advertiser,
                'timestamp': ad.timestamp,
                'price': ad.price,
                'category': ad.category
            })
        
        return pd.DataFrame(data)
    
    def save_results(self, filepath: str, format: str = 'csv'):
        """ê²°ê³¼ ì €ì¥"""
        df = self._results_to_dataframe()
        
        if format == 'csv':
            df.to_csv(filepath, index=False, encoding='utf-8')
        elif format == 'json':
            df.to_json(filepath, orient='records', force_ascii=False, indent=2)
        elif format == 'excel':
            df.to_excel(filepath, index=False)
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.scrapers['dynamic'].close_driver()

class AntiScrapingHandler:
    """ì•ˆí‹° ìŠ¤í¬ë˜í•‘ ìš°íšŒ"""
    
    def __init__(self):
        self.proxies = []
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
        
    def rotate_user_agent(self) -> str:
        """User-Agent ë¡œí…Œì´ì…˜"""
        return random.choice(self.user_agents)
    
    def add_proxies(self, proxy_list: List[str]):
        """í”„ë¡ì‹œ ì¶”ê°€"""
        self.proxies.extend(proxy_list)
    
    def get_random_proxy(self) -> Optional[Dict]:
        """ëœë¤ í”„ë¡ì‹œ ì„ íƒ"""
        if not self.proxies:
            return None
        
        proxy = random.choice(self.proxies)
        return {
            'http': proxy,
            'https': proxy
        }
    
    def handle_rate_limiting(self, retry_count: int = 3):
        """ì†ë„ ì œí•œ ì²˜ë¦¬"""
        base_delay = 2
        for attempt in range(retry_count):
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            time.sleep(delay)
            yield attempt + 1

class PriceTracker:
    """ê°€ê²© ì¶”ì ê¸°"""
    
    def __init__(self):
        self.scraper = BaseWebScraper()
        
    def track_competitor_prices(self, products: List[Dict]) -> pd.DataFrame:
        """ê²½ìŸì‚¬ ê°€ê²© ì¶”ì """
        results = []
        
        for product in products:
            product_name = product['name']
            sites = product['sites']
            
            for site, config in sites.items():
                try:
                    price = self._scrape_price(config['url'], config['selector'])
                    
                    results.append({
                        'product': product_name,
                        'site': site,
                        'price': price,
                        'timestamp': pd.Timestamp.now(),
                        'url': config['url']
                    })
                    
                except Exception as e:
                    logging.error(f"Error tracking price for {product_name} on {site}: {e}")
                    continue
        
        return pd.DataFrame(results)
    
    def _scrape_price(self, url: str, selector: str) -> Optional[float]:
        """ê°€ê²© ìŠ¤í¬ë˜í•‘"""
        response = self.scraper.get_page(url)
        if not response:
            return None
        
        soup = self.scraper.parse_html(response.text)
        price_elem = soup.select_one(selector)
        
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            return self._extract_price(price_text)
        
        return None
    
    def _extract_price(self, price_text: str) -> Optional[float]:
        """ê°€ê²© ì¶”ì¶œ"""
        import re
        # ìˆ«ìì™€ ì‰¼í‘œë§Œ ì¶”ì¶œ
        numbers = re.findall(r'[\d,]+', price_text.replace(',', ''))
        if numbers:
            try:
                return float(numbers[0])
            except ValueError:
                return None
        return None

# ì‚¬ìš© ì˜ˆì‹œ
def example_monitoring_campaign():
    """ëª¨ë‹ˆí„°ë§ ìº í˜ì¸ ì˜ˆì‹œ"""
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    pipeline = ScrapingPipeline()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    config = {
        'search_keywords': ['ë§ˆì¼€íŒ… ë„êµ¬', 'ê´‘ê³  í”Œë«í¼', 'CRM ì†Œí”„íŠ¸ì›¨ì–´'],
        'product_keywords': ['ë…¸íŠ¸ë¶', 'ìŠ¤ë§ˆíŠ¸í°', 'íƒœë¸”ë¦¿'],
        'social_hashtags': ['ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'ë””ì§€í„¸ë§ˆì¼€íŒ…'],
        'platforms': ['naver', 'google']
    }
    
    try:
        # ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        results_df = pipeline.run_monitoring_campaign(config)
        
        # ê²°ê³¼ ì €ì¥
        pipeline.save_results('competitor_ads_monitoring.csv')
        
        # ê²°ê³¼ ë¶„ì„
        print(f"ì´ {len(results_df)} ê°œì˜ ê´‘ê³  ìˆ˜ì§‘ë¨")
        print(f"í”Œë«í¼ë³„ ë¶„í¬: {results_df['platform'].value_counts().to_dict()}")
        
        return results_df
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        pipeline.cleanup()

if __name__ == "__main__":
    results = example_monitoring_campaign()
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ê²½ìŸì‚¬ ê´‘ê³  ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**
2. **ê°€ê²© ì¶”ì  ë° ì•Œë¦¼ ì„œë¹„ìŠ¤**
3. **ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„**
4. **ì‹œì¥ ì¡°ì‚¬ ìë™í™” í”Œë«í¼**