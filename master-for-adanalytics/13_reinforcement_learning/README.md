# 13. Reinforcement Learning - ê°•í™”í•™ìŠµ

## ğŸ“š ê³¼ì • ì†Œê°œ
ê°•í™”í•™ìŠµì„ í™œìš©í•œ ê´‘ê³  ìµœì í™”ì™€ ìë™ ì…ì°° ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì •ê³¼ ì¥ê¸° ì„±ê³¼ ìµœì í™”ë¥¼ ìœ„í•œ RL ì•Œê³ ë¦¬ì¦˜ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê´‘ê³  ì…ì°° ìµœì í™” RL ëª¨ë¸
- ë©€í‹° ì•„ì•” ë°´ë”§ ì‹¤í—˜
- ë”¥ QëŸ¬ë‹ êµ¬í˜„
- ì‹¤ì‹œê°„ ì „ëµ ì¡°ì • ì‹œìŠ¤í…œ

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ë©€í‹° ì•„ì•” ë°´ë”§ (MAB) êµ¬í˜„
```python
import numpy as np
from typing import List, Dict, Tuple
from abc import ABC, abstractmethod

class Bandit(ABC):
    """ë°´ë”§ ì•Œê³ ë¦¬ì¦˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, n_arms: int):
        self.n_arms = n_arms
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
        self.t = 0
        
    @abstractmethod
    def select_arm(self) -> int:
        pass
    
    def update(self, arm: int, reward: float):
        self.counts[arm] += 1
        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]
        self.t += 1

class EpsilonGreedy(Bandit):
    """ì—¡ì‹¤ë¡  ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, n_arms: int, epsilon: float = 0.1):
        super().__init__(n_arms)
        self.epsilon = epsilon
        
    def select_arm(self) -> int:
        if np.random.random() > self.epsilon:
            # ê·¸ë¦¬ë””: ìµœê³  ê°€ì¹˜ ì„ íƒ
            return np.argmax(self.values)
        else:
            # íƒí—˜: ëœë¤ ì„ íƒ
            return np.random.randint(self.n_arms)

class UCB(Bandit):
    """Upper Confidence Bound"""
    
    def __init__(self, n_arms: int, c: float = 2.0):
        super().__init__(n_arms)
        self.c = c
        
    def select_arm(self) -> int:
        if self.t < self.n_arms:
            return self.t  # ëª¨ë“  armì„ í•œ ë²ˆì”© ì‹œë„
        
        # UCB ê³„ì‚°
        confidence_bounds = self.values + self.c * np.sqrt(
            np.log(self.t) / (self.counts + 1e-5)
        )
        return np.argmax(confidence_bounds)

class ThompsonSampling(Bandit):
    """í†°ìŠ¨ ìƒ˜í”Œë§ (ë² ì´ì§€ì•ˆ ì ‘ê·¼)"""
    
    def __init__(self, n_arms: int):
        super().__init__(n_arms)
        self.alpha = np.ones(n_arms)  # ì„±ê³µ íšŸìˆ˜ + 1
        self.beta = np.ones(n_arms)   # ì‹¤íŒ¨ íšŸìˆ˜ + 1
        
    def select_arm(self) -> int:
        # ë² íƒ€ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
        samples = np.random.beta(self.alpha, self.beta)
        return np.argmax(samples)
    
    def update(self, arm: int, reward: float):
        super().update(arm, reward)
        if reward > 0:
            self.alpha[arm] += 1
        else:
            self.beta[arm] += 1
```

### ê´‘ê³  ì…ì°° ìµœì í™” ë°´ë”§
```python
class AdBiddingBandit:
    """ê´‘ê³  ì…ì°° ìµœì í™”ë¥¼ ìœ„í•œ ë°´ë”§"""
    
    def __init__(self, bid_ranges: List[Tuple[float, float]], 
                 algorithm: str = 'ucb'):
        self.bid_ranges = bid_ranges
        self.n_arms = len(bid_ranges)
        
        if algorithm == 'epsilon_greedy':
            self.bandit = EpsilonGreedy(self.n_arms, epsilon=0.1)
        elif algorithm == 'ucb':
            self.bandit = UCB(self.n_arms, c=2.0)
        else:
            self.bandit = ThompsonSampling(self.n_arms)
        
        self.performance_history = []
        
    def select_bid(self) -> float:
        """ìµœì  ì…ì°°ê°€ ì„ íƒ"""
        arm = self.bandit.select_arm()
        min_bid, max_bid = self.bid_ranges[arm]
        # ë²”ìœ„ ë‚´ì—ì„œ ëœë¤ ì„ íƒ
        return np.random.uniform(min_bid, max_bid)
    
    def update_performance(self, bid: float, cost: float, 
                          conversions: int, revenue: float):
        """ì„±ê³¼ ì—…ë°ì´íŠ¸"""
        # ì…ì°°ê°€ê°€ ì†í•œ arm ì°¾ê¸°
        arm = self._find_arm_for_bid(bid)
        
        # ROI ê³„ì‚°
        roi = (revenue - cost) / cost if cost > 0 else 0
        
        # ë°´ë”§ ì—…ë°ì´íŠ¸
        self.bandit.update(arm, roi)
        
        # ì´ë ¥ ì €ì¥
        self.performance_history.append({
            'bid': bid,
            'arm': arm,
            'cost': cost,
            'conversions': conversions,
            'revenue': revenue,
            'roi': roi
        })
    
    def _find_arm_for_bid(self, bid: float) -> int:
        for i, (min_bid, max_bid) in enumerate(self.bid_ranges):
            if min_bid <= bid <= max_bid:
                return i
        return 0  # ê¸°ë³¸ê°’
```

### Deep Q-Learning (DQN) êµ¬í˜„
```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

class DQN(nn.Module):
    """Deep Q Network"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
    def forward(self, x):
        return self.network(x)

class DQNAgent:
    """DQN ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_dim: int, action_dim: int, 
                 lr: float = 0.001, gamma: float = 0.99,
                 epsilon: float = 1.0, epsilon_decay: float = 0.995,
                 memory_size: int = 10000):
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01
        
        # ë„¤íŠ¸ì›Œí¬
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        # ê²½í—˜ ì¬ìƒ
        self.memory = deque(maxlen=memory_size)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ë™ê¸°í™”
        self.update_target_network()
        
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
        
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
        
    def act(self, state):
        """í–‰ë™ ì„ íƒ (ì—¡ì‹¤ë¡  ê·¸ë¦¬ë””)"""
        if np.random.random() <= self.epsilon:
            return random.randrange(self.action_dim)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return q_values.argmax().item()
        
    def replay(self, batch_size: int = 32):
        """ê²½í—˜ ì¬ìƒ í•™ìŠµ"""
        if len(self.memory) < batch_size:
            return
            
        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

### ê´‘ê³  í™˜ê²½ ì‹œë®¬ë ˆì´í„°
```python
class AdEnvironment:
    """ê´‘ê³  í™˜ê²½ ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self):
        self.reset()
        
    def reset(self) -> np.ndarray:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.time_step = 0
        self.budget_remaining = 100000  # ì´ˆê¸° ì˜ˆì‚°
        self.current_ctr = 0.02
        self.current_cpm = 1000
        self.market_competition = np.random.uniform(0.5, 1.5)
        
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return np.array([
            self.time_step / 24,  # ì •ê·œí™”ëœ ì‹œê°„ (24ì‹œê°„ ê¸°ì¤€)
            self.budget_remaining / 100000,  # ì •ê·œí™”ëœ ì˜ˆì‚°
            self.current_ctr,
            self.current_cpm / 2000,  # ì •ê·œí™”ëœ CPM
            self.market_competition,
        ])
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """í–‰ë™ ì‹¤í–‰"""
        # í–‰ë™ì„ ì…ì°° ë°°ìˆ˜ë¡œ ë³€í™˜ (0.5x ~ 2.0x)
        bid_multiplier = 0.5 + (action / 9) * 1.5  # 10ê°œ í–‰ë™
        
        bid_amount = self.current_cpm * bid_multiplier
        
        # í™˜ê²½ ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
        win_probability = min(bid_multiplier / self.market_competition, 1.0)
        impressions = int(np.random.poisson(1000 * win_probability))
        
        clicks = np.random.binomial(impressions, self.current_ctr)
        cost = impressions * bid_amount / 1000
        
        # ì˜ˆì‚° ì°¨ê°
        cost = min(cost, self.budget_remaining)
        self.budget_remaining -= cost
        
        # ìˆ˜ìµ ê³„ì‚° (ê°€ì •: í´ë¦­ë‹¹ í‰ê·  ìˆ˜ìµ 1500ì›)
        revenue = clicks * 1500
        
        # ë³´ìƒ ê³„ì‚° (ROI ê¸°ë°˜)
        reward = (revenue - cost) / max(cost, 1) if cost > 0 else 0
        
        # í™˜ê²½ ì—…ë°ì´íŠ¸
        self.time_step += 1
        self._update_market_conditions()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = self.time_step >= 24 or self.budget_remaining <= 0
        
        info = {
            'impressions': impressions,
            'clicks': clicks,
            'cost': cost,
            'revenue': revenue,
            'ctr': clicks / max(impressions, 1),
            'cpm': cost / max(impressions, 1) * 1000
        }
        
        return self._get_state(), reward, done, info
    
    def _update_market_conditions(self):
        """ì‹œì¥ ìƒí™© ì—…ë°ì´íŠ¸"""
        # ì‹œê°„ì— ë”°ë¥¸ CTR ë³€ë™
        hour = self.time_step % 24
        if 9 <= hour <= 18:  # ì—…ë¬´ì‹œê°„
            self.current_ctr = 0.025
        elif 18 <= hour <= 23:  # ì €ë…ì‹œê°„
            self.current_ctr = 0.03
        else:  # ì‹¬ì•¼/ìƒˆë²½
            self.current_ctr = 0.015
            
        # ê²½ìŸ ê°•ë„ ë³€ë™
        self.market_competition += np.random.normal(0, 0.1)
        self.market_competition = np.clip(self.market_competition, 0.3, 2.0)
```

### ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
```python
class RLAdOptimizer:
    """ê°•í™”í•™ìŠµ ê¸°ë°˜ ê´‘ê³  ìµœì í™”ê¸°"""
    
    def __init__(self, state_dim: int = 5, action_dim: int = 10):
        self.env = AdEnvironment()
        self.agent = DQNAgent(state_dim, action_dim)
        self.training_history = []
        
    def train(self, episodes: int = 1000):
        """ëª¨ë¸ í•™ìŠµ"""
        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            total_cost = 0
            total_revenue = 0
            
            while True:
                action = self.agent.act(state)
                next_state, reward, done, info = self.env.step(action)
                
                self.agent.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                total_cost += info['cost']
                total_revenue += info['revenue']
                
                if done:
                    break
            
            # ê²½í—˜ ì¬ìƒ í•™ìŠµ
            if len(self.agent.memory) > 1000:
                self.agent.replay()
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if episode % 100 == 0:
                self.agent.update_target_network()
            
            # ì„±ê³¼ ê¸°ë¡
            self.training_history.append({
                'episode': episode,
                'total_reward': total_reward,
                'total_cost': total_cost,
                'total_revenue': total_revenue,
                'roi': total_revenue / max(total_cost, 1),
                'epsilon': self.agent.epsilon
            })
            
            if episode % 100 == 0:
                avg_reward = np.mean([h['total_reward'] for h in self.training_history[-100:]])
                print(f"Episode {episode}, Avg Reward: {avg_reward:.4f}, "
                      f"Epsilon: {self.agent.epsilon:.4f}")
    
    def optimize_campaign(self, campaign_state: Dict) -> Dict:
        """ì‹¤ì œ ìº í˜ì¸ ìµœì í™” ì¶”ì²œ"""
        state = np.array([
            campaign_state['time_progress'],
            campaign_state['budget_remaining'],
            campaign_state['current_ctr'],
            campaign_state['current_cpm'] / 2000,
            campaign_state['competition_level']
        ])
        
        # ìµœì  í–‰ë™ ì„ íƒ (íƒí—˜ ì—†ì´)
        self.agent.epsilon = 0
        action = self.agent.act(state)
        
        # í–‰ë™ì„ ì‹¤ì œ ì…ì°° ì „ëµìœ¼ë¡œ ë³€í™˜
        bid_multiplier = 0.5 + (action / 9) * 1.5
        
        return {
            'recommended_bid_multiplier': bid_multiplier,
            'action_index': action,
            'confidence': 1 - self.agent.epsilon
        }
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ì‹¤ì‹œê°„ ê´‘ê³  ì…ì°° ìµœì í™” ì‹œìŠ¤í…œ**
2. **A/B í…ŒìŠ¤íŠ¸ ìë™í™” ë°´ë”§**
3. **ì˜ˆì‚° ë°°ë¶„ ìµœì í™” RL ëª¨ë¸**
4. **í¬ë¦¬ì—ì´í‹°ë¸Œ ì„ íƒ ê°•í™”í•™ìŠµ**