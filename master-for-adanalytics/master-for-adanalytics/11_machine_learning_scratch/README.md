# 11. Machine Learning from Scratch - ë¨¸ì‹ ëŸ¬ë‹ ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„

## ğŸ“š ê³¼ì • ì†Œê°œ
ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ë„ëª¨í•©ë‹ˆë‹¤. ê´‘ê³  ë§ˆì¼€íŒ… ë„ë©”ì¸ì— íŠ¹í™”ëœ ML ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ë§Œë“¤ì–´ë´…ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ML ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜í•™ì  ì›ë¦¬ ì™„ë²½ ì´í•´
- NumPyë§Œìœ¼ë¡œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- ê´‘ê³  ë„ë©”ì¸ íŠ¹í™” ìµœì í™”
- í”„ë¡œë•ì…˜ ë ˆë²¨ ì½”ë“œ ì‘ì„±

## ğŸ“– ì»¤ë¦¬í˜ëŸ¼

### Chapter 01-02: ì„ í˜•ëŒ€ìˆ˜ & ë¯¸ì ë¶„ ê¸°ì´ˆ
```python
# ê´‘ê³  CTR ì˜ˆì¸¡ì„ ìœ„í•œ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„
class GradientDescent:
    def __init__(self, learning_rate=0.01):
        self.lr = learning_rate
    
    def compute_gradient(self, X, y, weights):
        m = len(y)
        predictions = X.dot(weights)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        return gradient
```

### Chapter 03-04: ì„ í˜•/ë¡œì§€ìŠ¤í‹± íšŒê·€
- CTR ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„
- ê´‘ê³  ë¹„ìš© ìµœì í™”
- ì •ê·œí™” ê¸°ë²• ì ìš©

### Chapter 05-08: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸
- ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ë¡œ ê³ ê° ì„¸ë¶„í™”
- ëœë¤í¬ë ˆìŠ¤íŠ¸ ì•™ìƒë¸”
- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… êµ¬í˜„

### Chapter 09-10: í´ëŸ¬ìŠ¤í„°ë§
- K-meansë¡œ ê³ ê° êµ°ì§‘í™”
- DBSCANìœ¼ë¡œ ì´ìƒ íƒì§€

### Chapter 11-12: ì‹ ê²½ë§ ê¸°ì´ˆ
- í¼ì…‰íŠ¸ë¡ ë¶€í„° ì‹œì‘
- ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- ê´‘ê³  ì¶”ì²œ ì‹ ê²½ë§

### Chapter 13-15: ìµœì í™” & ì •ê·œí™”
- Adam, RMSprop êµ¬í˜„
- Dropout, L1/L2 ì •ê·œí™”
- ì¡°ê¸° ì¢…ë£Œ ì „ëµ

### Chapter 16-17: ì•™ìƒë¸” ê¸°ë²•
- ë°°ê¹… & ë¶€ìŠ¤íŒ…
- ìŠ¤íƒœí‚¹ êµ¬í˜„

### Chapter 18-20: ë§ˆì¼€íŒ… ML íŠ¹í™”
- LTV ì˜ˆì¸¡ ëª¨ë¸
- ì´íƒˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- ê´‘ê³  ì…ì°° ìµœì í™”

## ğŸš€ í•µì‹¬ í”„ë¡œì íŠ¸
1. **CTR ì˜ˆì¸¡ ì—”ì§„ (ìŠ¤í¬ë˜ì¹˜)**
2. **ê³ ê° ì„¸ë¶„í™” ì‹œìŠ¤í…œ**
3. **ê´‘ê³  ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜**
4. **ì‹¤ì‹œê°„ ì…ì°° ìµœì í™”ê¸°**

## ğŸ’¡ êµ¬í˜„ ì˜ˆì‹œ: CTR ì˜ˆì¸¡ ëª¨ë¸

```python
import numpy as np

class CTRPredictor:
    """ê´‘ê³  CTR ì˜ˆì¸¡ ëª¨ë¸ (ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„)"""
    
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.lr = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None
        
    def sigmoid(self, z):
        """ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def fit(self, X, y):
        """ëª¨ë¸ í•™ìŠµ"""
        m, n = X.shape
        self.weights = np.zeros(n)
        self.bias = 0
        
        for epoch in range(self.epochs):
            # Forward propagation
            z = X.dot(self.weights) + self.bias
            predictions = self.sigmoid(z)
            
            # Compute loss
            loss = -np.mean(y * np.log(predictions + 1e-7) + 
                           (1 - y) * np.log(1 - predictions + 1e-7))
            
            # Backward propagation
            dz = predictions - y
            dw = (1/m) * X.T.dot(dz)
            db = (1/m) * np.sum(dz)
            
            # Update parameters
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def predict_proba(self, X):
        """CTR í™•ë¥  ì˜ˆì¸¡"""
        z = X.dot(self.weights) + self.bias
        return self.sigmoid(z)
    
    def predict(self, X, threshold=0.5):
        """í´ë¦­ ì—¬ë¶€ ì˜ˆì¸¡"""
        return self.predict_proba(X) >= threshold

# ì‚¬ìš© ì˜ˆì‹œ
# ê´‘ê³  íŠ¹ì„±: [ë…¸ì¶œì‹œê°„, ìœ„ì¹˜ì ìˆ˜, ì‚¬ìš©ìê´€ë ¨ë„, ê´‘ê³ í’ˆì§ˆì ìˆ˜]
X_train = np.array([
    [0.8, 0.9, 0.7, 0.8],  # ë†’ì€ CTR ì˜ˆìƒ
    [0.2, 0.3, 0.1, 0.4],  # ë‚®ì€ CTR ì˜ˆìƒ
    # ... ë” ë§ì€ ë°ì´í„°
])
y_train = np.array([1, 0, ...])  # ì‹¤ì œ í´ë¦­ ì—¬ë¶€

model = CTRPredictor(learning_rate=0.1, epochs=1000)
model.fit(X_train, y_train)

# ìƒˆë¡œìš´ ê´‘ê³ ì˜ CTR ì˜ˆì¸¡
new_ad = np.array([[0.7, 0.8, 0.6, 0.9]])
ctr_probability = model.predict_proba(new_ad)
print(f"ì˜ˆìƒ CTR: {ctr_probability[0]:.2%}")
```

---

ë‹¤ìŒ: [Chapter 01: Linear Algebra with NumPy â†’](01_linear_algebra_numpy/README.md)