# Chapter 03: Multiprocessing - ë©€í‹°í”„ë¡œì„¸ì‹±

## ğŸ“š í•™ìŠµ ëª©í‘œ
- í”„ë¡œì„¸ìŠ¤ì™€ ìŠ¤ë ˆë“œì˜ ì°¨ì´ì  ì´í•´
- Python GIL(Global Interpreter Lock) ì´í•´
- multiprocessing ëª¨ë“ˆì„ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
- ëŒ€ìš©ëŸ‰ ê´‘ê³  ë°ì´í„° ì²˜ë¦¬ ìµœì í™”

## ğŸ“– ì´ë¡ : ë³‘ë ¬ ì²˜ë¦¬ ê¸°ì´ˆ

### 1. í”„ë¡œì„¸ìŠ¤ vs ìŠ¤ë ˆë“œ

```python
import time
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# CPU ì§‘ì•½ì  ì‘ì—… ì˜ˆì‹œ
def cpu_bound_task(n):
    """CPU ì§‘ì•½ì  ì‘ì—… - ì†Œìˆ˜ ê³„ì‚°"""
    count = 0
    for i in range(n):
        for j in range(2, int(i**0.5) + 1):
            if i % j == 0:
                break
        else:
            if i > 1:
                count += 1
    return count

# I/O ì§‘ì•½ì  ì‘ì—… ì˜ˆì‹œ
def io_bound_task(url):
    """I/O ì§‘ì•½ì  ì‘ì—… - ë„¤íŠ¸ì›Œí¬ ìš”ì²­"""
    import requests
    response = requests.get(url)
    return len(response.content)

# ì„±ëŠ¥ ë¹„êµ
def compare_performance():
    """ìŠ¤ë ˆë“œ vs í”„ë¡œì„¸ìŠ¤ ì„±ëŠ¥ ë¹„êµ"""
    numbers = [100000] * 4
    
    # ìˆœì°¨ ì²˜ë¦¬
    start = time.time()
    results = [cpu_bound_task(n) for n in numbers]
    sequential_time = time.time() - start
    print(f"ìˆœì°¨ ì²˜ë¦¬: {sequential_time:.2f}ì´ˆ")
    
    # ìŠ¤ë ˆë“œ í’€ (GILë¡œ ì¸í•´ CPU ì‘ì—…ì—ì„œëŠ” ë¹„íš¨ìœ¨ì )
    start = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_bound_task, numbers))
    thread_time = time.time() - start
    print(f"ìŠ¤ë ˆë“œ í’€: {thread_time:.2f}ì´ˆ")
    
    # í”„ë¡œì„¸ìŠ¤ í’€ (ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬)
    start = time.time()
    with ProcessPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(cpu_bound_task, numbers))
    process_time = time.time() - start
    print(f"í”„ë¡œì„¸ìŠ¤ í’€: {process_time:.2f}ì´ˆ")
```

### 2. multiprocessing ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import multiprocessing as mp
from typing import List, Dict, Tuple
import pandas as pd
import numpy as np

class AdDataProcessor:
    """ê´‘ê³  ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, num_processes: int = None):
        self.num_processes = num_processes or mp.cpu_count()
    
    def process_campaign_data(self, campaign_ids: List[str]) -> Dict:
        """ìº í˜ì¸ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬"""
        # ì‘ì—…ì„ í”„ë¡œì„¸ìŠ¤ ìˆ˜ë§Œí¼ ë¶„í• 
        chunks = self._split_into_chunks(campaign_ids, self.num_processes)
        
        # í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„±
        with mp.Pool(processes=self.num_processes) as pool:
            # ê° ì²­í¬ë¥¼ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì²˜ë¦¬
            results = pool.map(self._process_campaign_chunk, chunks)
        
        # ê²°ê³¼ ë³‘í•©
        return self._merge_results(results)
    
    def _split_into_chunks(self, data: List, n: int) -> List[List]:
        """ë°ì´í„°ë¥¼ nê°œì˜ ì²­í¬ë¡œ ë¶„í• """
        chunk_size = len(data) // n
        chunks = []
        
        for i in range(n):
            start = i * chunk_size
            end = start + chunk_size if i < n - 1 else len(data)
            chunks.append(data[start:end])
        
        return chunks
    
    def _process_campaign_chunk(self, campaign_ids: List[str]) -> Dict:
        """ìº í˜ì¸ ì²­í¬ ì²˜ë¦¬"""
        results = {}
        
        for campaign_id in campaign_ids:
            # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
            campaign_data = self._fetch_campaign_data(campaign_id)
            
            # ë³µì¡í•œ ê³„ì‚° ìˆ˜í–‰
            metrics = {
                'impressions': campaign_data['impressions'],
                'clicks': campaign_data['clicks'],
                'conversions': campaign_data['conversions'],
                'ctr': campaign_data['clicks'] / campaign_data['impressions'],
                'cvr': campaign_data['conversions'] / campaign_data['clicks'],
                'cost': campaign_data['cost'],
                'revenue': campaign_data['revenue'],
                'roas': campaign_data['revenue'] / campaign_data['cost']
            }
            
            results[campaign_id] = metrics
        
        return results
    
    def _fetch_campaign_data(self, campaign_id: str) -> Dict:
        """ìº í˜ì¸ ë°ì´í„° ì¡°íšŒ (ì‹œë®¬ë ˆì´ì…˜)"""
        np.random.seed(hash(campaign_id) % 1000)
        return {
            'impressions': np.random.randint(10000, 1000000),
            'clicks': np.random.randint(100, 10000),
            'conversions': np.random.randint(10, 1000),
            'cost': np.random.uniform(1000, 100000),
            'revenue': np.random.uniform(5000, 500000)
        }
    
    def _merge_results(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ë³‘í•©"""
        merged = {}
        for result in results:
            merged.update(result)
        return merged
```

### 3. í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ 

```python
from multiprocessing import Queue, Pipe, Value, Array, Lock
import time

class AdBidProcessor:
    """ì‹¤ì‹œê°„ ê´‘ê³  ì…ì°° ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.bid_queue = mp.Queue()
        self.result_queue = mp.Queue()
        self.stats = mp.Manager().dict()
        self.lock = mp.Lock()
    
    def bid_worker(self, worker_id: int):
        """ì…ì°° ì²˜ë¦¬ ì›Œì»¤"""
        processed = 0
        
        while True:
            try:
                # ì…ì°° ìš”ì²­ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                bid_request = self.bid_queue.get(timeout=1)
                
                if bid_request is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                # ì…ì°° ì²˜ë¦¬
                result = self._process_bid(bid_request)
                
                # ê²°ê³¼ ì „ì†¡
                self.result_queue.put(result)
                
                # í†µê³„ ì—…ë°ì´íŠ¸ (ë™ê¸°í™” í•„ìš”)
                with self.lock:
                    processed += 1
                    self.stats[f'worker_{worker_id}_processed'] = processed
                    
            except mp.queues.Empty:
                continue
    
    def _process_bid(self, bid_request: Dict) -> Dict:
        """ì…ì°° ì²˜ë¦¬ ë¡œì§"""
        # ë³µì¡í•œ ì…ì°° ê³„ì‚°
        time.sleep(0.01)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        
        bid_price = self._calculate_bid_price(
            bid_request['user_profile'],
            bid_request['ad_inventory'],
            bid_request['context']
        )
        
        return {
            'request_id': bid_request['request_id'],
            'bid_price': bid_price,
            'timestamp': time.time()
        }
    
    def _calculate_bid_price(self, user_profile: Dict, 
                           ad_inventory: Dict, 
                           context: Dict) -> float:
        """ì…ì°°ê°€ ê³„ì‚°"""
        base_bid = ad_inventory.get('floor_price', 100)
        
        # ì‚¬ìš©ì ê°€ì¹˜ ì ìˆ˜
        user_value = user_profile.get('ltv_score', 0.5)
        
        # ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜
        context_score = context.get('relevance_score', 0.5)
        
        # ìµœì¢… ì…ì°°ê°€
        bid_price = base_bid * (1 + user_value) * (1 + context_score)
        
        return min(bid_price, ad_inventory.get('max_bid', 1000))
    
    def process_bid_stream(self, bid_requests: List[Dict], 
                          num_workers: int = 4) -> List[Dict]:
        """ì…ì°° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
        workers = []
        for i in range(num_workers):
            p = mp.Process(target=self.bid_worker, args=(i,))
            p.start()
            workers.append(p)
        
        # ì…ì°° ìš”ì²­ ì „ì†¡
        for request in bid_requests:
            self.bid_queue.put(request)
        
        # ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
        for _ in range(num_workers):
            self.bid_queue.put(None)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for _ in range(len(bid_requests)):
            result = self.result_queue.get()
            results.append(result)
        
        # ì›Œì»¤ ì¢…ë£Œ ëŒ€ê¸°
        for p in workers:
            p.join()
        
        # í†µê³„ ì¶œë ¥
        print("ì²˜ë¦¬ í†µê³„:", dict(self.stats))
        
        return results
```

## ğŸ› ï¸ ì‹¤ìŠµ: ëŒ€ìš©ëŸ‰ ê´‘ê³  ë°ì´í„° ì²˜ë¦¬

### ì‹¤ìŠµ 1: ë³‘ë ¬ ë°ì´í„° ì§‘ê³„

```python
import pandas as pd
import numpy as np
from functools import partial
from multiprocessing import Pool
import time

class ParallelAdAggregator:
    """ë³‘ë ¬ ê´‘ê³  ë°ì´í„° ì§‘ê³„ê¸°"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.num_cores = mp.cpu_count()
    
    def aggregate_campaign_performance(self, 
                                     start_date: str, 
                                     end_date: str) -> pd.DataFrame:
        """ìº í˜ì¸ ì„±ê³¼ ë³‘ë ¬ ì§‘ê³„"""
        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        date_range = pd.date_range(start_date, end_date)
        
        # ë‚ ì§œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        date_chunks = np.array_split(date_range, self.num_cores)
        
        # ë¶€ë¶„ í•¨ìˆ˜ ìƒì„± (ì¶”ê°€ ì¸ì ê³ ì •)
        process_func = partial(
            self._process_date_chunk,
            data_path=self.data_path
        )
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with Pool(processes=self.num_cores) as pool:
            chunk_results = pool.map(process_func, date_chunks)
        
        # ê²°ê³¼ ë³‘í•©
        final_result = pd.concat(chunk_results, ignore_index=True)
        
        # ìµœì¢… ì§‘ê³„
        aggregated = final_result.groupby('campaign_id').agg({
            'impressions': 'sum',
            'clicks': 'sum',
            'conversions': 'sum',
            'cost': 'sum',
            'revenue': 'sum'
        }).reset_index()
        
        # íŒŒìƒ ì§€í‘œ ê³„ì‚°
        aggregated['ctr'] = aggregated['clicks'] / aggregated['impressions']
        aggregated['cvr'] = aggregated['conversions'] / aggregated['clicks']
        aggregated['cpc'] = aggregated['cost'] / aggregated['clicks']
        aggregated['roas'] = aggregated['revenue'] / aggregated['cost']
        
        return aggregated
    
    @staticmethod
    def _process_date_chunk(dates: pd.DatetimeIndex, 
                           data_path: str) -> pd.DataFrame:
        """ë‚ ì§œ ì²­í¬ ì²˜ë¦¬"""
        chunk_data = []
        
        for date in dates:
            # ì¼ë³„ ë°ì´í„° ë¡œë“œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬)
            daily_data = ParallelAdAggregator._load_daily_data(
                data_path, date
            )
            chunk_data.append(daily_data)
        
        return pd.concat(chunk_data, ignore_index=True)
    
    @staticmethod
    def _load_daily_data(data_path: str, date: pd.Timestamp) -> pd.DataFrame:
        """ì¼ë³„ ë°ì´í„° ë¡œë“œ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” íŒŒì¼ì´ë‚˜ DBì—ì„œ ë¡œë“œ
        num_campaigns = 100
        
        data = {
            'date': [date] * num_campaigns,
            'campaign_id': [f'camp_{i}' for i in range(num_campaigns)],
            'impressions': np.random.randint(1000, 100000, num_campaigns),
            'clicks': np.random.randint(10, 1000, num_campaigns),
            'conversions': np.random.randint(1, 100, num_campaigns),
            'cost': np.random.uniform(100, 10000, num_campaigns),
            'revenue': np.random.uniform(500, 50000, num_campaigns)
        }
        
        return pd.DataFrame(data)

# ë³‘ë ¬ ì²˜ë¦¬ vs ìˆœì°¨ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ
def benchmark_aggregation():
    """ì§‘ê³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    aggregator = ParallelAdAggregator('dummy_path')
    
    # ë³‘ë ¬ ì²˜ë¦¬
    start = time.time()
    parallel_result = aggregator.aggregate_campaign_performance(
        '2024-01-01', '2024-01-31'
    )
    parallel_time = time.time() - start
    
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„: {parallel_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ ìº í˜ì¸ ìˆ˜: {len(parallel_result)}")
    print(f"í‰ê·  ROAS: {parallel_result['roas'].mean():.2f}")
```

### ì‹¤ìŠµ 2: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

```python
from multiprocessing import Process, Queue, Event
import queue
import json

class RealTimeAdStreamProcessor:
    """ì‹¤ì‹œê°„ ê´‘ê³  ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, num_workers: int = 4):
        self.num_workers = num_workers
        self.input_queue = mp.Queue(maxsize=10000)
        self.output_queue = mp.Queue(maxsize=10000)
        self.error_queue = mp.Queue(maxsize=1000)
        self.shutdown_event = mp.Event()
        self.stats = mp.Manager().dict({
            'processed': 0,
            'errors': 0,
            'filtered': 0
        })
    
    def stream_processor(self, worker_id: int):
        """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì›Œì»¤"""
        local_stats = {'processed': 0, 'errors': 0, 'filtered': 0}
        
        while not self.shutdown_event.is_set():
            try:
                # ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                event = self.input_queue.get(timeout=0.1)
                
                # ì´ë²¤íŠ¸ ì²˜ë¦¬
                try:
                    processed_event = self._process_ad_event(event)
                    
                    if processed_event:
                        self.output_queue.put(processed_event)
                        local_stats['processed'] += 1
                    else:
                        local_stats['filtered'] += 1
                        
                except Exception as e:
                    self.error_queue.put({
                        'worker_id': worker_id,
                        'event': event,
                        'error': str(e)
                    })
                    local_stats['errors'] += 1
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
        
        # ìµœì¢… í†µê³„ ì—…ë°ì´íŠ¸
        for key, value in local_stats.items():
            self.stats[key] = self.stats.get(key, 0) + value
    
    def _process_ad_event(self, event: Dict) -> Optional[Dict]:
        """ê´‘ê³  ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = event.get('type')
        
        if event_type == 'impression':
            return self._process_impression(event)
        elif event_type == 'click':
            return self._process_click(event)
        elif event_type == 'conversion':
            return self._process_conversion(event)
        else:
            return None
    
    def _process_impression(self, event: Dict) -> Dict:
        """ë…¸ì¶œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # ë´‡ í•„í„°ë§
        if self._is_bot(event.get('user_agent', '')):
            return None
        
        # ìœ íš¨ì„± ê²€ì¦
        if not all(k in event for k in ['ad_id', 'user_id', 'timestamp']):
            return None
        
        # ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ë°˜í™˜
        return {
            'type': 'processed_impression',
            'ad_id': event['ad_id'],
            'user_id': event['user_id'],
            'timestamp': event['timestamp'],
            'cost': self._calculate_impression_cost(event)
        }
    
    def _process_click(self, event: Dict) -> Dict:
        """í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # ì¤‘ë³µ í´ë¦­ í•„í„°ë§
        if self._is_duplicate_click(event):
            return None
        
        return {
            'type': 'processed_click',
            'ad_id': event['ad_id'],
            'user_id': event['user_id'],
            'timestamp': event['timestamp'],
            'cost': self._calculate_click_cost(event)
        }
    
    def _process_conversion(self, event: Dict) -> Dict:
        """ì „í™˜ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # ì–´íŠ¸ë¦¬ë·°ì…˜ ìœˆë„ìš° í™•ì¸
        if not self._within_attribution_window(event):
            return None
        
        return {
            'type': 'processed_conversion',
            'ad_id': event['ad_id'],
            'user_id': event['user_id'],
            'timestamp': event['timestamp'],
            'value': event.get('value', 0),
            'attribution': self._calculate_attribution(event)
        }
    
    def _is_bot(self, user_agent: str) -> bool:
        """ë´‡ íƒì§€"""
        bot_patterns = ['bot', 'crawler', 'spider', 'scraper']
        return any(pattern in user_agent.lower() for pattern in bot_patterns)
    
    def _is_duplicate_click(self, event: Dict) -> bool:
        """ì¤‘ë³µ í´ë¦­ í™•ì¸"""
        # ì‹¤ì œë¡œëŠ” Redis ë“±ì„ ì‚¬ìš©í•˜ì—¬ í™•ì¸
        return False
    
    def _within_attribution_window(self, event: Dict) -> bool:
        """ì–´íŠ¸ë¦¬ë·°ì…˜ ìœˆë„ìš° ë‚´ í™•ì¸"""
        # ì‹¤ì œë¡œëŠ” í´ë¦­ ì‹œê°„ê³¼ ë¹„êµ
        return True
    
    def _calculate_impression_cost(self, event: Dict) -> float:
        """ë…¸ì¶œ ë¹„ìš© ê³„ì‚°"""
        return event.get('bid_price', 0) / 1000  # CPM
    
    def _calculate_click_cost(self, event: Dict) -> float:
        """í´ë¦­ ë¹„ìš© ê³„ì‚°"""
        return event.get('bid_price', 0)  # CPC
    
    def _calculate_attribution(self, event: Dict) -> str:
        """ì–´íŠ¸ë¦¬ë·°ì…˜ ê³„ì‚°"""
        return 'last_click'  # ë‹¨ìˆœí™”ëœ ì–´íŠ¸ë¦¬ë·°ì…˜
    
    def start_processing(self):
        """ì²˜ë¦¬ ì‹œì‘"""
        # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
        workers = []
        for i in range(self.num_workers):
            p = Process(target=self.stream_processor, args=(i,))
            p.start()
            workers.append(p)
        
        return workers
    
    def stop_processing(self, workers: List[Process]):
        """ì²˜ë¦¬ ì¤‘ì§€"""
        # ì¢…ë£Œ ì‹ í˜¸
        self.shutdown_event.set()
        
        # ì›Œì»¤ ì¢…ë£Œ ëŒ€ê¸°
        for p in workers:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        print("ì²˜ë¦¬ í†µê³„:")
        print(f"  ì²˜ë¦¬ë¨: {self.stats['processed']}")
        print(f"  í•„í„°ë¨: {self.stats['filtered']}")
        print(f"  ì˜¤ë¥˜: {self.stats['errors']}")
```

### ì‹¤ìŠµ 3: ë¶„ì‚° ë¨¸ì‹ ëŸ¬ë‹ í›ˆë ¨

```python
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np

class DistributedAdModelTrainer:
    """ë¶„ì‚° ê´‘ê³  ëª¨ë¸ í›ˆë ¨ê¸°"""
    
    def __init__(self, n_estimators: int = 100):
        self.n_estimators = n_estimators
        self.n_jobs = mp.cpu_count()
    
    def train_click_prediction_model(self, 
                                   X: np.ndarray, 
                                   y: np.ndarray) -> RandomForestClassifier:
        """í´ë¦­ ì˜ˆì¸¡ ëª¨ë¸ ë³‘ë ¬ í›ˆë ¨"""
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ê° í”„ë¡œì„¸ìŠ¤ê°€ í›ˆë ¨í•  íŠ¸ë¦¬ ìˆ˜ ê³„ì‚°
        trees_per_process = self.n_estimators // self.n_jobs
        
        # ë³‘ë ¬ í›ˆë ¨ ì‘ì—… ìƒì„±
        train_tasks = []
        for i in range(self.n_jobs):
            n_trees = trees_per_process
            if i == self.n_jobs - 1:
                # ë§ˆì§€ë§‰ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚˜ë¨¸ì§€ íŠ¸ë¦¬ë„ í›ˆë ¨
                n_trees += self.n_estimators % self.n_jobs
            
            task = (X_train, y_train, n_trees, i)
            train_tasks.append(task)
        
        # ë³‘ë ¬ í›ˆë ¨
        with Pool(processes=self.n_jobs) as pool:
            sub_models = pool.map(self._train_sub_model, train_tasks)
        
        # ëª¨ë¸ ì•™ìƒë¸”
        ensemble_model = self._ensemble_models(sub_models)
        
        # í‰ê°€
        score = ensemble_model.score(X_test, y_test)
        print(f"ëª¨ë¸ ì •í™•ë„: {score:.4f}")
        
        return ensemble_model
    
    @staticmethod
    def _train_sub_model(args: Tuple) -> RandomForestClassifier:
        """ì„œë¸Œ ëª¨ë¸ í›ˆë ¨"""
        X_train, y_train, n_trees, seed = args
        
        model = RandomForestClassifier(
            n_estimators=n_trees,
            random_state=seed,
            n_jobs=1  # ê° í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
        )
        
        model.fit(X_train, y_train)
        return model
    
    def _ensemble_models(self, 
                        sub_models: List[RandomForestClassifier]
                        ) -> RandomForestClassifier:
        """ì„œë¸Œ ëª¨ë¸ ì•™ìƒë¸”"""
        # ëª¨ë“  íŠ¸ë¦¬ ìˆ˜ì§‘
        all_estimators = []
        for model in sub_models:
            all_estimators.extend(model.estimators_)
        
        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
        ensemble = RandomForestClassifier(n_estimators=1)
        ensemble.fit([[0]], [0])  # ë”ë¯¸ í•™ìŠµ
        ensemble.estimators_ = all_estimators
        ensemble.n_estimators = len(all_estimators)
        
        # ê¸°íƒ€ ì†ì„± ë³µì‚¬
        ensemble.classes_ = sub_models[0].classes_
        ensemble.n_classes_ = sub_models[0].n_classes_
        ensemble.n_features_in_ = sub_models[0].n_features_in_
        
        return ensemble
    
    def parallel_feature_importance(self, 
                                  model: RandomForestClassifier,
                                  feature_names: List[str]) -> pd.DataFrame:
        """ë³‘ë ¬ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""
        # ê° íŠ¸ë¦¬ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°ì„ ë³‘ë ¬í™”
        with Pool(processes=self.n_jobs) as pool:
            tree_importances = pool.map(
                self._get_tree_importance,
                model.estimators_
            )
        
        # í‰ê·  ê³„ì‚°
        avg_importance = np.mean(tree_importances, axis=0)
        
        # ê²°ê³¼ ì •ë¦¬
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': avg_importance
        }).sort_values('importance', ascending=False)
        
        return importance_df
    
    @staticmethod
    def _get_tree_importance(tree):
        """ê°œë³„ íŠ¸ë¦¬ì˜ íŠ¹ì„± ì¤‘ìš”ë„"""
        return tree.feature_importances_
```

## ğŸš€ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ê´‘ê³  ì…ì°° ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
real_time_bidding_system/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ bidder/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bid_processor.py
â”‚   â”œâ”€â”€ bid_strategy.py
â”‚   â””â”€â”€ feature_extractor.py
â”œâ”€â”€ stream/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ event_processor.py
â”‚   â””â”€â”€ data_pipeline.py
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ real_time_analytics.py
â”‚   â””â”€â”€ batch_analytics.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ click_predictor.py
â”‚   â””â”€â”€ bid_optimizer.py
â””â”€â”€ main.py
```

### í†µí•© ì‹œìŠ¤í…œ êµ¬í˜„

```python
# main.py
import multiprocessing as mp
from multiprocessing import Process, Queue, Manager
import time
import json
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np

class RealTimeBiddingSystem:
    """ì‹¤ì‹œê°„ ì…ì°° ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.manager = Manager()
        
        # ê³µìœ  ìì›
        self.bid_requests = Queue(maxsize=100000)
        self.bid_responses = Queue(maxsize=100000)
        self.analytics_queue = Queue(maxsize=50000)
        
        # ê³µìœ  ìƒíƒœ
        self.system_stats = self.manager.dict({
            'total_requests': 0,
            'total_bids': 0,
            'total_wins': 0,
            'total_spend': 0.0,
            'avg_response_time': 0.0
        })
        
        # í”„ë¡œì„¸ìŠ¤ í’€
        self.bidder_processes = []
        self.analytics_processes = []
        self.is_running = mp.Event()
    
    def bidder_worker(self, worker_id: int):
        """ì…ì°° ì›Œì»¤ í”„ë¡œì„¸ìŠ¤"""
        local_stats = {
            'requests': 0,
            'bids': 0,
            'total_time': 0
        }
        
        while self.is_running.is_set():
            try:
                # ì…ì°° ìš”ì²­ ë°›ê¸°
                request = self.bid_requests.get(timeout=0.1)
                start_time = time.time()
                
                # ì…ì°° ê²°ì •
                bid_response = self._process_bid_request(request, worker_id)
                
                # ì‘ë‹µ ì „ì†¡
                if bid_response:
                    self.bid_responses.put(bid_response)
                    local_stats['bids'] += 1
                
                # ë¶„ì„ìš© ë°ì´í„° ì „ì†¡
                self.analytics_queue.put({
                    'type': 'bid_event',
                    'worker_id': worker_id,
                    'request_id': request['id'],
                    'bid': bid_response is not None,
                    'response_time': time.time() - start_time
                })
                
                local_stats['requests'] += 1
                local_stats['total_time'] += time.time() - start_time
                
            except mp.queues.Empty:
                continue
            except Exception as e:
                print(f"Bidder {worker_id} error: {e}")
        
        # ìµœì¢… í†µê³„ ì—…ë°ì´íŠ¸
        avg_time = local_stats['total_time'] / max(local_stats['requests'], 1)
        print(f"Bidder {worker_id} - Requests: {local_stats['requests']}, "
              f"Bids: {local_stats['bids']}, Avg Time: {avg_time:.3f}s")
    
    def _process_bid_request(self, request: Dict, worker_id: int) -> Optional[Dict]:
        """ì…ì°° ìš”ì²­ ì²˜ë¦¬"""
        # íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_features(request)
        
        # í´ë¦­ í™•ë¥  ì˜ˆì¸¡
        click_prob = self._predict_click_probability(features)
        
        # ì…ì°°ê°€ ê³„ì‚°
        bid_price = self._calculate_bid_price(
            click_prob,
            request.get('floor_price', 0),
            request.get('user_value', 1.0)
        )
        
        # ì…ì°° ê²°ì •
        if bid_price > request.get('floor_price', 0):
            return {
                'request_id': request['id'],
                'bid_price': bid_price,
                'worker_id': worker_id,
                'timestamp': time.time(),
                'click_prob': click_prob
            }
        
        return None
    
    def _extract_features(self, request: Dict) -> np.ndarray:
        """íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        # ì‚¬ìš©ì íŠ¹ì§•
        user = request.get('user', {})
        features.extend([
            user.get('age', 0) / 100,
            1 if user.get('gender') == 'M' else 0,
            len(user.get('interests', [])) / 10
        ])
        
        # ê´‘ê³  ìŠ¬ë¡¯ íŠ¹ì§•
        slot = request.get('slot', {})
        features.extend([
            slot.get('width', 0) / 1000,
            slot.get('height', 0) / 1000,
            1 if slot.get('position') == 'above_fold' else 0
        ])
        
        # ì»¨í…ìŠ¤íŠ¸ íŠ¹ì§•
        context = request.get('context', {})
        features.extend([
            context.get('hour', 0) / 24,
            context.get('day_of_week', 0) / 7,
            1 if context.get('device') == 'mobile' else 0
        ])
        
        return np.array(features)
    
    def _predict_click_probability(self, features: np.ndarray) -> float:
        """í´ë¦­ í™•ë¥  ì˜ˆì¸¡ (ê°„ë‹¨í•œ ëª¨ë¸)"""
        # ì‹¤ì œë¡œëŠ” í›ˆë ¨ëœ ML ëª¨ë¸ ì‚¬ìš©
        weights = np.random.rand(len(features))
        logit = np.dot(features, weights)
        return 1 / (1 + np.exp(-logit))
    
    def _calculate_bid_price(self, click_prob: float, 
                           floor_price: float,
                           user_value: float) -> float:
        """ì…ì°°ê°€ ê³„ì‚°"""
        # ì˜ˆìƒ ê°€ì¹˜ = í´ë¦­ í™•ë¥  * ì‚¬ìš©ì ê°€ì¹˜
        expected_value = click_prob * user_value * 1000  # CPM ê¸°ì¤€
        
        # ë§ˆì§„ ê³ ë ¤
        bid_price = expected_value * 0.8
        
        # ìµœì†Œ/ìµœëŒ€ ì œì•½
        bid_price = max(bid_price, floor_price)
        bid_price = min(bid_price, self.config.get('max_bid', 1000))
        
        return bid_price
    
    def analytics_worker(self):
        """ë¶„ì„ ì›Œì»¤ í”„ë¡œì„¸ìŠ¤"""
        batch = []
        batch_size = 1000
        last_flush = time.time()
        
        while self.is_running.is_set() or not self.analytics_queue.empty():
            try:
                # ì´ë²¤íŠ¸ ìˆ˜ì§‘
                event = self.analytics_queue.get(timeout=0.1)
                batch.append(event)
                
                # ë°°ì¹˜ ì²˜ë¦¬
                if len(batch) >= batch_size or time.time() - last_flush > 5:
                    self._process_analytics_batch(batch)
                    batch = []
                    last_flush = time.time()
                    
            except mp.queues.Empty:
                # íƒ€ì„ì•„ì›ƒ ì‹œ ë°°ì¹˜ ì²˜ë¦¬
                if batch:
                    self._process_analytics_batch(batch)
                    batch = []
                    last_flush = time.time()
            except Exception as e:
                print(f"Analytics error: {e}")
    
    def _process_analytics_batch(self, batch: List[Dict]):
        """ë¶„ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        # ì§‘ê³„
        total_requests = sum(1 for e in batch if e['type'] == 'bid_event')
        total_bids = sum(1 for e in batch if e.get('bid', False))
        avg_response_time = np.mean([
            e['response_time'] for e in batch 
            if 'response_time' in e
        ])
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.system_stats['total_requests'] += total_requests
        self.system_stats['total_bids'] += total_bids
        self.system_stats['avg_response_time'] = avg_response_time
        
        # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ì‹œìŠ¤í…œ)
        print(f"[Analytics] Batch processed - "
              f"Requests: {total_requests}, "
              f"Bids: {total_bids}, "
              f"Avg Response: {avg_response_time*1000:.1f}ms")
    
    def start(self):
        """ì‹œìŠ¤í…œ ì‹œì‘"""
        self.is_running.set()
        
        # ì…ì°° ì›Œì»¤ ì‹œì‘
        num_bidders = self.config.get('num_bidders', 4)
        for i in range(num_bidders):
            p = Process(target=self.bidder_worker, args=(i,))
            p.start()
            self.bidder_processes.append(p)
        
        # ë¶„ì„ ì›Œì»¤ ì‹œì‘
        analytics_process = Process(target=self.analytics_worker)
        analytics_process.start()
        self.analytics_processes.append(analytics_process)
        
        print(f"RTB System started with {num_bidders} bidders")
    
    def stop(self):
        """ì‹œìŠ¤í…œ ì¤‘ì§€"""
        print("Shutting down RTB System...")
        
        # ì¢…ë£Œ ì‹ í˜¸
        self.is_running.clear()
        
        # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
        for p in self.bidder_processes:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
        
        for p in self.analytics_processes:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
        
        # ìµœì¢… í†µê³„
        print("\n=== Final Statistics ===")
        print(f"Total Requests: {self.system_stats['total_requests']}")
        print(f"Total Bids: {self.system_stats['total_bids']}")
        print(f"Bid Rate: {self.system_stats['total_bids'] / max(self.system_stats['total_requests'], 1) * 100:.1f}%")
        print(f"Avg Response Time: {self.system_stats['avg_response_time']*1000:.1f}ms")
    
    def simulate_bid_stream(self, duration: int = 10):
        """ì…ì°° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜"""
        print(f"Simulating bid stream for {duration} seconds...")
        
        start_time = time.time()
        request_id = 0
        
        while time.time() - start_time < duration:
            # ì…ì°° ìš”ì²­ ìƒì„±
            request = {
                'id': f'req_{request_id}',
                'user': {
                    'age': np.random.randint(18, 65),
                    'gender': np.random.choice(['M', 'F']),
                    'interests': np.random.choice(
                        ['sports', 'fashion', 'tech', 'food'],
                        size=np.random.randint(1, 4)
                    ).tolist()
                },
                'slot': {
                    'width': np.random.choice([300, 728, 970]),
                    'height': np.random.choice([250, 90, 250]),
                    'position': np.random.choice(['above_fold', 'below_fold'])
                },
                'context': {
                    'hour': datetime.now().hour,
                    'day_of_week': datetime.now().weekday(),
                    'device': np.random.choice(['mobile', 'desktop'])
                },
                'floor_price': np.random.uniform(0.1, 5.0),
                'user_value': np.random.uniform(0.5, 10.0)
            }
            
            self.bid_requests.put(request)
            request_id += 1
            
            # QPS ì œì–´
            time.sleep(0.001)  # 1000 QPS
        
        print(f"Generated {request_id} bid requests")

# ì‹œìŠ¤í…œ ì‹¤í–‰
if __name__ == "__main__":
    config = {
        'num_bidders': 8,
        'max_bid': 10.0,
        'batch_size': 1000
    }
    
    rtb_system = RealTimeBiddingSystem(config)
    
    try:
        # ì‹œìŠ¤í…œ ì‹œì‘
        rtb_system.start()
        
        # ì…ì°° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
        rtb_system.simulate_bid_stream(duration=30)
        
        # ì²˜ë¦¬ ëŒ€ê¸°
        time.sleep(5)
        
    finally:
        # ì‹œìŠ¤í…œ ì¢…ë£Œ
        rtb_system.stop()
```

## ğŸ“ ê³¼ì œ

### ê³¼ì œ 1: ë³‘ë ¬ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„
ì—¬ëŸ¬ ìº í˜ì¸ì˜ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”:
- í†µê³„ì  ìœ ì˜ì„± ê²€ì •
- ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
- ë‹¤ì¤‘ ë¹„êµ ë³´ì •

### ê³¼ì œ 2: ë¶„ì‚° CTR ì˜ˆì¸¡ ëª¨ë¸
ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ CTR ì˜ˆì¸¡ ëª¨ë¸ì„ ë¶„ì‚° í›ˆë ¨í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”:
- ë°ì´í„° ë³‘ë ¬í™”
- ëª¨ë¸ ë³‘ë ¬í™”
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ê³¼ì œ 3: ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
ê´‘ê³  íŠ¸ë˜í”½ì˜ ì´ìƒì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒì§€í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”:
- ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
- ì´ìƒ íŒ¨í„´ ê°ì§€
- ì‹¤ì‹œê°„ ì•Œë¦¼

### ê³¼ì œ 4: ë¶„ì‚° ë¦¬í¬íŠ¸ ìƒì„±
ëŒ€ê·œëª¨ ê´‘ê³  ìº í˜ì¸ ë¦¬í¬íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”:
- ë‹¤ì°¨ì› ì§‘ê³„
- ì‹œê°í™” ìƒì„±
- PDF/Excel ì¶œë ¥

## ğŸ”— ì°¸ê³  ìë£Œ
- [Python multiprocessing Documentation](https://docs.python.org/3/library/multiprocessing.html)
- [Parallel and Distributed Computing in Python](https://realpython.com/python-parallel-processing/)
- [High Performance Python](https://www.oreilly.com/library/view/high-performance-python/9781492055013/)
- [Scaling Python](https://scaling-python.com/)

---

ë‹¤ìŒ ì¥: [Chapter 04: Async Programming â†’](../04_async_programming/README.md)