# 18. NLP Text Processing - ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨

## üìö Í≥ºÏ†ï ÏÜåÍ∞ú
ÎßàÏºÄÌåÖ Î∞è Í¥ëÍ≥† ÎèÑÎ©îÏù∏Ïóê ÌäπÌôîÎêú ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ Í∏∞Ïà†ÏùÑ ÌïôÏäµÌï©ÎãàÎã§. Í¥ëÍ≥† Ïπ¥Ìîº Î∂ÑÏÑù, Í∞êÏ†ï Î∂ÑÏÑù, ÌÇ§ÏõåÎìú Ï∂îÏ∂ú, ÏûêÎèô Ïπ¥Ìîº ÏÉùÏÑ±ÍπåÏßÄ Ìè¨Í¥ÑÏ†ÅÏúºÎ°ú Îã§Î£πÎãàÎã§.

## üéØ ÌïôÏäµ Î™©Ìëú
- Í¥ëÍ≥† ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ Î∞è Î∂ÑÏÑù
- Í∞êÏ†ï Î∂ÑÏÑùÏúºÎ°ú Î∏åÎûúÎìú Î™®ÎãàÌÑ∞ÎßÅ
- ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Î∞è ÌÜ†ÌîΩ Î™®Îç∏ÎßÅ
- ÏûêÎèô Í¥ëÍ≥† Ïπ¥Ìîº ÏÉùÏÑ±

## üìñ Ï£ºÏöî ÎÇ¥Ïö©

### Í¥ëÍ≥† ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨
```python
import re
import nltk
from konlpy.tag import Okt, Mecab
from typing import List, Dict, Tuple
import numpy as np
from collections import Counter

class AdTextPreprocessor:
    """Í¥ëÍ≥† ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨Í∏∞"""
    
    def __init__(self, language='ko'):
        self.language = language
        if language == 'ko':
            self.tokenizer = Okt()
        else:
            nltk.download('punkt')
            nltk.download('stopwords')
            from nltk.corpus import stopwords
            self.stop_words = set(stopwords.words('english'))
    
    def clean_text(self, text: str) -> str:
        """ÌÖçÏä§Ìä∏ Ï†ïÏ†ú"""
        # HTML ÌÉúÍ∑∏ Ï†úÍ±∞
        text = re.sub(r'<[^>]+>', '', text)
        
        # URL Ï†úÍ±∞
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Ïù¥Î©îÏùº Ï†úÍ±∞
        text = re.sub(r'\S+@\S+', '', text)
        
        # ÌäπÏàòÎ¨∏Ïûê Ï†ïÎ¶¨ (ÌïúÍ∏Ä, ÏòÅÎ¨∏, Ïà´Ïûê, Í≥µÎ∞±Îßå Ïú†ÏßÄ)
        if self.language == 'ko':
            text = re.sub(r'[^Í∞Ä-Ìû£a-zA-Z0-9\s]', '', text)
        else:
            text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        
        # Ïó∞ÏÜç Í≥µÎ∞± Ï†úÍ±∞
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """ÌÜ†ÌÅ∞Ìôî"""
        if self.language == 'ko':
            # Î™ÖÏÇ¨, ÌòïÏö©ÏÇ¨, ÎèôÏÇ¨Îßå Ï∂îÏ∂ú
            tokens = self.tokenizer.pos(text, stem=True)
            return [word for word, pos in tokens if pos in ['Noun', 'Adjective', 'Verb'] and len(word) > 1]
        else:
            from nltk.tokenize import word_tokenize
            tokens = word_tokenize(text.lower())
            return [word for word in tokens if word not in self.stop_words and len(word) > 2]
    
    def extract_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:
        """N-gram Ï∂îÏ∂ú"""
        if len(tokens) < n:
            return []
        return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
    
    def preprocess_ad_copy(self, text: str) -> Dict:
        """Í¥ëÍ≥† Ïπ¥Ìîº Ï†ÑÏ≤òÎ¶¨ (Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏)"""
        # Ï†ïÏ†ú
        cleaned = self.clean_text(text)
        
        # ÌÜ†ÌÅ∞Ìôî
        tokens = self.tokenize(cleaned)
        
        # N-gram Ï∂îÏ∂ú
        bigrams = self.extract_ngrams(tokens, 2)
        trigrams = self.extract_ngrams(tokens, 3)
        
        return {
            'original': text,
            'cleaned': cleaned,
            'tokens': tokens,
            'bigrams': bigrams,
            'trigrams': trigrams,
            'token_count': len(tokens),
            'unique_tokens': len(set(tokens))
        }
```

### Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.nn.functional import softmax

class SentimentAnalyzer:
    """Í∞êÏ†ï Î∂ÑÏÑùÍ∏∞ (Î∏åÎûúÎìú Î™®ÎãàÌÑ∞ÎßÅÏö©)"""
    
    def __init__(self, model_name='klue/roberta-base'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # Í∞êÏ†ï Î†àÏù¥Î∏î
        self.sentiment_labels = ['Îß§Ïö∞ Î∂ÄÏ†ï', 'Î∂ÄÏ†ï', 'Ï§ëÎ¶Ω', 'Í∏çÏ†ï', 'Îß§Ïö∞ Í∏çÏ†ï']
    
    def analyze_sentiment(self, text: str) -> Dict:
        """Îã®Ïùº ÌÖçÏä§Ìä∏ Í∞êÏ†ï Î∂ÑÏÑù"""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = softmax(outputs.logits, dim=-1)
            
        pred_label_idx = torch.argmax(probabilities, dim=-1).item()
        confidence = probabilities[0][pred_label_idx].item()
        
        return {
            'sentiment': self.sentiment_labels[pred_label_idx],
            'confidence': confidence,
            'probabilities': {
                label: prob.item() 
                for label, prob in zip(self.sentiment_labels, probabilities[0])
            }
        }
    
    def batch_analyze(self, texts: List[str]) -> List[Dict]:
        """Î∞∞Ïπò Í∞êÏ†ï Î∂ÑÏÑù"""
        results = []
        for text in texts:
            results.append(self.analyze_sentiment(text))
        return results
    
    def analyze_brand_mentions(self, texts: List[str], brand_keywords: List[str]) -> Dict:
        """Î∏åÎûúÎìú Ïñ∏Í∏â Í∞êÏ†ï Î∂ÑÏÑù"""
        brand_mentions = []
        
        for text in texts:
            # Î∏åÎûúÎìú ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú ÌÖçÏä§Ìä∏Îßå ÌïÑÌÑ∞ÎßÅ
            if any(keyword.lower() in text.lower() for keyword in brand_keywords):
                sentiment = self.analyze_sentiment(text)
                brand_mentions.append({
                    'text': text,
                    'sentiment': sentiment['sentiment'],
                    'confidence': sentiment['confidence']
                })
        
        if not brand_mentions:
            return {'message': 'Î∏åÎûúÎìú Ïñ∏Í∏âÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.'}
        
        # Í∞êÏ†ï Î∂ÑÌè¨ Í≥ÑÏÇ∞
        sentiment_counts = Counter([mention['sentiment'] for mention in brand_mentions])
        total_mentions = len(brand_mentions)
        
        return {
            'total_mentions': total_mentions,
            'sentiment_distribution': {
                sentiment: count / total_mentions * 100
                for sentiment, count in sentiment_counts.items()
            },
            'detailed_mentions': brand_mentions[:10],  # ÏÉÅÏúÑ 10Í∞úÎßå
            'overall_sentiment_score': self._calculate_sentiment_score(sentiment_counts)
        }
    
    def _calculate_sentiment_score(self, sentiment_counts: Counter) -> float:
        """Ï†ÑÏ≤¥ Í∞êÏ†ï Ï†êÏàò Í≥ÑÏÇ∞ (-2 ~ +2)"""
        weights = {'Îß§Ïö∞ Î∂ÄÏ†ï': -2, 'Î∂ÄÏ†ï': -1, 'Ï§ëÎ¶Ω': 0, 'Í∏çÏ†ï': 1, 'Îß§Ïö∞ Í∏çÏ†ï': 2}
        total_weighted = sum(weights[sentiment] * count for sentiment, count in sentiment_counts.items())
        total_count = sum(sentiment_counts.values())
        return total_weighted / total_count if total_count > 0 else 0
```

### ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Î∞è ÌÜ†ÌîΩ Î™®Îç∏ÎßÅ
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import networkx as nx
from textrank import TextRank4Keyword

class KeywordExtractor:
    """ÌÇ§ÏõåÎìú Ï∂îÏ∂úÍ∏∞"""
    
    def __init__(self):
        self.preprocessor = AdTextPreprocessor()
        
    def extract_tfidf_keywords(self, documents: List[str], top_k: int = 10) -> Dict:
        """TF-IDF Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        # Ï†ÑÏ≤òÎ¶¨
        processed_docs = []
        for doc in documents:
            tokens = self.preprocessor.tokenize(self.preprocessor.clean_text(doc))
            processed_docs.append(' '.join(tokens))
        
        # TF-IDF Î≤°ÌÑ∞Ìôî
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        tfidf_matrix = vectorizer.fit_transform(processed_docs)
        feature_names = vectorizer.get_feature_names_out()
        
        # ÌèâÍ∑† TF-IDF Ï†êÏàò Í≥ÑÏÇ∞
        mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
        keyword_scores = list(zip(feature_names, mean_scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'keywords': keyword_scores[:top_k],
            'total_documents': len(documents),
            'vocabulary_size': len(feature_names)
        }
    
    def extract_textrank_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """TextRank Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        tr4w = TextRank4Keyword()
        tr4w.analyze(text, candidate_pos=['NOUN', 'PROPN'], window_size=4, lower=False)
        
        keywords = tr4w.get_keywords(top_k)
        return [(kw, score) for kw, score in keywords]
    
    def extract_campaign_keywords(self, campaign_texts: List[str], 
                                 performance_data: List[Dict]) -> Dict:
        """Ï∫†ÌéòÏù∏Î≥Ñ ÏÑ±Í≥º Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂ú"""
        # ÏÑ±Í≥º Í∏∞Ï§ÄÏúºÎ°ú ÌÖçÏä§Ìä∏ Í∑∏Î£πÌôî
        high_performance = []
        low_performance = []
        
        for text, perf in zip(campaign_texts, performance_data):
            if perf['ctr'] > np.median([p['ctr'] for p in performance_data]):
                high_performance.append(text)
            else:
                low_performance.append(text)
        
        # Í∞Å Í∑∏Î£πÎ≥Ñ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
        high_perf_keywords = self.extract_tfidf_keywords(high_performance)
        low_perf_keywords = self.extract_tfidf_keywords(low_performance)
        
        # ÏÑ±Í≥º Ï∞®Ïù¥ Î∂ÑÏÑù
        high_kw_set = set([kw[0] for kw in high_perf_keywords['keywords']])
        low_kw_set = set([kw[0] for kw in low_perf_keywords['keywords']])
        
        return {
            'high_performance_unique': list(high_kw_set - low_kw_set),
            'low_performance_unique': list(low_kw_set - high_kw_set),
            'common_keywords': list(high_kw_set & low_kw_set),
            'high_performance_keywords': high_perf_keywords['keywords'],
            'low_performance_keywords': low_perf_keywords['keywords']
        }

class TopicModeler:
    """ÌÜ†ÌîΩ Î™®Îç∏ÎßÅ"""
    
    def __init__(self, n_topics: int = 5):
        self.n_topics = n_topics
        self.preprocessor = AdTextPreprocessor()
        
    def fit_lda(self, documents: List[str]) -> Dict:
        """LDA ÌÜ†ÌîΩ Î™®Îç∏ÎßÅ"""
        # Ï†ÑÏ≤òÎ¶¨
        processed_docs = []
        for doc in documents:
            tokens = self.preprocessor.tokenize(self.preprocessor.clean_text(doc))
            processed_docs.append(' '.join(tokens))
        
        # Î≤°ÌÑ∞Ìôî
        vectorizer = TfidfVectorizer(
            max_features=1000,
            min_df=2,
            max_df=0.8,
            stop_words=None
        )
        
        doc_term_matrix = vectorizer.fit_transform(processed_docs)
        
        # LDA ÌïôÏäµ
        lda = LatentDirichletAllocation(
            n_components=self.n_topics,
            random_state=42,
            max_iter=100
        )
        
        lda.fit(doc_term_matrix)
        
        # ÌÜ†ÌîΩÎ≥Ñ Ï£ºÏöî Îã®Ïñ¥ Ï∂îÏ∂ú
        feature_names = vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            top_scores = [topic[i] for i in top_words_idx]
            
            topics.append({
                'topic_id': topic_idx,
                'words': list(zip(top_words, top_scores)),
                'weight': topic.sum()
            })
        
        # Î¨∏ÏÑúÎ≥Ñ ÌÜ†ÌîΩ Î∂ÑÌè¨
        doc_topic_matrix = lda.transform(doc_term_matrix)
        
        return {
            'topics': topics,
            'document_topic_distribution': doc_topic_matrix,
            'perplexity': lda.perplexity(doc_term_matrix),
            'log_likelihood': lda.score(doc_term_matrix)
        }
```

### ÏûêÎèô Í¥ëÍ≥† Ïπ¥Ìîº ÏÉùÏÑ±
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline

class AdCopyGenerator:
    """Í¥ëÍ≥† Ïπ¥Ìîº ÏûêÎèô ÏÉùÏÑ±Í∏∞"""
    
    def __init__(self, model_name='skt/kogpt2-base-v2'):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.generator = pipeline('text-generation', 
                                 model=self.model,
                                 tokenizer=self.tokenizer)
        
        # Í¥ëÍ≥† Ïπ¥Ìîº ÌÖúÌîåÎ¶ø
        self.templates = [
            "{product}Î°ú {benefit}ÏùÑ Í≤ΩÌóòÌïòÏÑ∏Ïöî!",
            "ÏßÄÍ∏à {product} {discount}% Ìï†Ïù∏!",
            "{target_audience}Î•º ÏúÑÌïú ÌäπÎ≥ÑÌïú {product}",
            "{problem}Ïóê ÏßÄÏπú ÎãπÏã†, {product}Í∞Ä Ìï¥ÎãµÏûÖÎãàÎã§",
            "ÌïúÏ†ï ÌäπÍ∞Ä! {product} {action} Í∏∞Ìöå"
        ]
    
    def generate_copy_variants(self, product: str, keywords: List[str], 
                              target_audience: str = "", num_variants: int = 5) -> List[Dict]:
        """Ïπ¥Ìîº Î≥ÄÌòï ÏÉùÏÑ±"""
        base_prompt = f"{product} Í¥ëÍ≥† Ïπ¥Ìîº: "
        if target_audience:
            base_prompt += f"{target_audience} ÎåÄÏÉÅ, "
        base_prompt += f"ÌÇ§ÏõåÎìú: {', '.join(keywords[:3])}"
        
        variants = []
        
        # GPT Í∏∞Î∞ò ÏÉùÏÑ±
        for i in range(num_variants):
            generated = self.generator(
                base_prompt,
                max_length=50,
                num_return_sequences=1,
                temperature=0.8,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # ÌõÑÏ≤òÎ¶¨
            text = generated[0]['generated_text'].replace(base_prompt, '').strip()
            text = self._clean_generated_text(text)
            
            variants.append({
                'copy': text,
                'method': 'gpt_generation',
                'score': self._score_copy(text, keywords)
            })
        
        # ÌÖúÌîåÎ¶ø Í∏∞Î∞ò ÏÉùÏÑ±
        for template in self.templates[:3]:
            try:
                filled_template = self._fill_template(template, product, keywords, target_audience)
                variants.append({
                    'copy': filled_template,
                    'method': 'template_based',
                    'score': self._score_copy(filled_template, keywords)
                })
            except:
                continue
        
        # Ï†êÏàò Í∏∞Ï§Ä Ï†ïÎ†¨
        variants.sort(key=lambda x: x['score'], reverse=True)
        return variants[:num_variants]
    
    def _clean_generated_text(self, text: str) -> str:
        """ÏÉùÏÑ±Îêú ÌÖçÏä§Ìä∏ Ï†ïÏ†ú"""
        # Î∂àÏôÑÏ†ÑÌïú Î¨∏Ïû• Ï†úÍ±∞
        sentences = text.split('.')
        if len(sentences) > 1:
            text = sentences[0] + '.'
        
        # ÌäπÏàòÎ¨∏Ïûê Ï†ïÎ¶¨
        text = re.sub(r'[^\w\s!?.,()%]', '', text)
        return text.strip()
    
    def _fill_template(self, template: str, product: str, 
                      keywords: List[str], target_audience: str) -> str:
        """ÌÖúÌîåÎ¶ø Ï±ÑÏö∞Í∏∞"""
        replacements = {
            'product': product,
            'benefit': keywords[0] if keywords else 'ÎßåÏ°±',
            'discount': np.random.choice([10, 20, 30, 50]),
            'target_audience': target_audience or 'Í≥†Í∞ù',
            'problem': keywords[1] if len(keywords) > 1 else 'Í≥†ÎØº',
            'action': np.random.choice(['Íµ¨Îß§', 'Ï≤¥Ìóò', 'Î¨∏Ïùò'])
        }
        
        for key, value in replacements.items():
            template = template.replace(f'{{{key}}}', str(value))
        
        return template
    
    def _score_copy(self, copy: str, keywords: List[str]) -> float:
        """Ïπ¥Ìîº ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 0
        
        # ÌÇ§ÏõåÎìú Ìè¨Ìï® Ï†êÏàò
        for keyword in keywords:
            if keyword.lower() in copy.lower():
                score += 1
        
        # Í∏∏Ïù¥ Ï†êÏàò (15-50Ïûê Ï†ÅÏ†ï)
        length = len(copy)
        if 15 <= length <= 50:
            score += 2
        elif 10 <= length <= 60:
            score += 1
        
        # Í∞êÏ†ï ÌëúÌòÑ Ï†êÏàò
        emotional_words = ['ÌäπÎ≥ÑÌïú', 'ÎÜÄÎùºÏö¥', 'ÌòÅÏã†Ï†ÅÏù∏', 'ÏµúÍ≥†Ïùò', 'ÏôÑÎ≤ΩÌïú', 'ÏÉàÎ°úÏö¥']
        for word in emotional_words:
            if word in copy:
                score += 0.5
        
        # CTA Ìè¨Ìï® Ï†êÏàò
        cta_words = ['ÏßÄÍ∏à', 'Ïò§Îäò', 'Î∞îÎ°ú', 'Ï¶âÏãú', 'ÌÅ¥Î¶≠', 'Íµ¨Îß§', 'Ï≤¥Ìóò']
        for word in cta_words:
            if word in copy:
                score += 1
                break
        
        return score
    
    def optimize_for_platform(self, copy: str, platform: str) -> str:
        """ÌîåÎû´ÌèºÎ≥Ñ ÏµúÏ†ÅÌôî"""
        if platform.lower() == 'facebook':
            # Facebook: Í∞úÏù∏Ï†ÅÏù¥Í≥† Ïä§ÌÜ†Î¶¨ÌÖîÎßÅ
            return f"ÏπúÍµ¨Îì§ÏïÑ! {copy} ÏßÑÏßú Ï∂îÏ≤úÌï¥!"
        elif platform.lower() == 'google':
            # Google: ÏßÅÏ†ëÏ†ÅÏù¥Í≥† Í≤ÄÏÉâ ÏπúÌôîÏ†Å
            return copy.replace('!', '').replace('?', '') + ' | Ìï†Ïù∏Í∞ÄÍ≤© ÌôïÏù∏'
        elif platform.lower() == 'instagram':
            # Instagram: Ìï¥ÏãúÌÉúÍ∑∏ Ï∂îÍ∞Ä
            hashtags = ['#ÌäπÍ∞Ä', '#Ìï†Ïù∏', '#Ï∂îÏ≤ú', '#Í¥ëÍ≥†']
            return f"{copy} {' '.join(hashtags[:2])}"
        else:
            return copy
```

## üöÄ ÌîÑÎ°úÏ†ùÌä∏
1. **Î∏åÎûúÎìú Î™®ÎãàÌÑ∞ÎßÅ Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏä§ÌÖú**
2. **ÏûêÎèô Í¥ëÍ≥† Ïπ¥Ìîº ÏÉùÏÑ± ÌîåÎû´Ìèº**
3. **Í≤ΩÏüÅÏÇ¨ ÌÇ§ÏõåÎìú Î∂ÑÏÑù ÎèÑÍµ¨**
4. **Îã§Íµ≠Ïñ¥ Í¥ëÍ≥† Î≤àÏó≠ Î∞è ÌòÑÏßÄÌôî**