# 63. Media Planning Theory - ë¯¸ë””ì–´ í”Œë˜ë‹ ì´ë¡ 

## ğŸ“š ê³¼ì • ì†Œê°œ
ë°ì´í„° ê¸°ë°˜ ë¯¸ë””ì–´ í”Œë˜ë‹ì˜ í•µì‹¬ ì´ë¡ ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. ë„ë‹¬ë¥ -ë¹ˆë„ ìµœì í™”ë¶€í„° í¬ë¡œìŠ¤ ë¯¸ë””ì–´ íš¨ê³¼ê¹Œì§€ ê³¼í•™ì  ë¯¸ë””ì–´ ì „ëµ ìˆ˜ë¦½ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ë„ë‹¬ë¥ -ë¹ˆë„ ì´ë¡ ê³¼ íš¨ê³¼ì  ë¹ˆë„ ëª¨ë¸ë§
- ë¯¸ë””ì–´ ë¯¹ìŠ¤ ìµœì í™”ì™€ ì˜ˆì‚° ë°°ë¶„ ì „ëµ
- í¬ë¡œìŠ¤ ë¯¸ë””ì–´ ì‹œë„ˆì§€ íš¨ê³¼ ë¶„ì„
- ì‹¤ì‹œê°„ ë¯¸ë””ì–´ ì„±ê³¼ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™”

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ë¯¸ë””ì–´ í”Œë˜ë‹ ìµœì í™” ì‹œìŠ¤í…œ
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, optimize
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class MediaPlanningOptimizer:
    """ë¯¸ë””ì–´ í”Œë˜ë‹ ìµœì í™”ë¥¼ ìœ„í•œ ì¢…í•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.media_efficiency_curves = {}
        
    def reach_frequency_optimization(self, media_data, budget_constraint, target_audience):
        """ë„ë‹¬ë¥ -ë¹ˆë„ ìµœì í™”"""
        
        # ë„ë‹¬ë¥  ê³¡ì„  ëª¨ë¸ë§ (ì¤‘ë³µ ì œê±°)
        def reach_curve(impressions, a=1.0, b=0.8, c=0.1):
            """ë„ë‹¬ë¥  = 1 - e^(-a * (impressions/target_audience)^b) - c"""
            exposure_rate = impressions / target_audience
            return (1 - np.exp(-a * (exposure_rate ** b))) * (1 - c)
        
        # ë¹ˆë„ ë¶„í¬ ëª¨ë¸ë§
        def frequency_distribution(impressions, reach):
            """í‰ê·  ë¹ˆë„ = ë…¸ì¶œ / ë„ë‹¬ë¥ """
            if reach > 0:
                return impressions / (reach * target_audience)
            return 0
        
        # íš¨ê³¼ì  ë¹ˆë„ ëª¨ë¸ (S-curve)
        def effective_frequency_curve(frequency, threshold=3.0, saturation=10.0):
            """íš¨ê³¼ì  ë¹ˆë„ ê³¡ì„ """
            if frequency < threshold:
                return 0.3 * (frequency / threshold)
            elif frequency <= saturation:
                return 0.3 + 0.7 * ((frequency - threshold) / (saturation - threshold))
            else:
                return 1.0 - 0.2 * (1 - np.exp(-(frequency - saturation)))
        
        # ë¯¸ë””ì–´ë³„ ìµœì í™”
        optimized_allocation = {}
        total_effectiveness = 0
        
        for media_type in media_data['media_type'].unique():
            media_subset = media_data[media_data['media_type'] == media_type]
            
            # ë¹„ìš© íš¨ìœ¨ì„± ê³„ì‚°
            cost_per_impression = media_subset['cost'] / media_subset['impressions']
            
            # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©
            quality_weight = (
                media_subset['viewability'] * 0.3 +
                media_subset['brand_safety'] * 0.2 +
                media_subset['audience_quality'] * 0.5
            )
            
            # ìµœì  ì„í”„ë ˆì…˜ ìˆ˜ ê³„ì‚°
            def objective_function(impressions):
                reach = reach_curve(impressions)
                frequency = frequency_distribution(impressions, reach)
                effectiveness = effective_frequency_curve(frequency)
                return -(reach * effectiveness * quality_weight.mean())
            
            # ì˜ˆì‚° ì œì•½ í•˜ì—ì„œ ìµœì í™”
            max_impressions = budget_constraint / cost_per_impression.min()
            
            result = optimize.minimize_scalar(
                objective_function,
                bounds=(0, max_impressions),
                method='bounded'
            )
            
            optimal_impressions = result.x
            optimal_reach = reach_curve(optimal_impressions)
            optimal_frequency = frequency_distribution(optimal_impressions, optimal_reach)
            optimal_cost = optimal_impressions * cost_per_impression.mean()
            
            optimized_allocation[media_type] = {
                'impressions': optimal_impressions,
                'reach': optimal_reach,
                'frequency': optimal_frequency,
                'cost': optimal_cost,
                'effectiveness': -result.fun
            }
            
            total_effectiveness += -result.fun
        
        return {
            'allocation': optimized_allocation,
            'total_effectiveness': total_effectiveness,
            'reach_frequency_curves': self._generate_rf_curves(optimized_allocation)
        }
    
    def media_mix_modeling(self, media_spend_data, sales_data, external_factors=None):
        """ë¯¸ë””ì–´ ë¯¹ìŠ¤ ëª¨ë¸ë§ (MMM)"""
        
        # ë¯¸ë””ì–´ ì±„ë„ë³„ ê´‘ê³ ìŠ¤í†¡(Adstock) ì ìš©
        def apply_adstock(media_series, decay_rate=0.5, max_lag=4):
            """ê´‘ê³ ìŠ¤í†¡ ë³€í™˜ (ì§€ì—° íš¨ê³¼)"""
            adstock_series = media_series.copy()
            
            for lag in range(1, max_lag + 1):
                lagged_effect = media_series.shift(lag) * (decay_rate ** lag)
                adstock_series += lagged_effect.fillna(0)
            
            return adstock_series
        
        # í¬í™” ê³¡ì„  ì ìš© (ìˆ˜í™• ì²´ê°)
        def apply_saturation(adstock_series, saturation_point=0.8):
            """í¬í™” ê³¡ì„  ë³€í™˜"""
            normalized = adstock_series / adstock_series.max()
            saturated = saturation_point * normalized / (saturation_point - normalized + 1)
            return saturated * adstock_series.max()
        
        # ê° ë¯¸ë””ì–´ ì±„ë„ì— ë³€í™˜ ì ìš©
        transformed_media = pd.DataFrame(index=media_spend_data.index)
        
        for channel in media_spend_data.columns:
            # 1ë‹¨ê³„: ê´‘ê³ ìŠ¤í†¡ ì ìš©
            adstock = apply_adstock(media_spend_data[channel])
            
            # 2ë‹¨ê³„: í¬í™” ê³¡ì„  ì ìš©
            saturated = apply_saturation(adstock)
            
            transformed_media[f'{channel}_transformed'] = saturated
        
        # ë² ì´ìŠ¤ë¼ì¸ íš¨ê³¼ ê³„ì‚° (ìœ ê¸°ì  ì„±ì¥)
        baseline_trend = np.arange(len(sales_data)) * (sales_data.mean() * 0.001)
        seasonal_effect = np.sin(2 * np.pi * np.arange(len(sales_data)) / 52) * (sales_data.std() * 0.1)
        baseline = sales_data.mean() + baseline_trend + seasonal_effect
        
        # ì™¸ë¶€ ìš”ì¸ í†µí•©
        if external_factors is not None:
            transformed_media = pd.concat([transformed_media, external_factors], axis=1)
        
        # ë¯¸ë””ì–´ ê¸°ì—¬ë„ ëª¨ë¸ í›ˆë ¨
        X = transformed_media.fillna(0)
        y = sales_data - baseline  # ë² ì´ìŠ¤ë¼ì¸ ì œê±°
        
        # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš© (ë¹„ì„ í˜• ê´€ê³„ í¬ì°©)
        rf_model = RandomForestRegressor(n_estimators=200, random_state=42)
        rf_model.fit(X, y)
        
        # ì±„ë„ë³„ ê¸°ì—¬ë„ ê³„ì‚°
        feature_importance = dict(zip(X.columns, rf_model.feature_importances_))
        
        # ROAS (Return on Ad Spend) ê³„ì‚°
        channel_roas = {}
        for channel in media_spend_data.columns:
            if f'{channel}_transformed' in feature_importance:
                contribution = feature_importance[f'{channel}_transformed'] * y.sum()
                spend = media_spend_data[channel].sum()
                if spend > 0:
                    channel_roas[channel] = contribution / spend
                else:
                    channel_roas[channel] = 0
        
        # ì˜ˆì¸¡ ë° ê²€ì¦
        y_pred = rf_model.predict(X) + baseline
        model_accuracy = 1 - np.mean(np.abs(y_pred - sales_data) / sales_data)
        
        return {
            'model': rf_model,
            'transformed_features': transformed_media,
            'feature_importance': feature_importance,
            'channel_roas': channel_roas,
            'baseline_effect': baseline,
            'model_accuracy': model_accuracy,
            'predictions': y_pred
        }
    
    def cross_media_synergy_analysis(self, media_interactions, performance_data):
        """í¬ë¡œìŠ¤ ë¯¸ë””ì–´ ì‹œë„ˆì§€ ë¶„ì„"""
        
        # ë¯¸ë””ì–´ ì¡°í•©ë³„ ì‹œë„ˆì§€ íš¨ê³¼ ì¸¡ì •
        synergy_matrix = pd.DataFrame(
            index=media_interactions.columns,
            columns=media_interactions.columns
        )
        
        # í˜ì–´ì™€ì´ì¦ˆ ì‹œë„ˆì§€ ê³„ì‚°
        for media1 in media_interactions.columns:
            for media2 in media_interactions.columns:
                if media1 != media2:
                    # ê°œë³„ íš¨ê³¼
                    solo_effect1 = self._calculate_solo_effect(media1, media_interactions, performance_data)
                    solo_effect2 = self._calculate_solo_effect(media2, media_interactions, performance_data)
                    
                    # ê²°í•© íš¨ê³¼
                    combined_effect = self._calculate_combined_effect(
                        [media1, media2], media_interactions, performance_data
                    )
                    
                    # ì‹œë„ˆì§€ = ê²°í•© íš¨ê³¼ - ê°œë³„ íš¨ê³¼ í•©
                    synergy = combined_effect - (solo_effect1 + solo_effect2)
                    synergy_matrix.loc[media1, media2] = synergy
                else:
                    synergy_matrix.loc[media1, media2] = 0
        
        # ìµœì  ë¯¸ë””ì–´ ì¡°í•© ì‹ë³„
        best_combinations = self._find_optimal_combinations(synergy_matrix, top_n=5)
        
        # ì‹œë„ˆì§€ íƒ€ì´ë° ë¶„ì„
        timing_analysis = self._analyze_synergy_timing(media_interactions, performance_data)
        
        # ì‹œë„ˆì§€ íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(10, 8))
        sns.heatmap(synergy_matrix.astype(float), annot=True, cmap='RdYlBu_r', center=0)
        plt.title('Cross-Media Synergy Matrix')
        plt.tight_layout()
        
        return {
            'synergy_matrix': synergy_matrix,
            'best_combinations': best_combinations,
            'timing_analysis': timing_analysis,
            'synergy_score': synergy_matrix.values.sum()
        }
    
    def frequency_capping_optimization(self, exposure_data, conversion_data):
        """ë¹ˆë„ ìºí•‘ ìµœì í™”"""
        
        # ë¹ˆë„ë³„ ì „í™˜ìœ¨ ë¶„ì„
        frequency_performance = exposure_data.groupby('frequency').agg({
            'conversions': 'sum',
            'impressions': 'sum',
            'cost': 'sum'
        }).reset_index()
        
        frequency_performance['conversion_rate'] = (
            frequency_performance['conversions'] / frequency_performance['impressions']
        )
        frequency_performance['cost_per_conversion'] = (
            frequency_performance['cost'] / frequency_performance['conversions']
        )
        
        # í•œê³„ íš¨ê³¼ ë¶„ì„
        frequency_performance['marginal_conversion_rate'] = (
            frequency_performance['conversion_rate'].diff().fillna(0)
        )
        frequency_performance['marginal_cost_efficiency'] = (
            frequency_performance['cost_per_conversion'].diff().fillna(0)
        )
        
        # ìµœì  ë¹ˆë„ ìºí•‘ ì  ì°¾ê¸°
        # í•œê³„ ì „í™˜ìœ¨ì´ ìŒìˆ˜ê°€ ë˜ê¸° ì‹œì‘í•˜ëŠ” ì§€ì 
        optimal_frequency = self._find_optimal_frequency_cap(frequency_performance)
        
        # ë¹ˆë„ ìºí•‘ ì‹œë®¬ë ˆì´ì…˜
        capping_simulation = self._simulate_frequency_capping(
            exposure_data, range(1, 21), conversion_data
        )
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ë¹ˆë„ë³„ ì „í™˜ìœ¨
        axes[0, 0].plot(frequency_performance['frequency'], 
                       frequency_performance['conversion_rate'], 'bo-')
        axes[0, 0].axvline(x=optimal_frequency, color='red', linestyle='--', 
                          label=f'Optimal Cap: {optimal_frequency}')
        axes[0, 0].set_xlabel('Frequency')
        axes[0, 0].set_ylabel('Conversion Rate')
        axes[0, 0].set_title('Conversion Rate by Frequency')
        axes[0, 0].legend()
        
        # í•œê³„ ì „í™˜ìœ¨
        axes[0, 1].plot(frequency_performance['frequency'], 
                       frequency_performance['marginal_conversion_rate'], 'ro-')
        axes[0, 1].axhline(y=0, color='gray', linestyle='--')
        axes[0, 1].set_xlabel('Frequency')
        axes[0, 1].set_ylabel('Marginal Conversion Rate')
        axes[0, 1].set_title('Marginal Conversion Rate')
        
        # ë¹„ìš© íš¨ìœ¨ì„±
        axes[1, 0].plot(frequency_performance['frequency'], 
                       frequency_performance['cost_per_conversion'], 'go-')
        axes[1, 0].set_xlabel('Frequency')
        axes[1, 0].set_ylabel('Cost per Conversion')
        axes[1, 0].set_title('Cost Efficiency by Frequency')
        
        # ìºí•‘ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
        caps = list(capping_simulation.keys())
        rois = [sim['roi'] for sim in capping_simulation.values()]
        axes[1, 1].plot(caps, rois, 'mo-')
        axes[1, 1].axvline(x=optimal_frequency, color='red', linestyle='--')
        axes[1, 1].set_xlabel('Frequency Cap')
        axes[1, 1].set_ylabel('ROI')
        axes[1, 1].set_title('ROI by Frequency Cap')
        
        plt.tight_layout()
        plt.show()
        
        return {
            'frequency_performance': frequency_performance,
            'optimal_frequency': optimal_frequency,
            'capping_simulation': capping_simulation,
            'efficiency_gain': self._calculate_efficiency_gain(capping_simulation, optimal_frequency)
        }
    
    def dayparting_optimization(self, hourly_performance_data, audience_data):
        """ë°ì´íŒŒíŒ… ìµœì í™”"""
        
        # ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ë¶„ì„
        hourly_analysis = hourly_performance_data.groupby('hour').agg({
            'impressions': 'sum',
            'clicks': 'sum',
            'conversions': 'sum',
            'cost': 'sum'
        }).reset_index()
        
        hourly_analysis['ctr'] = hourly_analysis['clicks'] / hourly_analysis['impressions']
        hourly_analysis['conversion_rate'] = hourly_analysis['conversions'] / hourly_analysis['clicks']
        hourly_analysis['cpc'] = hourly_analysis['cost'] / hourly_analysis['clicks']
        hourly_analysis['roas'] = hourly_analysis['conversions'] / hourly_analysis['cost']
        
        # ì˜¤ë””ì–¸ìŠ¤ í™œë™ íŒ¨í„´ ë¶„ì„
        audience_activity = audience_data.groupby('hour')['active_users'].sum().reset_index()
        
        # íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (ROAS Ã— ì˜¤ë””ì–¸ìŠ¤ í™œë™ë„)
        merged_data = hourly_analysis.merge(audience_activity, on='hour')
        merged_data['efficiency_score'] = (
            merged_data['roas'] * merged_data['active_users'] / merged_data['active_users'].max()
        )
        
        # ìµœì  ì‹œê°„ëŒ€ ì‹ë³„
        optimal_hours = merged_data.nlargest(8, 'efficiency_score')['hour'].tolist()
        
        # ì˜ˆì‚° ì¬ë°°ë¶„ ì‹œë®¬ë ˆì´ì…˜
        current_allocation = hourly_analysis.set_index('hour')['cost'].to_dict()
        optimized_allocation = self._optimize_hourly_budget(merged_data, current_allocation)
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ì‹œê°„ëŒ€ë³„ ROAS
        axes[0, 0].bar(merged_data['hour'], merged_data['roas'], alpha=0.7)
        axes[0, 0].set_xlabel('Hour of Day')
        axes[0, 0].set_ylabel('ROAS')
        axes[0, 0].set_title('ROAS by Hour')
        
        # ì˜¤ë””ì–¸ìŠ¤ í™œë™ë„
        axes[0, 1].plot(merged_data['hour'], merged_data['active_users'], 'o-', color='orange')
        axes[0, 1].set_xlabel('Hour of Day')
        axes[0, 1].set_ylabel('Active Users')
        axes[0, 1].set_title('Audience Activity Pattern')
        
        # íš¨ìœ¨ì„± ì ìˆ˜
        axes[1, 0].bar(merged_data['hour'], merged_data['efficiency_score'], 
                      color=['red' if h in optimal_hours else 'lightblue' for h in merged_data['hour']])
        axes[1, 0].set_xlabel('Hour of Day')
        axes[1, 0].set_ylabel('Efficiency Score')
        axes[1, 0].set_title('Efficiency Score by Hour (Red = Optimal)')
        
        # ì˜ˆì‚° ì¬ë°°ë¶„ ë¹„êµ
        hours = list(current_allocation.keys())
        current_budgets = list(current_allocation.values())
        optimized_budgets = [optimized_allocation.get(h, 0) for h in hours]
        
        x = np.arange(len(hours))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, current_budgets, width, label='Current', alpha=0.7)
        axes[1, 1].bar(x + width/2, optimized_budgets, width, label='Optimized', alpha=0.7)
        axes[1, 1].set_xlabel('Hour of Day')
        axes[1, 1].set_ylabel('Budget Allocation')
        axes[1, 1].set_title('Budget Allocation Comparison')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(hours)
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.show()
        
        return {
            'hourly_performance': merged_data,
            'optimal_hours': optimal_hours,
            'current_allocation': current_allocation,
            'optimized_allocation': optimized_allocation,
            'efficiency_improvement': self._calculate_dayparting_improvement(
                current_allocation, optimized_allocation, merged_data
            )
        }
    
    def media_attribution_modeling(self, touchpoint_data, conversion_data):
        """ë¯¸ë””ì–´ ì–´íŠ¸ë¦¬ë·°ì…˜ ëª¨ë¸ë§"""
        
        # ë‹¤ì–‘í•œ ì–´íŠ¸ë¦¬ë·°ì…˜ ëª¨ë¸ êµ¬í˜„
        attribution_models = {
            'first_touch': self._first_touch_attribution,
            'last_touch': self._last_touch_attribution,
            'linear': self._linear_attribution,
            'time_decay': self._time_decay_attribution,
            'position_based': self._position_based_attribution,
            'data_driven': self._data_driven_attribution
        }
        
        # ê° ëª¨ë¸ë³„ ì–´íŠ¸ë¦¬ë·°ì…˜ ê²°ê³¼
        attribution_results = {}
        
        for model_name, model_func in attribution_models.items():
            attribution_results[model_name] = model_func(touchpoint_data, conversion_data)
        
        # ëª¨ë¸ ë¹„êµ ë° ê²€ì¦
        model_comparison = self._compare_attribution_models(attribution_results, conversion_data)
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        best_model = max(model_comparison.items(), key=lambda x: x[1]['accuracy'])[0]
        
        # ì–´íŠ¸ë¦¬ë·°ì…˜ ì‹œê°í™”
        self._visualize_attribution_results(attribution_results, model_comparison)
        
        return {
            'attribution_results': attribution_results,
            'model_comparison': model_comparison,
            'best_model': best_model,
            'recommended_attribution': attribution_results[best_model]
        }

# ë³´ì¡° ë©”ì„œë“œë“¤
    def _calculate_solo_effect(self, media, interactions, performance):
        """ê°œë³„ ë¯¸ë””ì–´ íš¨ê³¼ ê³„ì‚°"""
        solo_periods = interactions[interactions[media] > 0]
        other_media = [col for col in interactions.columns if col != media]
        solo_periods = solo_periods[solo_periods[other_media].sum(axis=1) == 0]
        
        if len(solo_periods) > 0:
            return performance.loc[solo_periods.index].mean()
        return 0
    
    def _calculate_combined_effect(self, media_list, interactions, performance):
        """ê²°í•© ë¯¸ë””ì–´ íš¨ê³¼ ê³„ì‚°"""
        combined_periods = interactions[
            (interactions[media_list].sum(axis=1) == len(media_list)) &
            (interactions.drop(columns=media_list).sum(axis=1) == 0)
        ]
        
        if len(combined_periods) > 0:
            return performance.loc[combined_periods.index].mean()
        return 0
    
    def _find_optimal_frequency_cap(self, frequency_performance):
        """ìµœì  ë¹ˆë„ ìºí•‘ ì  ì°¾ê¸°"""
        # í•œê³„ ì „í™˜ìœ¨ì´ ì²˜ìŒìœ¼ë¡œ ìŒìˆ˜ê°€ ë˜ëŠ” ì§€ì 
        negative_marginal = frequency_performance[
            frequency_performance['marginal_conversion_rate'] < 0
        ]
        
        if len(negative_marginal) > 0:
            return negative_marginal.iloc[0]['frequency'] - 1
        else:
            return frequency_performance['frequency'].max()

# ì‹¤ìŠµ ì˜ˆì œ: ì¢…í•© ë¯¸ë””ì–´ í”Œë˜ë‹
def comprehensive_media_planning():
    """ì¢…í•©ì ì¸ ë¯¸ë””ì–´ í”Œë˜ë‹ ìµœì í™”"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # ë¯¸ë””ì–´ ì„±ê³¼ ë°ì´í„°
    media_data = pd.DataFrame({
        'media_type': ['TV', 'Digital', 'Radio', 'Print', 'OOH'] * 20,
        'impressions': np.random.uniform(100000, 1000000, 100),
        'cost': np.random.uniform(10000, 100000, 100),
        'viewability': np.random.uniform(0.6, 0.95, 100),
        'brand_safety': np.random.uniform(0.7, 1.0, 100),
        'audience_quality': np.random.uniform(0.5, 0.9, 100)
    })
    
    # ë¹ˆë„ ë…¸ì¶œ ë°ì´í„°
    exposure_data = pd.DataFrame({
        'frequency': np.random.randint(1, 15, 1000),
        'impressions': np.random.uniform(1000, 10000, 1000),
        'conversions': np.random.poisson(5, 1000),
        'cost': np.random.uniform(100, 1000, 1000)
    })
    
    # ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ë°ì´í„°
    hourly_data = pd.DataFrame({
        'hour': list(range(24)) * 30,
        'impressions': np.random.uniform(1000, 10000, 720),
        'clicks': np.random.uniform(50, 500, 720),
        'conversions': np.random.uniform(5, 50, 720),
        'cost': np.random.uniform(100, 1000, 720)
    })
    
    # ì˜¤ë””ì–¸ìŠ¤ í™œë™ ë°ì´í„°
    audience_data = pd.DataFrame({
        'hour': range(24),
        'active_users': [
            500, 300, 200, 150, 100, 200, 400, 800, 1200, 1000,
            900, 950, 1100, 1200, 1300, 1400, 1500, 1600, 1400, 1200, 1000, 800, 700, 600
        ]
    })
    
    # ìµœì í™” ë„êµ¬ ì´ˆê¸°í™”
    optimizer = MediaPlanningOptimizer()
    
    # 1. ë„ë‹¬ë¥ -ë¹ˆë„ ìµœì í™”
    rf_results = optimizer.reach_frequency_optimization(
        media_data, budget_constraint=500000, target_audience=1000000
    )
    
    # 2. ë¹ˆë„ ìºí•‘ ìµœì í™”
    frequency_results = optimizer.frequency_capping_optimization(
        exposure_data, exposure_data
    )
    
    # 3. ë°ì´íŒŒíŒ… ìµœì í™”
    daypart_results = optimizer.dayparting_optimization(
        hourly_data, audience_data
    )
    
    # ê²°ê³¼ ìš”ì•½
    print("=== ë¯¸ë””ì–´ í”Œë˜ë‹ ìµœì í™” ê²°ê³¼ ===")
    print(f"ìµœì  ë¹ˆë„ ìºí•‘: {frequency_results['optimal_frequency']}íšŒ")
    print(f"ìµœì  ì‹œê°„ëŒ€: {len(daypart_results['optimal_hours'])}ê°œ ì‹œê°„ëŒ€")
    print(f"ì´ íš¨ê³¼ì„± ì ìˆ˜: {rf_results['total_effectiveness']:.3f}")
    
    return {
        'rf_results': rf_results,
        'frequency_results': frequency_results,
        'daypart_results': daypart_results
    }

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("=== ë¯¸ë””ì–´ í”Œë˜ë‹ ì´ë¡  ì‹¤ìŠµ ===")
    print("ë„ë‹¬ë¥ -ë¹ˆë„ ìµœì í™”ì™€ ë¯¸ë””ì–´ ë¯¹ìŠ¤ ëª¨ë¸ë§")
    
    results = comprehensive_media_planning()
    
    print(f"\nìµœì í™” ì™„ë£Œ:")
    print(f"- ë¹ˆë„ ìºí•‘ íš¨ìœ¨ì„± ê°œì„ : {results['frequency_results']['efficiency_gain']:.2%}")
    print(f"- ë°ì´íŒŒíŒ… ê°œì„  íš¨ê³¼: {results['daypart_results']['efficiency_improvement']:.2%}")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ë¯¸ë””ì–´ ë¯¹ìŠ¤ ëª¨ë¸ë§ í”Œë«í¼** - MMM ê¸°ë°˜ ì˜ˆì‚° ìµœì í™” ì‹œìŠ¤í…œ
2. **ì‹¤ì‹œê°„ ë¹ˆë„ ìºí•‘ ì—”ì§„** - ë™ì  ë…¸ì¶œ ë¹ˆë„ ì œì–´ ì‹œìŠ¤í…œ  
3. **í¬ë¡œìŠ¤ ë¯¸ë””ì–´ ì‹œë„ˆì§€ ë¶„ì„** - ì±„ë„ ê°„ ìƒí˜¸ì‘ìš© íš¨ê³¼ ì¸¡ì •
4. **ì–´íŠ¸ë¦¬ë·°ì…˜ ëª¨ë¸ë§ ëŒ€ì‹œë³´ë“œ** - ë‹¤ì¤‘ í„°ì¹˜í¬ì¸íŠ¸ ê¸°ì—¬ë„ ë¶„ì„