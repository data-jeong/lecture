"""
ì›¹ ìŠ¤í¬ë˜í•‘ ì˜ˆì œ
ë¹„ë™ê¸° ì›¹ í¬ë¡¤ë§ê³¼ ì†ë„ ì œí•œ
"""

import asyncio
import aiohttp
import time
from typing import List, Dict, Optional
from dataclasses import dataclass
import json
from bs4 import BeautifulSoup

from ..patterns.rate_limiter import AsyncRateLimiter
from ..patterns.producer_consumer import AsyncProducerConsumer
from ..utils.monitoring import PerformanceTracker


@dataclass
class WebPage:
    """ì›¹ í˜ì´ì§€ ì •ë³´"""
    url: str
    title: Optional[str] = None
    content: Optional[str] = None
    links: List[str] = None
    error: Optional[str] = None
    fetch_time: float = 0
    
    def __post_init__(self):
        if self.links is None:
            self.links = []


class WebScraperExample:
    """ì›¹ ìŠ¤í¬ë˜í¼ ì˜ˆì œ"""
    
    def __init__(self):
        self.rate_limiter = AsyncRateLimiter(rate=5, per=1.0)  # 5 requests/second
        self.tracker = PerformanceTracker()
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def fetch_page(self, url: str) -> WebPage:
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        page = WebPage(url=url)
        
        await self.rate_limiter.acquire()  # ì†ë„ ì œí•œ
        
        start_time = time.time()
        
        try:
            async with self.tracker.track_async(f"Fetch {url}"):
                async with self.session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        
                        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ì œëª© ì¶”ì¶œ
                        title_tag = soup.find('title')
                        page.title = title_tag.text.strip() if title_tag else "No Title"
                        
                        # ë³¸ë¬¸ ì¶”ì¶œ (ì²« 500ì)
                        text = soup.get_text()
                        page.content = ' '.join(text.split())[:500]
                        
                        # ë§í¬ ì¶”ì¶œ
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            if href.startswith('http'):
                                page.links.append(href)
                        
                        page.fetch_time = time.time() - start_time
                    else:
                        page.error = f"HTTP {response.status}"
        
        except asyncio.TimeoutError:
            page.error = "Timeout"
        except Exception as e:
            page.error = str(e)
        
        return page
    
    async def scrape_urls(self, urls: List[str]) -> List[WebPage]:
        """ì—¬ëŸ¬ URL ìŠ¤í¬ë˜í•‘"""
        self.session = aiohttp.ClientSession()
        
        try:
            # ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´ ì‚¬ìš©
            pc = AsyncProducerConsumer[str, WebPage](
                max_queue_size=20,
                num_consumers=5
            )
            
            # URL ìƒì„±ê¸°
            async def url_generator():
                for url in urls:
                    yield url
            
            # í˜ì´ì§€ ì²˜ë¦¬ í•¨ìˆ˜
            async def process_url(url: str) -> WebPage:
                return await self.fetch_page(url)
            
            # ì‹¤í–‰
            results = await pc.run(
                source=url_generator(),
                processor=process_url
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            pages = []
            for result in results:
                if "result" in result and result["result"]:
                    pages.append(result["result"])
            
            return pages
        
        finally:
            await self.session.close()
    
    async def crawl_recursive(self, start_url: str, max_depth: int = 2, max_pages: int = 20) -> Dict[str, WebPage]:
        """ì¬ê·€ì  í¬ë¡¤ë§"""
        visited = set()
        to_visit = {(start_url, 0)}  # (url, depth)
        pages = {}
        
        self.session = aiohttp.ClientSession()
        
        try:
            while to_visit and len(pages) < max_pages:
                # í˜„ì¬ ë ˆë²¨ì˜ URLë“¤
                current_batch = []
                current_depth = None
                
                # ê°™ì€ ê¹Šì´ì˜ URLë“¤ ë°°ì¹˜ ì²˜ë¦¬
                for url, depth in list(to_visit):
                    if current_depth is None:
                        current_depth = depth
                    
                    if depth == current_depth and url not in visited:
                        current_batch.append(url)
                        to_visit.remove((url, depth))
                        visited.add(url)
                
                if not current_batch:
                    break
                
                print(f"\nê¹Šì´ {current_depth}: {len(current_batch)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§")
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_pages = await self.scrape_urls(current_batch)
                
                # ê²°ê³¼ ì €ì¥ ë° ìƒˆ ë§í¬ ì¶”ê°€
                for page in batch_pages:
                    if not page.error:
                        pages[page.url] = page
                        
                        # ë‹¤ìŒ ê¹Šì´ ë§í¬ ì¶”ê°€
                        if current_depth < max_depth - 1:
                            for link in page.links[:5]:  # ê° í˜ì´ì§€ë‹¹ ìµœëŒ€ 5ê°œ ë§í¬
                                if link not in visited:
                                    to_visit.add((link, current_depth + 1))
            
            return pages
        
        finally:
            await self.session.close()
    
    async def run(self):
        """ì˜ˆì œ ì‹¤í–‰"""
        print("ğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ì˜ˆì œ")
        print("=" * 60)
        
        # 1. ë‹¨ìˆœ ìŠ¤í¬ë˜í•‘
        print("\n1. ì—¬ëŸ¬ í˜ì´ì§€ ë™ì‹œ ìŠ¤í¬ë˜í•‘")
        
        urls = [
            "https://example.com",
            "https://httpbin.org/html",
            "https://httpbin.org/delay/1",
            "https://httpbin.org/status/200",
            "https://httpbin.org/json"
        ]
        
        start_time = time.time()
        pages = await self.scrape_urls(urls)
        duration = time.time() - start_time
        
        print(f"\nâœ… {len(pages)}ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        
        for page in pages:
            if page.error:
                print(f"  âŒ {page.url}: {page.error}")
            else:
                print(f"  âœ… {page.url}: {page.title} ({page.fetch_time:.2f}ì´ˆ)")
        
        # 2. ì¬ê·€ì  í¬ë¡¤ë§ (ì‹œë®¬ë ˆì´ì…˜)
        print("\n\n2. ì¬ê·€ì  ì›¹ í¬ë¡¤ë§ (example.com ê¸°ì¤€)")
        
        crawled_pages = await self.crawl_recursive(
            "https://example.com",
            max_depth=2,
            max_pages=10
        )
        
        print(f"\nâœ… ì´ {len(crawled_pages)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§")
        
        # ê¹Šì´ë³„ í†µê³„
        depth_stats = {}
        for url, page in crawled_pages.items():
            # URL ê¸°ë°˜ìœ¼ë¡œ ê¹Šì´ ì¶”ì • (ì‹¤ì œë¡œëŠ” í¬ë¡¤ë§ ì¤‘ ì¶”ì )
            depth = 0 if url == "https://example.com" else 1
            if depth not in depth_stats:
                depth_stats[depth] = 0
            depth_stats[depth] += 1
        
        print("\nğŸ“Š ê¹Šì´ë³„ í˜ì´ì§€ ìˆ˜:")
        for depth, count in sorted(depth_stats.items()):
            print(f"  ê¹Šì´ {depth}: {count}ê°œ")
        
        # 3. ì„±ëŠ¥ í†µê³„
        print("\n\n3. ì„±ëŠ¥ í†µê³„")
        
        # ì†ë„ ì œí•œ í†µê³„
        rate_stats = await self.rate_limiter.get_statistics()
        print("\nğŸš¦ ì†ë„ ì œí•œ í†µê³„:")
        for key, value in rate_stats.items():
            print(f"  {key}: {value}")
        
        # ì„±ëŠ¥ ì¶”ì  í†µê³„
        self.tracker.print_report()
        
        # ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥")
        
        result_data = {
            "scraping_results": [
                {
                    "url": page.url,
                    "title": page.title,
                    "fetch_time": page.fetch_time,
                    "links_count": len(page.links),
                    "error": page.error
                }
                for page in pages
            ],
            "crawling_results": {
                "total_pages": len(crawled_pages),
                "depth_stats": depth_stats
            },
            "performance": self.tracker.get_statistics()
        }
        
        with open("web_scraping_results.json", "w", encoding="utf-8") as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print("  âœ… web_scraping_results.json ì €ì¥ ì™„ë£Œ")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    example = WebScraperExample()
    await example.run()


if __name__ == "__main__":
    asyncio.run(main())