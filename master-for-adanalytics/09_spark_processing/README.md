# 09. Spark Processing - ìŠ¤íŒŒí¬ ì²˜ë¦¬

## ğŸ“š ê³¼ì • ì†Œê°œ
Apache Sparkë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ê´‘ê³  ë°ì´í„° ë¶„ì‚° ì²˜ë¦¬ë¥¼ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ë¶€í„° ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ê¹Œì§€ ê´‘ê³  í”Œë«í¼ì˜ ë¹…ë°ì´í„° ì²˜ë¦¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ëŒ€ê·œëª¨ ê´‘ê³  ë¡œê·¸ ë¶„ì‚° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ë¶„ì„
- Spark MLì„ í™œìš©í•œ ê´‘ê³  ìµœì í™”
- ê³ ì„±ëŠ¥ ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## ğŸ“– ì£¼ìš” ë‚´ìš©

### Spark ê¸°ë³¸ ê´‘ê³  ë°ì´í„° ì²˜ë¦¬
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from typing import Dict, List
import logging

class SparkAdProcessor:
    """Spark ê´‘ê³  ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self, app_name: str = "AdPlatformSpark"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        
    def load_ad_logs(self, path: str, format: str = "parquet") -> DataFrame:
        """ê´‘ê³  ë¡œê·¸ ë°ì´í„° ë¡œë“œ"""
        if format == "json":
            return self.spark.read.json(path)
        elif format == "csv":
            return self.spark.read.option("header", "true").csv(path)
        else:
            return self.spark.read.parquet(path)
    
    def create_sample_ad_data(self, num_records: int = 1000000) -> DataFrame:
        """ìƒ˜í”Œ ê´‘ê³  ë°ì´í„° ìƒì„±"""
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("campaign_id", StringType(), True),
            StructField("ad_id", StringType(), True),
            StructField("timestamp", TimestampType(), True),
            StructField("event_type", StringType(), True),  # impression, click, conversion
            StructField("device_type", StringType(), True),
            StructField("os", StringType(), True),
            StructField("browser", StringType(), True),
            StructField("country", StringType(), True),
            StructField("city", StringType(), True),
            StructField("age_group", StringType(), True),
            StructField("gender", StringType(), True),
            StructField("bid_price", DoubleType(), True),
            StructField("cost", DoubleType(), True),
            StructField("revenue", DoubleType(), True)
        ])
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±ì„ ìœ„í•œ SQL
        sample_data_sql = f"""
        SELECT 
            concat('user_', cast(rand() * 100000 as int)) as user_id,
            concat('campaign_', cast(rand() * 1000 as int)) as campaign_id,
            concat('ad_', cast(rand() * 10000 as int)) as ad_id,
            current_timestamp() - interval cast(rand() * 30 as int) day - 
                interval cast(rand() * 24 as int) hour as timestamp,
            case when rand() < 0.7 then 'impression'
                 when rand() < 0.95 then 'click'
                 else 'conversion' end as event_type,
            case when rand() < 0.6 then 'mobile'
                 when rand() < 0.85 then 'desktop'
                 else 'tablet' end as device_type,
            case when rand() < 0.5 then 'iOS'
                 when rand() < 0.8 then 'Android'
                 else 'Windows' end as os,
            case when rand() < 0.4 then 'Chrome'
                 when rand() < 0.7 then 'Safari'
                 else 'Firefox' end as browser,
            case when rand() < 0.3 then 'KR'
                 when rand() < 0.5 then 'US'
                 when rand() < 0.7 then 'JP'
                 else 'CN' end as country,
            case when rand() < 0.2 then 'Seoul'
                 when rand() < 0.3 then 'New York'
                 when rand() < 0.4 then 'Tokyo'
                 else 'Beijing' end as city,
            case when rand() < 0.25 then '18-24'
                 when rand() < 0.5 then '25-34'
                 when rand() < 0.75 then '35-44'
                 else '45+' end as age_group,
            case when rand() < 0.5 then 'M' else 'F' end as gender,
            round(rand() * 5, 2) as bid_price,
            round(rand() * 2, 3) as cost,
            case when rand() < 0.1 then round(rand() * 100, 2) else 0.0 end as revenue
        FROM range({num_records})
        """
        
        return self.spark.sql(sample_data_sql)
    
    def calculate_campaign_performance(self, df: DataFrame) -> DataFrame:
        """ìº í˜ì¸ ì„±ê³¼ ê³„ì‚°"""
        # ì´ë²¤íŠ¸ë³„ ì§‘ê³„
        performance = df.groupBy("campaign_id", "event_type") \
            .agg(
                count("*").alias("event_count"),
                sum("cost").alias("total_cost"),
                sum("revenue").alias("total_revenue"),
                avg("bid_price").alias("avg_bid_price")
            )
        
        # í”¼ë²—í•˜ì—¬ ê°€ë¡œë¡œ ì •ë ¬
        performance_pivot = performance.groupBy("campaign_id") \
            .pivot("event_type", ["impression", "click", "conversion"]) \
            .agg(
                first("event_count").alias("count"),
                first("total_cost").alias("cost"),
                first("total_revenue").alias("revenue")
            )
        
        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        performance_metrics = performance_pivot.select(
            col("campaign_id"),
            coalesce(col("impression_count"), lit(0)).alias("impressions"),
            coalesce(col("click_count"), lit(0)).alias("clicks"),
            coalesce(col("conversion_count"), lit(0)).alias("conversions"),
            coalesce(col("impression_cost"), lit(0.0)).alias("impression_cost"),
            coalesce(col("click_cost"), lit(0.0)).alias("click_cost"),
            coalesce(col("conversion_cost"), lit(0.0)).alias("conversion_cost"),
            coalesce(col("conversion_revenue"), lit(0.0)).alias("total_revenue")
        ).withColumn("total_cost", 
            col("impression_cost") + col("click_cost") + col("conversion_cost")
        ).withColumn("ctr", 
            when(col("impressions") > 0, col("clicks") / col("impressions")).otherwise(0)
        ).withColumn("cvr",
            when(col("clicks") > 0, col("conversions") / col("clicks")).otherwise(0)
        ).withColumn("cpc",
            when(col("clicks") > 0, col("total_cost") / col("clicks")).otherwise(0)
        ).withColumn("cpa",
            when(col("conversions") > 0, col("total_cost") / col("conversions")).otherwise(0)
        ).withColumn("roas",
            when(col("total_cost") > 0, col("total_revenue") / col("total_cost")).otherwise(0)
        )
        
        return performance_metrics
    
    def user_behavior_analysis(self, df: DataFrame) -> DataFrame:
        """ì‚¬ìš©ì í–‰ë™ ë¶„ì„"""
        # ì‚¬ìš©ìë³„ ì„¸ì…˜ ë¶„ì„
        window_spec = Window.partitionBy("user_id").orderBy("timestamp")
        
        user_sessions = df.withColumn(
            "session_id",
            concat(
                col("user_id"), 
                lit("_"),
                date_format(col("timestamp"), "yyyyMMdd"),
                lit("_"),
                when(
                    lag("timestamp").over(window_spec).isNull() |
                    (col("timestamp").cast("long") - lag("timestamp").over(window_spec).cast("long") > 1800),
                    monotonically_increasing_id()
                ).otherwise(lag("session_id").over(window_spec))
            )
        )
        
        # ì‚¬ìš©ìë³„ í–‰ë™ íŒ¨í„´ ë¶„ì„
        user_behavior = user_sessions.groupBy("user_id") \
            .agg(
                countDistinct("session_id").alias("session_count"),
                count("*").alias("total_events"),
                sum(when(col("event_type") == "impression", 1).otherwise(0)).alias("impressions"),
                sum(when(col("event_type") == "click", 1).otherwise(0)).alias("clicks"),
                sum(when(col("event_type") == "conversion", 1).otherwise(0)).alias("conversions"),
                countDistinct("campaign_id").alias("campaigns_engaged"),
                countDistinct("ad_id").alias("ads_seen"),
                sum("cost").alias("total_cost_generated"),
                sum("revenue").alias("total_revenue_generated"),
                first("device_type").alias("primary_device"),
                first("country").alias("country"),
                first("age_group").alias("age_group"),
                first("gender").alias("gender")
            )
        
        # ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜
        user_segments = user_behavior.withColumn(
            "user_segment",
            when((col("conversions") > 5) & (col("total_revenue_generated") > 100), "High Value")
            .when((col("conversions") > 0) & (col("clicks") > 10), "Active Converter")
            .when(col("clicks") > 20, "Active Clicker")
            .when(col("impressions") > 50, "Viewer")
            .otherwise("Low Engagement")
        )
        
        return user_segments
    
    def real_time_attribution_analysis(self, df: DataFrame) -> DataFrame:
        """ì‹¤ì‹œê°„ ì–´íŠ¸ë¦¬ë·°ì…˜ ë¶„ì„"""
        # ì „í™˜ ê²½ë¡œ ë¶„ì„ì„ ìœ„í•œ ìœˆë„ìš° í•¨ìˆ˜
        conversion_window = Window.partitionBy("user_id") \
            .orderBy("timestamp") \
            .rangeBetween(-3600, 0)  # 1ì‹œê°„ ì´ë‚´ í„°ì¹˜í¬ì¸íŠ¸
        
        # ì „í™˜ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì–´íŠ¸ë¦¬ë·°ì…˜
        conversions = df.filter(col("event_type") == "conversion")
        
        # ì „í™˜ ì „ í„°ì¹˜í¬ì¸íŠ¸ ìˆ˜ì§‘
        attribution_data = df.alias("events") \
            .join(conversions.alias("conv"), ["user_id"]) \
            .filter(
                (col("events.timestamp") <= col("conv.timestamp")) &
                (col("events.timestamp") >= col("conv.timestamp") - expr("interval 24 hours"))
            ) \
            .select(
                col("conv.user_id"),
                col("conv.timestamp").alias("conversion_time"),
                col("conv.campaign_id").alias("conversion_campaign"),
                col("conv.revenue").alias("conversion_revenue"),
                col("events.campaign_id").alias("touchpoint_campaign"),
                col("events.ad_id").alias("touchpoint_ad"),
                col("events.event_type").alias("touchpoint_type"),
                col("events.timestamp").alias("touchpoint_time"),
                ((col("conv.timestamp").cast("long") - col("events.timestamp").cast("long")) / 3600).alias("hours_before_conversion")
            )
        
        # First Touch Attribution
        first_touch = attribution_data \
            .withColumn("rn", row_number().over(Window.partitionBy("user_id", "conversion_time").orderBy("touchpoint_time"))) \
            .filter(col("rn") == 1) \
            .groupBy("touchpoint_campaign") \
            .agg(
                count("*").alias("first_touch_conversions"),
                sum("conversion_revenue").alias("first_touch_revenue")
            )
        
        # Last Touch Attribution
        last_touch = attribution_data \
            .withColumn("rn", row_number().over(Window.partitionBy("user_id", "conversion_time").orderBy(desc("touchpoint_time")))) \
            .filter(col("rn") == 1) \
            .groupBy("touchpoint_campaign") \
            .agg(
                count("*").alias("last_touch_conversions"),
                sum("conversion_revenue").alias("last_touch_revenue")
            )
        
        # Linear Attribution (ê· ë“± ë¶„ë°°)
        linear_attribution = attribution_data \
            .withColumn("touchpoint_count", count("*").over(Window.partitionBy("user_id", "conversion_time"))) \
            .withColumn("linear_conversion_credit", lit(1.0) / col("touchpoint_count")) \
            .withColumn("linear_revenue_credit", col("conversion_revenue") / col("touchpoint_count")) \
            .groupBy("touchpoint_campaign") \
            .agg(
                sum("linear_conversion_credit").alias("linear_conversions"),
                sum("linear_revenue_credit").alias("linear_revenue")
            )
        
        # ê²°ê³¼ ë³‘í•©
        attribution_results = first_touch \
            .join(last_touch, "touchpoint_campaign", "full_outer") \
            .join(linear_attribution, "touchpoint_campaign", "full_outer") \
            .fillna(0)
        
        return attribution_results

class SparkStreamingProcessor:
    """Spark ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("AdStreamingProcessor") \
            .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
            .getOrCreate()
    
    def process_real_time_bids(self, kafka_servers: str, topic: str):
        """ì‹¤ì‹œê°„ ì…ì°° ë°ì´í„° ì²˜ë¦¬"""
        # Kafkaì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì½ê¸°
        bid_stream = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_servers) \
            .option("subscribe", topic) \
            .option("startingOffsets", "latest") \
            .load()
        
        # JSON íŒŒì‹±
        bid_schema = StructType([
            StructField("bid_id", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("campaign_id", StringType(), True),
            StructField("bid_price", DoubleType(), True),
            StructField("device_type", StringType(), True),
            StructField("timestamp", TimestampType(), True)
        ])
        
        parsed_bids = bid_stream.select(
            from_json(col("value").cast("string"), bid_schema).alias("data")
        ).select("data.*")
        
        # ì‹¤ì‹œê°„ ì§‘ê³„
        bid_aggregates = parsed_bids \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "1 minute"),
                col("campaign_id")
            ) \
            .agg(
                count("*").alias("bid_count"),
                avg("bid_price").alias("avg_bid_price"),
                max("bid_price").alias("max_bid_price"),
                min("bid_price").alias("min_bid_price")
            )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘
        query = bid_aggregates.writeStream \
            .outputMode("append") \
            .format("console") \
            .trigger(processingTime='10 seconds') \
            .start()
        
        return query
    
    def detect_ad_fraud(self, stream_df: DataFrame) -> DataFrame:
        """ì‹¤ì‹œê°„ ê´‘ê³  ì‚¬ê¸° íƒì§€"""
        # ìœˆë„ìš° ê¸°ë°˜ ì‚¬ê¸° íŒ¨í„´ íƒì§€
        fraud_detection = stream_df \
            .withWatermark("timestamp", "5 minutes") \
            .groupBy(
                window(col("timestamp"), "1 minute"),
                col("user_id"),
                col("campaign_id")
            ) \
            .agg(
                count("*").alias("event_count"),
                countDistinct("ad_id").alias("unique_ads"),
                avg("bid_price").alias("avg_bid_price")
            ) \
            .withColumn(
                "fraud_score",
                when(col("event_count") > 100, 0.9)  # 1ë¶„ì— 100ë²ˆ ì´ìƒ ì´ë²¤íŠ¸
                .when(col("unique_ads") < 2, 0.7)     # ë‹¤ì–‘ì„± ë¶€ì¡±
                .when(col("avg_bid_price") > 10, 0.6) # ë¹„ì •ìƒì  ê³ ê°€ ì…ì°°
                .otherwise(0.1)
            ) \
            .filter(col("fraud_score") > 0.5)
        
        return fraud_detection

class SparkMLAdOptimizer:
    """Spark MLì„ í™œìš©í•œ ê´‘ê³  ìµœì í™”"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
    
    def build_ctr_prediction_model(self, df: DataFrame):
        """CTR ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•"""
        from pyspark.ml.feature import StringIndexer, OneHotEncoder
        from pyspark.ml.regression import LinearRegression
        from pyspark.ml import Pipeline
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ë±ì‹±
        categorical_cols = ["device_type", "os", "browser", "country", "age_group", "gender"]
        indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_index") for col in categorical_cols]
        
        # ì›í•« ì¸ì½”ë”©
        encoders = [OneHotEncoder(inputCol=f"{col}_index", outputCol=f"{col}_encoded") 
                   for col in categorical_cols]
        
        # íŠ¹ì„± ë²¡í„° ìƒì„±
        feature_cols = [f"{col}_encoded" for col in categorical_cols] + ["bid_price"]
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        
        # ì •ê·œí™”
        scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
        
        # ì„ í˜• íšŒê·€ ëª¨ë¸
        lr = LinearRegression(featuresCol="scaled_features", labelCol="ctr")
        
        # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, lr])
        
        # CTR ê³„ì‚° (í´ë¦­ ì´ë²¤íŠ¸ ë¹„ìœ¨)
        training_data = df.withColumn("ctr", 
            when(col("event_type") == "click", 1.0).otherwise(0.0))
        
        # ëª¨ë¸ í›ˆë ¨
        model = pipeline.fit(training_data)
        
        return model
    
    def audience_segmentation_kmeans(self, df: DataFrame, k: int = 5):
        """K-meansë¥¼ í™œìš©í•œ ì˜¤ë””ì–¸ìŠ¤ ì„¸ë¶„í™”"""
        # ì‚¬ìš©ìë³„ íŠ¹ì„± ì§‘ê³„
        user_features = df.groupBy("user_id") \
            .agg(
                count("*").alias("total_events"),
                sum(when(col("event_type") == "impression", 1).otherwise(0)).alias("impressions"),
                sum(when(col("event_type") == "click", 1).otherwise(0)).alias("clicks"),
                sum(when(col("event_type") == "conversion", 1).otherwise(0)).alias("conversions"),
                avg("bid_price").alias("avg_bid_price"),
                sum("cost").alias("total_cost"),
                sum("revenue").alias("total_revenue"),
                countDistinct("campaign_id").alias("campaigns_engaged")
            ) \
            .fillna(0)
        
        # íŠ¹ì„± ë²¡í„° ìƒì„±
        feature_cols = ["total_events", "impressions", "clicks", "conversions", 
                       "avg_bid_price", "total_cost", "total_revenue", "campaigns_engaged"]
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        
        # ì •ê·œí™”
        scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(k=k, seed=42, featuresCol="scaled_features")
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = Pipeline(stages=[assembler, scaler, kmeans])
        model = pipeline.fit(user_features)
        
        # í´ëŸ¬ìŠ¤í„° í• ë‹¹
        clustered_users = model.transform(user_features)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
        cluster_analysis = clustered_users.groupBy("prediction") \
            .agg(
                count("*").alias("cluster_size"),
                avg("total_events").alias("avg_total_events"),
                avg("impressions").alias("avg_impressions"),
                avg("clicks").alias("avg_clicks"),
                avg("conversions").alias("avg_conversions"),
                avg("total_revenue").alias("avg_revenue")
            )
        
        return model, clustered_users, cluster_analysis
    
    def recommend_ads_als(self, df: DataFrame):
        """ALSë¥¼ í™œìš©í•œ ê´‘ê³  ì¶”ì²œ"""
        # ì‚¬ìš©ì-ê´‘ê³  ìƒí˜¸ì‘ìš© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        user_ad_interactions = df.filter(col("event_type").isin(["click", "conversion"])) \
            .groupBy("user_id", "ad_id") \
            .agg(
                count("*").alias("interaction_count"),
                sum(when(col("event_type") == "conversion", 2).otherwise(1)).alias("rating")
            )
        
        # ì‚¬ìš©ìì™€ ê´‘ê³  IDë¥¼ ìˆ«ìë¡œ ë³€í™˜
        user_indexer = StringIndexer(inputCol="user_id", outputCol="user_index")
        ad_indexer = StringIndexer(inputCol="ad_id", outputCol="ad_index")
        
        indexed_data = user_indexer.fit(user_ad_interactions).transform(user_ad_interactions)
        indexed_data = ad_indexer.fit(indexed_data).transform(indexed_data)
        
        # ALS ëª¨ë¸
        als = ALS(
            maxIter=10,
            regParam=0.1,
            userCol="user_index",
            itemCol="ad_index", 
            ratingCol="rating",
            coldStartStrategy="drop"
        )
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        train, test = indexed_data.randomSplit([0.8, 0.2], seed=42)
        
        # ëª¨ë¸ í›ˆë ¨
        model = als.fit(train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        predictions = model.transform(test)
        evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")
        rmse = evaluator.evaluate(predictions)
        
        # ì‚¬ìš©ìë³„ ìƒìœ„ Nê°œ ê´‘ê³  ì¶”ì²œ
        user_recommendations = model.recommendForAllUsers(10)
        
        return model, rmse, user_recommendations

# ì‚¬ìš© ì˜ˆì‹œ
def example_spark_ad_processing():
    """Spark ê´‘ê³  ì²˜ë¦¬ ì˜ˆì‹œ"""
    # Spark ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = SparkAdProcessor()
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print("ìƒ˜í”Œ ê´‘ê³  ë°ì´í„° ìƒì„± ì¤‘...")
    ad_data = processor.create_sample_ad_data(1000000)
    
    # ë°ì´í„° ìºì‹±
    ad_data.cache()
    print(f"ì´ {ad_data.count():,} ê°œì˜ ê´‘ê³  ì´ë²¤íŠ¸ ìƒì„±ë¨")
    
    # ìº í˜ì¸ ì„±ê³¼ ë¶„ì„
    print("ìº í˜ì¸ ì„±ê³¼ ë¶„ì„ ì¤‘...")
    campaign_performance = processor.calculate_campaign_performance(ad_data)
    campaign_performance.show(20)
    
    # ì‚¬ìš©ì í–‰ë™ ë¶„ì„
    print("ì‚¬ìš©ì í–‰ë™ ë¶„ì„ ì¤‘...")
    user_behavior = processor.user_behavior_analysis(ad_data)
    user_behavior.groupBy("user_segment").count().show()
    
    # ì–´íŠ¸ë¦¬ë·°ì…˜ ë¶„ì„
    print("ì–´íŠ¸ë¦¬ë·°ì…˜ ë¶„ì„ ì¤‘...")
    attribution = processor.real_time_attribution_analysis(ad_data)
    attribution.show()
    
    # ML ìµœì í™”
    print("ML ëª¨ë¸ êµ¬ì¶• ì¤‘...")
    ml_optimizer = SparkMLAdOptimizer(processor.spark)
    
    # CTR ì˜ˆì¸¡ ëª¨ë¸
    ctr_model = ml_optimizer.build_ctr_prediction_model(ad_data)
    
    # ì˜¤ë””ì–¸ìŠ¤ ì„¸ë¶„í™”
    kmeans_model, clustered_users, cluster_analysis = ml_optimizer.audience_segmentation_kmeans(ad_data)
    print("í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼:")
    cluster_analysis.show()
    
    # ê²°ê³¼ ì €ì¥
    campaign_performance.write.mode("overwrite").parquet("campaign_performance.parquet")
    user_behavior.write.mode("overwrite").parquet("user_behavior.parquet")
    
    # ìŠ¤íŒŒí¬ ì„¸ì…˜ ì¢…ë£Œ
    processor.spark.stop()
    
    return {
        "campaign_performance": campaign_performance.count(),
        "user_segments": user_behavior.groupBy("user_segment").count().collect(),
        "attribution_campaigns": attribution.count()
    }

if __name__ == "__main__":
    results = example_spark_ad_processing()
    print("Spark ê´‘ê³  ì²˜ë¦¬ ì™„ë£Œ!")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ëŒ€ê·œëª¨ ê´‘ê³  ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ**
2. **ì‹¤ì‹œê°„ ì…ì°° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**
3. **ML ê¸°ë°˜ ê´‘ê³  ìµœì í™” ì—”ì§„**
4. **ë¶„ì‚° ETL íŒŒì´í”„ë¼ì¸**