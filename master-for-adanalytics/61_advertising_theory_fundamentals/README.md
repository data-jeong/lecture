# 61. Advertising Theory Fundamentals - ê´‘ê³  ì´ë¡  ê¸°ì´ˆ

## ğŸ“š ê³¼ì • ì†Œê°œ
ê´‘ê³  íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ í•µì‹¬ ì´ë¡ ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. ì†Œë¹„ì ì‹¬ë¦¬í•™, ì„¤ë“ ì´ë¡ , ë¸Œëœë“œ í¬ì§€ì…”ë‹ ë“± ê´‘ê³ ì˜ ê³¼í•™ì  ê¸°ë°˜ì„ ë°ì´í„°ì™€ í•¨ê»˜ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê´‘ê³  ì‹¬ë¦¬í•™ê³¼ ì†Œë¹„ì í–‰ë™ ì´ë¡  ì´í•´
- ì„¤ë“ ì´ë¡ ê³¼ ì¸ì§€ ì²˜ë¦¬ ëª¨ë¸ ì ìš©
- ë¸Œëœë“œ í¬ì§€ì…”ë‹ê³¼ ê´‘ê³  íš¨ê³¼ì„± ì¸¡ì •
- ì£¼ì˜-ê¸°ì–µ ëª¨ë¸ ê¸°ë°˜ í¬ë¦¬ì—ì´í‹°ë¸Œ ìµœì í™”

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê´‘ê³  ì‹¬ë¦¬í•™ê³¼ ì†Œë¹„ì í–‰ë™ ì´ë¡ 
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

class ConsumerBehaviorAnalyzer:
    """ì†Œë¹„ì í–‰ë™ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        
    def elaboration_likelihood_model(self, user_data, ad_data):
        """ì •êµí™” ê°€ëŠ¥ì„± ëª¨ë¸(ELM) ì ìš©"""
        # ì¤‘ì•™ ê²½ë¡œ vs ì£¼ë³€ ê²½ë¡œ ì²˜ë¦¬ ì˜ˆì¸¡
        
        # ì‚¬ìš©ì ê´€ì—¬ë„ ê³„ì‚°
        involvement_score = (
            user_data['product_knowledge'] * 0.3 +
            user_data['purchase_importance'] * 0.4 +
            user_data['time_available'] * 0.3
        )
        
        # ê´‘ê³  ë©”ì‹œì§€ ê°•ë„ ê³„ì‚°
        message_strength = (
            ad_data['argument_quality'] * 0.5 +
            ad_data['evidence_strength'] * 0.3 +
            ad_data['source_credibility'] * 0.2
        )
        
        # ì²˜ë¦¬ ê²½ë¡œ ê²°ì •
        processing_route = np.where(
            involvement_score > 0.6,
            'central',  # ì¤‘ì•™ ê²½ë¡œ (ë…¼ë¦¬ì  ì²˜ë¦¬)
            'peripheral'  # ì£¼ë³€ ê²½ë¡œ (íœ´ë¦¬ìŠ¤í‹± ì²˜ë¦¬)
        )
        
        # íƒœë„ ë³€í™” ì˜ˆì¸¡
        attitude_change = np.where(
            processing_route == 'central',
            message_strength * involvement_score * 0.8,
            ad_data['peripheral_cues'] * (1 - involvement_score) * 0.6
        )
        
        return {
            'processing_route': processing_route,
            'attitude_change': attitude_change,
            'involvement_score': involvement_score
        }
    
    def hierarchy_of_effects_model(self, campaign_data):
        """íš¨ê³¼ ê³„ì¸µ ëª¨ë¸ ì ìš©"""
        stages = ['awareness', 'knowledge', 'liking', 'preference', 'conviction', 'purchase']
        
        # ê° ë‹¨ê³„ë³„ ì „í™˜ìœ¨ ê³„ì‚°
        conversion_rates = {}
        funnel_data = pd.DataFrame()
        
        for i, stage in enumerate(stages):
            if i == 0:
                conversion_rates[stage] = 1.0
                funnel_data[stage] = campaign_data['impressions']
            else:
                prev_stage = stages[i-1]
                # ë‹¨ê³„ë³„ ì „í™˜ìœ¨ ëª¨ë¸ë§
                base_rate = 0.8 - (i * 0.1)  # ê¸°ë³¸ ì „í™˜ìœ¨
                
                # ìº í˜ì¸ í’ˆì§ˆ ìš”ì†Œ ì ìš©
                quality_factor = (
                    campaign_data['creative_quality'] * 0.3 +
                    campaign_data['targeting_accuracy'] * 0.3 +
                    campaign_data['message_relevance'] * 0.4
                )
                
                conversion_rates[stage] = base_rate * quality_factor
                funnel_data[stage] = funnel_data[prev_stage] * conversion_rates[stage]
        
        return {
            'conversion_rates': conversion_rates,
            'funnel_data': funnel_data,
            'total_conversion': funnel_data['purchase'] / funnel_data['awareness']
        }
    
    def cognitive_response_model(self, ad_exposure_data):
        """ì¸ì§€ì  ë°˜ì‘ ëª¨ë¸"""
        # ê´‘ê³ ì— ëŒ€í•œ ì¸ì§€ì  ë°˜ì‘ ë¶„ì„
        
        # ë°˜ì‘ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        response_weights = {
            'support_arguments': 0.4,  # ì§€ì§€ ë…¼ì¦
            'counter_arguments': -0.3,  # ë°˜ë°• ë…¼ì¦
            'source_derogations': -0.2,  # ì¶œì²˜ ë¹„í•˜
            'execution_thoughts': 0.1   # ì‹¤í–‰ ê´€ë ¨ ìƒê°
        }
        
        # ê° ë°˜ì‘ ìœ í˜•ì˜ ê°•ë„ ê³„ì‚°
        cognitive_responses = {}
        for response_type, weight in response_weights.items():
            cognitive_responses[response_type] = (
                ad_exposure_data[f'{response_type}_count'] * 
                ad_exposure_data[f'{response_type}_intensity'] *
                weight
            )
        
        # ì´ ì¸ì§€ì  ë°˜ì‘ ì ìˆ˜
        total_cognitive_response = sum(cognitive_responses.values())
        
        # íƒœë„ í˜•ì„± ì˜ˆì¸¡
        attitude_formation = np.tanh(total_cognitive_response)  # -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        return {
            'cognitive_responses': cognitive_responses,
            'total_response': total_cognitive_response,
            'predicted_attitude': attitude_formation
        }
    
    def means_end_chain_analysis(self, product_attributes, consumer_values):
        """ìˆ˜ë‹¨-ëª©ì  ì‚¬ìŠ¬ ë¶„ì„"""
        # ì œí’ˆ ì†ì„± -> ê¸°ëŠ¥ì  í˜œíƒ -> ì‹¬ë¦¬ì  í˜œíƒ -> ê°œì¸ ê°€ì¹˜
        
        # ì†ì„±-í˜œíƒ ì—°ê²° ë§¤íŠ¸ë¦­ìŠ¤
        attribute_benefit_matrix = np.array([
            [0.8, 0.2, 0.1, 0.3],  # í’ˆì§ˆ -> ì„±ëŠ¥, ì‹ ë¢°ì„±, í¸ì˜ì„±, ì‚¬íšŒì  ì¸ì •
            [0.3, 0.1, 0.9, 0.2],  # í¸ì˜ì„± -> ì„±ëŠ¥, ì‹ ë¢°ì„±, í¸ì˜ì„±, ì‚¬íšŒì  ì¸ì •
            [0.2, 0.3, 0.2, 0.8],  # ë¸Œëœë“œ -> ì„±ëŠ¥, ì‹ ë¢°ì„±, í¸ì˜ì„±, ì‚¬íšŒì  ì¸ì •
            [0.6, 0.4, 0.3, 0.1]   # ê°€ê²© -> ì„±ëŠ¥, ì‹ ë¢°ì„±, í¸ì˜ì„±, ì‚¬íšŒì  ì¸ì •
        ])
        
        # í˜œíƒ-ê°€ì¹˜ ì—°ê²° ë§¤íŠ¸ë¦­ìŠ¤  
        benefit_value_matrix = np.array([
            [0.7, 0.2, 0.3, 0.1, 0.2],  # ì„±ëŠ¥ -> ì„±ì·¨, ì•ˆì „, ì¦ê±°ì›€, ì‚¬íšŒì  ì†Œì†, ìì•„ì‹¤í˜„
            [0.2, 0.8, 0.1, 0.3, 0.1],  # ì‹ ë¢°ì„± -> ì„±ì·¨, ì•ˆì „, ì¦ê±°ì›€, ì‚¬íšŒì  ì†Œì†, ìì•„ì‹¤í˜„
            [0.1, 0.3, 0.8, 0.2, 0.4],  # í¸ì˜ì„± -> ì„±ì·¨, ì•ˆì „, ì¦ê±°ì›€, ì‚¬íšŒì  ì†Œì†, ìì•„ì‹¤í˜„
            [0.3, 0.1, 0.2, 0.9, 0.3]   # ì‚¬íšŒì  ì¸ì • -> ì„±ì·¨, ì•ˆì „, ì¦ê±°ì›€, ì‚¬íšŒì  ì†Œì†, ìì•„ì‹¤í˜„
        ])
        
        # ì œí’ˆ ì†ì„±ì—ì„œ ê°œì¸ ê°€ì¹˜ê¹Œì§€ì˜ ì—°ê²° ê°•ë„ ê³„ì‚°
        functional_benefits = np.dot(product_attributes, attribute_benefit_matrix)
        personal_values = np.dot(functional_benefits, benefit_value_matrix)
        
        # ê°€ì¹˜ ì¼ì¹˜ë„ ê³„ì‚°
        value_congruence = np.corrcoef(personal_values, consumer_values)[0, 1]
        
        return {
            'functional_benefits': functional_benefits,
            'personal_values': personal_values,
            'value_congruence': value_congruence,
            'chain_strength': np.sum(personal_values * consumer_values)
        }
    
    def dual_process_theory_model(self, ad_data, user_state):
        """ì´ì¤‘ ì²˜ë¦¬ ì´ë¡  ëª¨ë¸"""
        # System 1 (ìë™ì , ì§ê´€ì ) vs System 2 (í†µì œì , ë¶„ì„ì ) ì²˜ë¦¬
        
        # ì²˜ë¦¬ ì‹œìŠ¤í…œ ê²°ì • ìš”ì¸
        system1_triggers = (
            user_state['cognitive_load'] * 0.4 +
            user_state['time_pressure'] * 0.3 +
            ad_data['emotional_appeal'] * 0.3
        )
        
        system2_triggers = (
            user_state['motivation'] * 0.4 +
            ad_data['information_complexity'] * 0.3 +
            user_state['analytical_thinking'] * 0.3
        )
        
        # ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„ íƒ (0: System 1, 1: System 2)
        processing_system = np.where(system2_triggers > system1_triggers, 1, 0)
        
        # ì‹œìŠ¤í…œë³„ ê´‘ê³  íš¨ê³¼ ì˜ˆì¸¡
        system1_effect = (
            ad_data['visual_appeal'] * 0.4 +
            ad_data['emotional_intensity'] * 0.4 +
            ad_data['familiarity'] * 0.2
        )
        
        system2_effect = (
            ad_data['argument_strength'] * 0.5 +
            ad_data['evidence_quality'] * 0.3 +
            ad_data['logical_structure'] * 0.2
        )
        
        # ìµœì¢… ê´‘ê³  íš¨ê³¼
        ad_effectiveness = np.where(
            processing_system == 0,
            system1_effect,
            system2_effect
        )
        
        return {
            'processing_system': processing_system,
            'system1_effect': system1_effect,
            'system2_effect': system2_effect,
            'ad_effectiveness': ad_effectiveness
        }
    
    def brand_positioning_map(self, brand_data, competitor_data):
        """ë¸Œëœë“œ í¬ì§€ì…”ë‹ ë§µ ìƒì„±"""
        # ì£¼ìš” ì†ì„± ì°¨ì›ì—ì„œ ë¸Œëœë“œ ìœ„ì¹˜ ë¶„ì„
        
        attributes = ['quality', 'price', 'innovation', 'reliability', 'prestige']
        
        # ì „ì²´ ë¸Œëœë“œ ë°ì´í„° ê²°í•©
        all_brands = pd.concat([brand_data, competitor_data])
        
        # í‘œì¤€í™”
        scaled_data = self.scaler.fit_transform(all_brands[attributes])
        
        # PCAë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œ
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        brand_positions = pca.fit_transform(scaled_data)
        
        # í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ê²½ìŸ ê·¸ë£¹ ì‹ë³„
        kmeans = KMeans(n_clusters=3, random_state=42)
        competitive_groups = kmeans.fit_predict(brand_positions)
        
        # í¬ì§€ì…”ë‹ ë§µ ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(brand_positions[:, 0], brand_positions[:, 1], 
                           c=competitive_groups, cmap='viridis', s=100, alpha=0.7)
        
        # ë¸Œëœë“œëª… ë¼ë²¨ë§
        for i, brand in enumerate(all_brands['brand_name']):
            plt.annotate(brand, (brand_positions[i, 0], brand_positions[i, 1]), 
                        xytext=(5, 5), textcoords='offset points')
        
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        plt.title('Brand Positioning Map')
        plt.colorbar(scatter, label='Competitive Group')
        plt.grid(True, alpha=0.3)
        
        return {
            'positions': brand_positions,
            'competitive_groups': competitive_groups,
            'pca_components': pca.components_,
            'explained_variance': pca.explained_variance_ratio_
        }
    
    def attitude_behavior_gap_analysis(self, attitude_data, behavior_data):
        """íƒœë„-í–‰ë™ ê²©ì°¨ ë¶„ì„"""
        # íƒœë„ì™€ ì‹¤ì œ í–‰ë™ ê°„ì˜ ë¶ˆì¼ì¹˜ ì›ì¸ ë¶„ì„
        
        # íƒœë„ ì ìˆ˜ ê³„ì‚°
        attitude_score = (
            attitude_data['cognitive_component'] * 0.4 +
            attitude_data['affective_component'] * 0.3 +
            attitude_data['conative_component'] * 0.3
        )
        
        # í–‰ë™ ì ìˆ˜ ê³„ì‚° (êµ¬ë§¤, ì¶”ì²œ, ì¬êµ¬ë§¤ ë“±)
        behavior_score = (
            behavior_data['purchase_behavior'] * 0.4 +
            behavior_data['recommendation_behavior'] * 0.3 +
            behavior_data['loyalty_behavior'] * 0.3
        )
        
        # íƒœë„-í–‰ë™ ê²©ì°¨ ê³„ì‚°
        attitude_behavior_gap = attitude_score - behavior_score
        
        # ê²©ì°¨ ì›ì¸ ë¶„ì„
        barrier_factors = {
            'situational_constraints': attitude_data['situational_barriers'],
            'social_pressure': attitude_data['social_influence'],
            'resource_limitations': attitude_data['resource_constraints'],
            'habit_strength': behavior_data['existing_habits']
        }
        
        # ê²©ì°¨ ì„¤ëª… ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
        from sklearn.ensemble import RandomForestRegressor
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        
        X = pd.DataFrame(barrier_factors)
        y = attitude_behavior_gap
        
        rf_model.fit(X, y)
        feature_importance = dict(zip(X.columns, rf_model.feature_importances_))
        
        return {
            'attitude_score': attitude_score,
            'behavior_score': behavior_score,
            'gap': attitude_behavior_gap,
            'barrier_importance': feature_importance,
            'gap_segments': pd.qcut(attitude_behavior_gap, q=3, labels=['Low', 'Medium', 'High'])
        }

# ì‹¤ìŠµ ì˜ˆì œ: ì¢…í•© ê´‘ê³  íš¨ê³¼ ë¶„ì„
def comprehensive_ad_effectiveness_analysis():
    """ì¢…í•©ì ì¸ ê´‘ê³  íš¨ê³¼ì„± ë¶„ì„"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_users = 1000
    
    # ì‚¬ìš©ì ë°ì´í„°
    user_data = pd.DataFrame({
        'user_id': range(n_users),
        'product_knowledge': np.random.beta(2, 5, n_users),
        'purchase_importance': np.random.beta(3, 3, n_users),
        'time_available': np.random.beta(2, 3, n_users),
        'cognitive_load': np.random.beta(2, 3, n_users),
        'motivation': np.random.beta(3, 2, n_users),
        'analytical_thinking': np.random.beta(3, 3, n_users)
    })
    
    # ê´‘ê³  ë°ì´í„°
    ad_data = pd.DataFrame({
        'campaign_id': range(n_users),
        'argument_quality': np.random.beta(3, 2, n_users),
        'evidence_strength': np.random.beta(3, 3, n_users),
        'source_credibility': np.random.beta(4, 2, n_users),
        'peripheral_cues': np.random.beta(2, 3, n_users),
        'creative_quality': np.random.beta(3, 2, n_users),
        'targeting_accuracy': np.random.beta(4, 3, n_users),
        'message_relevance': np.random.beta(3, 2, n_users),
        'emotional_appeal': np.random.beta(3, 3, n_users),
        'visual_appeal': np.random.beta(3, 2, n_users),
        'information_complexity': np.random.beta(2, 3, n_users)
    })
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ConsumerBehaviorAnalyzer()
    
    # 1. ì •êµí™” ê°€ëŠ¥ì„± ëª¨ë¸ ë¶„ì„
    elm_results = analyzer.elaboration_likelihood_model(user_data, ad_data)
    
    # 2. ì´ì¤‘ ì²˜ë¦¬ ì´ë¡  ë¶„ì„
    user_state = user_data[['cognitive_load', 'time_pressure', 'motivation', 'analytical_thinking']].copy()
    user_state['time_pressure'] = np.random.beta(2, 3, n_users)  # ì‹œê°„ ì••ë°• ì¶”ê°€
    
    dual_process_results = analyzer.dual_process_theory_model(ad_data, user_state)
    
    # ê²°ê³¼ ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # ELM ê²°ê³¼ ì‹œê°í™”
    processing_counts = pd.Series(elm_results['processing_route']).value_counts()
    axes[0, 0].pie(processing_counts.values, labels=processing_counts.index, autopct='%1.1f%%')
    axes[0, 0].set_title('Processing Route Distribution (ELM)')
    
    # ê´€ì—¬ë„ë³„ íƒœë„ ë³€í™”
    axes[0, 1].scatter(elm_results['involvement_score'], elm_results['attitude_change'], alpha=0.6)
    axes[0, 1].set_xlabel('Involvement Score')
    axes[0, 1].set_ylabel('Attitude Change')
    axes[0, 1].set_title('Involvement vs Attitude Change')
    
    # ì²˜ë¦¬ ì‹œìŠ¤í…œ ë¶„í¬
    system_counts = pd.Series(dual_process_results['processing_system']).value_counts()
    system_labels = ['System 1 (Intuitive)', 'System 2 (Analytical)']
    axes[1, 0].bar(system_labels, [system_counts.get(0, 0), system_counts.get(1, 0)])
    axes[1, 0].set_title('Processing System Usage')
    
    # ê´‘ê³  íš¨ê³¼ì„± ë¶„í¬
    axes[1, 1].hist(dual_process_results['ad_effectiveness'], bins=30, alpha=0.7)
    axes[1, 1].set_xlabel('Ad Effectiveness Score')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Ad Effectiveness Distribution')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'elm_results': elm_results,
        'dual_process_results': dual_process_results,
        'user_data': user_data,
        'ad_data': ad_data
    }

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("=== ê´‘ê³  ì´ë¡  ê¸°ì´ˆ ì‹¤ìŠµ ===")
    print("ì •êµí™” ê°€ëŠ¥ì„± ëª¨ë¸ê³¼ ì´ì¤‘ ì²˜ë¦¬ ì´ë¡ ì„ í™œìš©í•œ ê´‘ê³  íš¨ê³¼ ë¶„ì„")
    
    results = comprehensive_ad_effectiveness_analysis()
    
    print(f"\në¶„ì„ ì™„ë£Œ:")
    print(f"- ì¤‘ì•™ ê²½ë¡œ ì²˜ë¦¬ ì‚¬ìš©ì: {sum(results['elm_results']['processing_route'] == 'central')}ëª…")
    print(f"- ì£¼ë³€ ê²½ë¡œ ì²˜ë¦¬ ì‚¬ìš©ì: {sum(results['elm_results']['processing_route'] == 'peripheral')}ëª…")
    print(f"- System 2 ì‚¬ìš©ì: {sum(results['dual_process_results']['processing_system'] == 1)}ëª…")
    print(f"- í‰ê·  ê´‘ê³  íš¨ê³¼ì„±: {np.mean(results['dual_process_results']['ad_effectiveness']):.3f}")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ì†Œë¹„ì ì‹¬ë¦¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ** - ELM ëª¨ë¸ ê¸°ë°˜ ì‹¤ì‹œê°„ ë¶„ì„
2. **ë¸Œëœë“œ í¬ì§€ì…”ë‹ ë§µ** - ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜ ë¶„ì„
3. **íƒœë„-í–‰ë™ ê²©ì°¨ ì˜ˆì¸¡** - êµ¬ë§¤ ì „í™˜ ì¥ë²½ ì‹ë³„
4. **ê´‘ê³  ë©”ì‹œì§€ ìµœì í™”** - ì¸ì§€ ì²˜ë¦¬ ëª¨ë¸ ê¸°ë°˜ í¬ë¦¬ì—ì´í‹°ë¸Œ ê°œì„ 