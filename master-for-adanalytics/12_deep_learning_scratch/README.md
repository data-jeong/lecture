# 12. Deep Learning from Scratch - ë”¥ëŸ¬ë‹ ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„

## ğŸ“š ê³¼ì • ì†Œê°œ
NumPyë§Œìœ¼ë¡œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬í˜„í•˜ê³ , ê´‘ê³  ë„ë©”ì¸ì— íŠ¹í™”ëœ ì‹ ê²½ë§ì„ ì²˜ìŒë¶€í„° ë§Œë“¤ì–´ë´…ë‹ˆë‹¤. ìˆ˜í•™ì  ì›ë¦¬ë¶€í„° ìµœì í™”ê¹Œì§€ ì™„ë²½íˆ ì´í•´í•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ ì§ì ‘ êµ¬í˜„
- ê´‘ê³  CTR ì˜ˆì¸¡ ì‹ ê²½ë§ êµ¬ì¶•
- ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„
- ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì´í•´ì™€ êµ¬í˜„

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê¸°ë³¸ ì‹ ê²½ë§ êµ¬í˜„
```python
import numpy as np
from typing import List, Tuple, Dict

class Layer:
    """ê¸°ë³¸ ë ˆì´ì–´ ì¸í„°í˜ì´ìŠ¤"""
    def forward(self, x):
        raise NotImplementedError
    
    def backward(self, grad):
        raise NotImplementedError

class Dense(Layer):
    """ì™„ì „ ì—°ê²° ë ˆì´ì–´"""
    def __init__(self, input_dim: int, output_dim: int):
        # He ì´ˆê¸°í™”
        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)
        self.b = np.zeros((1, output_dim))
        self.x = None
        self.dW = None
        self.db = None
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.x = x
        return np.dot(x, self.W) + self.b
    
    def backward(self, dout: np.ndarray) -> np.ndarray:
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0, keepdims=True)
        return dx

class ReLU(Layer):
    """ReLU í™œì„±í™” í•¨ìˆ˜"""
    def __init__(self):
        self.mask = None
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out
    
    def backward(self, dout: np.ndarray) -> np.ndarray:
        dout[self.mask] = 0
        return dout

class Softmax(Layer):
    """Softmax with Cross Entropy Loss"""
    def __init__(self):
        self.y = None
        self.t = None
        
    def forward(self, x: np.ndarray, t: np.ndarray) -> float:
        self.t = t
        self.y = self._softmax(x)
        return self._cross_entropy(self.y, self.t)
    
    def backward(self, dout: float = 1) -> np.ndarray:
        batch_size = self.t.shape[0]
        return (self.y - self.t) / batch_size
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        x = x - np.max(x, axis=-1, keepdims=True)  # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
        return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)
    
    def _cross_entropy(self, y: np.ndarray, t: np.ndarray) -> float:
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)
        
        batch_size = y.shape[0]
        return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

### CTR ì˜ˆì¸¡ ì‹ ê²½ë§
```python
class CTRPredictionNetwork:
    """ê´‘ê³  CTR ì˜ˆì¸¡ ì‹ ê²½ë§"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int]):
        self.layers = []
        
        # ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
        dims = [input_dim] + hidden_dims + [2]  # ì´ì§„ ë¶„ë¥˜
        
        for i in range(len(dims) - 1):
            self.layers.append(Dense(dims[i], dims[i+1]))
            if i < len(dims) - 2:  # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œì™¸
                self.layers.append(ReLU())
        
        self.loss_layer = Softmax()
        
    def predict(self, x: np.ndarray) -> np.ndarray:
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def loss(self, x: np.ndarray, t: np.ndarray) -> float:
        y = self.predict(x)
        return self.loss_layer.forward(y, t)
    
    def accuracy(self, x: np.ndarray, t: np.ndarray) -> float:
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        return np.sum(y == t) / x.shape[0]
    
    def gradient(self, x: np.ndarray, t: np.ndarray) -> Dict:
        # Forward
        self.loss(x, t)
        
        # Backward
        dout = self.loss_layer.backward()
        
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        
        # ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
        grads = {}
        for i, layer in enumerate(self.layers):
            if isinstance(layer, Dense):
                grads[f'W{i}'] = layer.dW
                grads[f'b{i}'] = layer.db
        
        return grads
```

### ìµœì í™” ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
```python
class Optimizer:
    """ìµœì í™” ê¸°ë³¸ í´ë˜ìŠ¤"""
    def update(self, params: Dict, grads: Dict):
        raise NotImplementedError

class SGD(Optimizer):
    """í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•"""
    def __init__(self, lr: float = 0.01):
        self.lr = lr
        
    def update(self, params: Dict, grads: Dict):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

class Adam(Optimizer):
    """Adam ìµœì í™”"""
    def __init__(self, lr: float = 0.001, beta1: float = 0.9, 
                 beta2: float = 0.999, epsilon: float = 1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = {}
        self.v = {}
        self.t = 0
        
    def update(self, params: Dict, grads: Dict):
        self.t += 1
        
        for key in params.keys():
            if key not in self.m:
                self.m[key] = np.zeros_like(params[key])
                self.v[key] = np.zeros_like(params[key])
            
            # Momentum
            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]
            # RMSprop
            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)
            
            # Bias correction
            m_hat = self.m[key] / (1 - self.beta1 ** self.t)
            v_hat = self.v[key] / (1 - self.beta2 ** self.t)
            
            # Update
            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
```

### í•™ìŠµ í”„ë¡œì„¸ìŠ¤
```python
class Trainer:
    """ì‹ ê²½ë§ í•™ìŠµê¸°"""
    
    def __init__(self, network: CTRPredictionNetwork, 
                 optimizer: Optimizer):
        self.network = network
        self.optimizer = optimizer
        self.loss_history = []
        
    def train(self, x_train: np.ndarray, t_train: np.ndarray,
              x_val: np.ndarray, t_val: np.ndarray,
              epochs: int = 100, batch_size: int = 32):
        
        train_size = x_train.shape[0]
        iter_per_epoch = train_size // batch_size
        
        for epoch in range(epochs):
            # ë°ì´í„° ì…”í”Œ
            indices = np.random.permutation(train_size)
            x_train = x_train[indices]
            t_train = t_train[indices]
            
            epoch_loss = 0
            
            for i in range(iter_per_epoch):
                # ë¯¸ë‹ˆë°°ì¹˜
                batch_mask = slice(i * batch_size, (i + 1) * batch_size)
                x_batch = x_train[batch_mask]
                t_batch = t_train[batch_mask]
                
                # ê¸°ìš¸ê¸° ê³„ì‚°
                grads = self.network.gradient(x_batch, t_batch)
                
                # ë§¤ê°œë³€ìˆ˜ ìˆ˜ì§‘
                params = {}
                for i, layer in enumerate(self.network.layers):
                    if isinstance(layer, Dense):
                        params[f'W{i}'] = layer.W
                        params[f'b{i}'] = layer.b
                
                # ì—…ë°ì´íŠ¸
                self.optimizer.update(params, grads)
                
                # ì—…ë°ì´íŠ¸ëœ ë§¤ê°œë³€ìˆ˜ ì ìš©
                for i, layer in enumerate(self.network.layers):
                    if isinstance(layer, Dense):
                        layer.W = params[f'W{i}']
                        layer.b = params[f'b{i}']
                
                # ì†ì‹¤ ê¸°ë¡
                loss = self.network.loss(x_batch, t_batch)
                epoch_loss += loss
            
            # ê²€ì¦
            val_loss = self.network.loss(x_val, t_val)
            val_acc = self.network.accuracy(x_val, t_val)
            
            print(f"Epoch {epoch+1}/{epochs} - "
                  f"Loss: {epoch_loss/iter_per_epoch:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"Val Acc: {val_acc:.4f}")
```

### ê³ ê¸‰ ê¸°ë²• êµ¬í˜„
```python
class Dropout(Layer):
    """ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´"""
    def __init__(self, dropout_ratio: float = 0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
        self.training = True
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        if self.training:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask / (1.0 - self.dropout_ratio)
        else:
            return x
    
    def backward(self, dout: np.ndarray) -> np.ndarray:
        return dout * self.mask / (1.0 - self.dropout_ratio)

class BatchNormalization(Layer):
    """ë°°ì¹˜ ì •ê·œí™”"""
    def __init__(self, momentum: float = 0.9, epsilon: float = 1e-5):
        self.gamma = None
        self.beta = None
        self.momentum = momentum
        self.epsilon = epsilon
        self.running_mean = None
        self.running_var = None
        
    def forward(self, x: np.ndarray) -> np.ndarray:
        if self.gamma is None:
            self.gamma = np.ones(x.shape[1])
            self.beta = np.zeros(x.shape[1])
            self.running_mean = np.zeros(x.shape[1])
            self.running_var = np.ones(x.shape[1])
        
        if self.training:
            mu = x.mean(axis=0)
            var = x.var(axis=0)
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
        else:
            mu = self.running_mean
            var = self.running_var
        
        self.x_normalized = (x - mu) / np.sqrt(var + self.epsilon)
        return self.gamma * self.x_normalized + self.beta
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ê´‘ê³  CTR ì˜ˆì¸¡ ëª¨ë¸ (ìŠ¤í¬ë˜ì¹˜)**
2. **ì´ë¯¸ì§€ ê¸°ë°˜ ê´‘ê³  ë¶„ë¥˜ CNN**
3. **ì‹œê³„ì—´ ê´‘ê³  ì„±ê³¼ ì˜ˆì¸¡ RNN**
4. **GANì„ ì´ìš©í•œ ê´‘ê³  ì´ë¯¸ì§€ ìƒì„±**