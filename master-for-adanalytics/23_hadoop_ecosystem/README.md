# 23. Hadoop Ecosystem - í•˜ë‘¡ ìƒíƒœê³„

## ğŸ“š ê³¼ì • ì†Œê°œ
ê´‘ê³  ë¹…ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Hadoop ìƒíƒœê³„ë¥¼ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. HDFS, MapReduce, Hive, HBaseë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ê´‘ê³  ë¡œê·¸ ë¶„ì„ê³¼ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ëŒ€ìš©ëŸ‰ ê´‘ê³  ë¡œê·¸ ë°ì´í„° ì €ì¥ ë° ì²˜ë¦¬
- Hiveë¥¼ í†µí•œ ê´‘ê³  ì„±ê³¼ ë¶„ì„
- HBase ì‹¤ì‹œê°„ ê´‘ê³  ì„œë¹™ ì‹œìŠ¤í…œ
- Sqoopì„ í†µí•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## ğŸ“– ì£¼ìš” ë‚´ìš©

### Hadoop ê¸°ë°˜ ê´‘ê³  ë°ì´í„° ì²˜ë¦¬
```python
import os
import pandas as pd
import numpy as np
from hdfs import InsecureClient
from pyhive import hive
import happybase
from sqlalchemy import create_engine
from datetime import datetime, timedelta
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import subprocess
import threading
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AdLog:
    """ê´‘ê³  ë¡œê·¸ ë°ì´í„°"""
    timestamp: datetime
    user_id: str
    campaign_id: str
    ad_id: str
    event_type: str  # impression, click, conversion
    device_type: str
    os: str
    browser: str
    ip_address: str
    country: str
    city: str
    cost: float
    revenue: float = 0.0

class HDFSAdDataManager:
    """HDFS ê´‘ê³  ë°ì´í„° ê´€ë¦¬ì"""
    
    def __init__(self, hdfs_url: str = 'http://localhost:9870'):
        self.client = InsecureClient(hdfs_url)
        self.base_path = '/ad_data'
        self._setup_directories()
        
    def _setup_directories(self):
        """HDFS ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •"""
        directories = [
            f'{self.base_path}/raw_logs',
            f'{self.base_path}/processed',
            f'{self.base_path}/campaigns',
            f'{self.base_path}/users',
            f'{self.base_path}/reports',
            f'{self.base_path}/staging'
        ]
        
        for directory in directories:
            try:
                self.client.makedirs(directory)
                logger.info(f"Created directory: {directory}")
            except Exception as e:
                logger.debug(f"Directory {directory} already exists or error: {e}")
    
    def upload_ad_logs(self, local_file_path: str, 
                      partition_date: str = None) -> str:
        """ê´‘ê³  ë¡œê·¸ HDFS ì—…ë¡œë“œ"""
        if partition_date is None:
            partition_date = datetime.now().strftime('%Y/%m/%d')
        
        hdfs_path = f'{self.base_path}/raw_logs/date={partition_date}'
        
        try:
            self.client.makedirs(hdfs_path)
        except:
            pass  # ë””ë ‰í† ë¦¬ ì´ë¯¸ ì¡´ì¬
        
        file_name = os.path.basename(local_file_path)
        hdfs_file_path = f'{hdfs_path}/{file_name}'
        
        with open(local_file_path, 'rb') as local_file:
            self.client.write(hdfs_file_path, local_file)
        
        logger.info(f"Uploaded {local_file_path} to {hdfs_file_path}")
        return hdfs_file_path
    
    def create_partitioned_table_data(self, logs: List[AdLog]):
        """íŒŒí‹°ì…˜ëœ í…Œì´ë¸” ë°ì´í„° ìƒì„±"""
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘
        partitioned_data = {}
        
        for log in logs:
            date_key = log.timestamp.strftime('%Y/%m/%d')
            if date_key not in partitioned_data:
                partitioned_data[date_key] = []
            
            log_dict = {
                'timestamp': log.timestamp.isoformat(),
                'user_id': log.user_id,
                'campaign_id': log.campaign_id,
                'ad_id': log.ad_id,
                'event_type': log.event_type,
                'device_type': log.device_type,
                'os': log.os,
                'browser': log.browser,
                'ip_address': log.ip_address,
                'country': log.country,
                'city': log.city,
                'cost': log.cost,
                'revenue': log.revenue
            }
            partitioned_data[date_key].append(log_dict)
        
        # ê° íŒŒí‹°ì…˜ë³„ë¡œ JSON íŒŒì¼ ì €ì¥
        for date_key, logs_data in partitioned_data.items():
            hdfs_path = f'{self.base_path}/processed/date={date_key}'
            try:
                self.client.makedirs(hdfs_path)
            except:
                pass
            
            file_path = f'{hdfs_path}/ad_logs.json'
            json_data = '\n'.join([json.dumps(log) for log in logs_data])
            
            self.client.write(file_path, json_data, encoding='utf-8')
            logger.info(f"Created partitioned data: {file_path}")
    
    def list_partitions(self, table_type: str = 'raw_logs') -> List[str]:
        """íŒŒí‹°ì…˜ ëª©ë¡ ì¡°íšŒ"""
        base_path = f'{self.base_path}/{table_type}'
        try:
            partitions = []
            for item in self.client.list(base_path):
                if item.startswith('date='):
                    partitions.append(item)
            return sorted(partitions)
        except Exception as e:
            logger.error(f"Error listing partitions: {e}")
            return []

class HiveAdAnalyzer:
    """Hive ê´‘ê³  ë¶„ì„ê¸°"""
    
    def __init__(self, host: str = 'localhost', port: int = 10000,
                 database: str = 'ad_analytics'):
        self.host = host
        self.port = port
        self.database = database
        self.connection = None
        self._connect()
        self._setup_database()
    
    def _connect(self):
        """Hive ì—°ê²°"""
        try:
            self.connection = hive.Connection(
                host=self.host,
                port=self.port,
                database=self.database
            )
            logger.info(f"Connected to Hive at {self.host}:{self.port}")
        except Exception as e:
            logger.error(f"Failed to connect to Hive: {e}")
    
    def _setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ì„¤ì •"""
        queries = [
            f"CREATE DATABASE IF NOT EXISTS {self.database}",
            f"USE {self.database}",
            """
            CREATE TABLE IF NOT EXISTS ad_logs_external (
                timestamp STRING,
                user_id STRING,
                campaign_id STRING,
                ad_id STRING,
                event_type STRING,
                device_type STRING,
                os STRING,
                browser STRING,
                ip_address STRING,
                country STRING,
                city STRING,
                cost DOUBLE,
                revenue DOUBLE
            )
            PARTITIONED BY (date STRING)
            STORED AS TEXTFILE
            LOCATION '/ad_data/processed'
            """,
            """
            CREATE TABLE IF NOT EXISTS campaign_performance (
                campaign_id STRING,
                date STRING,
                impressions BIGINT,
                clicks BIGINT,
                conversions BIGINT,
                cost DOUBLE,
                revenue DOUBLE,
                ctr DOUBLE,
                cvr DOUBLE,
                roas DOUBLE
            )
            STORED AS PARQUET
            """,
            """
            CREATE TABLE IF NOT EXISTS user_segments (
                user_id STRING,
                segment STRING,
                last_activity STRING,
                total_conversions BIGINT,
                total_revenue DOUBLE,
                avg_session_duration DOUBLE,
                device_preference STRING
            )
            STORED AS PARQUET
            """
        ]
        
        for query in queries:
            try:
                self.execute_query(query)
            except Exception as e:
                logger.error(f"Error executing setup query: {e}")
    
    def execute_query(self, query: str) -> List[Dict]:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        if not self.connection:
            self._connect()
        
        try:
            cursor = self.connection.cursor()
            cursor.execute(query)
            
            if query.strip().upper().startswith('SELECT'):
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()
                return [dict(zip(columns, row)) for row in results]
            else:
                return []
                
        except Exception as e:
            logger.error(f"Query execution error: {e}")
            return []
    
    def analyze_campaign_performance(self, start_date: str, end_date: str) -> List[Dict]:
        """ìº í˜ì¸ ì„±ê³¼ ë¶„ì„"""
        query = f"""
        SELECT 
            campaign_id,
            COUNT(CASE WHEN event_type = 'impression' THEN 1 END) as impressions,
            COUNT(CASE WHEN event_type = 'click' THEN 1 END) as clicks,
            COUNT(CASE WHEN event_type = 'conversion' THEN 1 END) as conversions,
            SUM(cost) as total_cost,
            SUM(revenue) as total_revenue,
            COUNT(CASE WHEN event_type = 'click' THEN 1 END) * 100.0 / 
                NULLIF(COUNT(CASE WHEN event_type = 'impression' THEN 1 END), 0) as ctr,
            COUNT(CASE WHEN event_type = 'conversion' THEN 1 END) * 100.0 / 
                NULLIF(COUNT(CASE WHEN event_type = 'click' THEN 1 END), 0) as cvr,
            SUM(revenue) / NULLIF(SUM(cost), 0) as roas
        FROM ad_logs_external
        WHERE date BETWEEN '{start_date}' AND '{end_date}'
        GROUP BY campaign_id
        ORDER BY total_revenue DESC
        """
        
        return self.execute_query(query)
    
    def analyze_user_behavior(self, date_range: int = 30) -> List[Dict]:
        """ì‚¬ìš©ì í–‰ë™ ë¶„ì„"""
        query = f"""
        WITH user_stats AS (
            SELECT 
                user_id,
                COUNT(DISTINCT campaign_id) as campaigns_engaged,
                COUNT(CASE WHEN event_type = 'impression' THEN 1 END) as impressions,
                COUNT(CASE WHEN event_type = 'click' THEN 1 END) as clicks,
                COUNT(CASE WHEN event_type = 'conversion' THEN 1 END) as conversions,
                SUM(revenue) as total_revenue,
                COUNT(DISTINCT device_type) as device_variety,
                MAX(timestamp) as last_activity
            FROM ad_logs_external
            WHERE date >= date_sub(current_date(), {date_range})
            GROUP BY user_id
        )
        SELECT 
            user_id,
            campaigns_engaged,
            impressions,
            clicks,
            conversions,
            total_revenue,
            CASE 
                WHEN conversions >= 5 AND total_revenue >= 500 THEN 'High Value'
                WHEN conversions >= 2 AND total_revenue >= 100 THEN 'Medium Value'
                WHEN clicks >= 10 THEN 'Active Browser'
                WHEN impressions >= 50 THEN 'Aware User'
                ELSE 'Low Engagement'
            END as user_segment,
            last_activity
        FROM user_stats
        ORDER BY total_revenue DESC
        """
        
        return self.execute_query(query)
    
    def create_daily_performance_report(self, target_date: str):
        """ì¼ë³„ ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        insert_query = f"""
        INSERT OVERWRITE TABLE campaign_performance
        PARTITION (date = '{target_date}')
        SELECT 
            campaign_id,
            COUNT(CASE WHEN event_type = 'impression' THEN 1 END) as impressions,
            COUNT(CASE WHEN event_type = 'click' THEN 1 END) as clicks,
            COUNT(CASE WHEN event_type = 'conversion' THEN 1 END) as conversions,
            SUM(cost) as cost,
            SUM(revenue) as revenue,
            COUNT(CASE WHEN event_type = 'click' THEN 1 END) * 100.0 / 
                NULLIF(COUNT(CASE WHEN event_type = 'impression' THEN 1 END), 0) as ctr,
            COUNT(CASE WHEN event_type = 'conversion' THEN 1 END) * 100.0 / 
                NULLIF(COUNT(CASE WHEN event_type = 'click' THEN 1 END), 0) as cvr,
            SUM(revenue) / NULLIF(SUM(cost), 0) as roas
        FROM ad_logs_external
        WHERE date = '{target_date}'
        GROUP BY campaign_id
        """
        
        self.execute_query(insert_query)
        logger.info(f"Daily performance report created for {target_date}")

class HBaseAdServingSystem:
    """HBase ê´‘ê³  ì„œë¹™ ì‹œìŠ¤í…œ"""
    
    def __init__(self, host: str = 'localhost', port: int = 9090):
        self.connection = happybase.Connection(host=host, port=port)
        self.tables = {
            'user_profiles': 'user_profiles',
            'campaign_configs': 'campaign_configs',
            'real_time_stats': 'real_time_stats',
            'ad_inventory': 'ad_inventory'
        }
        self._setup_tables()
    
    def _setup_tables(self):
        """HBase í…Œì´ë¸” ì„¤ì •"""
        table_schemas = {
            'user_profiles': {
                'demographics': dict(),
                'interests': dict(),
                'behavior': dict(),
                'segments': dict()
            },
            'campaign_configs': {
                'basic': dict(),
                'targeting': dict(), 
                'budget': dict(),
                'performance': dict()
            },
            'real_time_stats': {
                'hourly': dict(),
                'daily': dict(),
                'cumulative': dict()
            },
            'ad_inventory': {
                'creative': dict(),
                'metadata': dict(),
                'performance': dict()
            }
        }
        
        for table_name, column_families in table_schemas.items():
            try:
                if table_name.encode() not in self.connection.tables():
                    self.connection.create_table(table_name, column_families)
                    logger.info(f"Created HBase table: {table_name}")
            except Exception as e:
                logger.error(f"Error creating table {table_name}: {e}")
    
    def store_user_profile(self, user_id: str, profile_data: Dict[str, Any]):
        """ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥"""
        table = self.connection.table('user_profiles')
        
        row_data = {}
        
        # ì¸êµ¬í†µê³„í•™ì  ì •ë³´
        if 'demographics' in profile_data:
            for key, value in profile_data['demographics'].items():
                row_data[f'demographics:{key}'.encode()] = str(value).encode()
        
        # ê´€ì‹¬ì‚¬
        if 'interests' in profile_data:
            for i, interest in enumerate(profile_data['interests']):
                row_data[f'interests:interest_{i}'.encode()] = interest.encode()
        
        # í–‰ë™ íŒ¨í„´
        if 'behavior' in profile_data:
            for key, value in profile_data['behavior'].items():
                row_data[f'behavior:{key}'.encode()] = str(value).encode()
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
        if 'segment' in profile_data:
            row_data[b'segments:current'] = profile_data['segment'].encode()
            row_data[b'segments:updated'] = datetime.now().isoformat().encode()
        
        table.put(user_id.encode(), row_data)
        logger.info(f"Stored user profile for: {user_id}")
    
    def get_user_profile(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ"""
        table = self.connection.table('user_profiles')
        
        try:
            row = table.row(user_id.encode())
            if not row:
                return {}
            
            profile = {
                'demographics': {},
                'interests': [],
                'behavior': {},
                'segment': None
            }
            
            for key, value in row.items():
                key_str = key.decode()
                value_str = value.decode()
                
                if key_str.startswith('demographics:'):
                    field = key_str.split(':', 1)[1]
                    profile['demographics'][field] = value_str
                elif key_str.startswith('interests:'):
                    profile['interests'].append(value_str)
                elif key_str.startswith('behavior:'):
                    field = key_str.split(':', 1)[1]
                    profile['behavior'][field] = value_str
                elif key_str == 'segments:current':
                    profile['segment'] = value_str
            
            return profile
            
        except Exception as e:
            logger.error(f"Error getting user profile {user_id}: {e}")
            return {}
    
    def update_real_time_stats(self, campaign_id: str, event_type: str, 
                              value: float = 1.0, hour: int = None):
        """ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸"""
        if hour is None:
            hour = datetime.now().hour
        
        table = self.connection.table('real_time_stats')
        row_key = f"{campaign_id}_{datetime.now().strftime('%Y%m%d')}"
        
        # ì›ìì  ì¦ê°€ ì—°ì‚°
        try:
            # ì‹œê°„ë³„ í†µê³„
            hourly_key = f'hourly:{event_type}_{hour:02d}'.encode()
            table.counter_inc(row_key.encode(), hourly_key, int(value))
            
            # ì¼ë³„ ëˆ„ì  í†µê³„
            daily_key = f'daily:{event_type}'.encode()
            table.counter_inc(row_key.encode(), daily_key, int(value))
            
            # ì „ì²´ ëˆ„ì  í†µê³„
            cumulative_key = f'cumulative:{event_type}'.encode()
            table.counter_inc(row_key.encode(), cumulative_key, int(value))
            
        except Exception as e:
            logger.error(f"Error updating real-time stats: {e}")
    
    def get_campaign_stats(self, campaign_id: str, date: str = None) -> Dict[str, int]:
        """ìº í˜ì¸ í†µê³„ ì¡°íšŒ"""
        if date is None:
            date = datetime.now().strftime('%Y%m%d')
        
        table = self.connection.table('real_time_stats')
        row_key = f"{campaign_id}_{date}"
        
        try:
            row = table.row(row_key.encode())
            stats = {}
            
            for key, value in row.items():
                key_str = key.decode()
                stats[key_str] = int.from_bytes(value, byteorder='big', signed=True)
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting campaign stats: {e}")
            return {}

class MapReduceAdProcessor:
    """MapReduce ê´‘ê³  ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self, hadoop_home: str = '/usr/local/hadoop'):
        self.hadoop_home = hadoop_home
        self.streaming_jar = f"{hadoop_home}/share/hadoop/tools/lib/hadoop-streaming-*.jar"
        
    def run_daily_aggregation(self, input_path: str, output_path: str, date: str):
        """ì¼ë³„ ì§‘ê³„ MapReduce ì‘ì—…"""
        
        # Mapper ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        mapper_script = """#!/usr/bin/env python3
import sys
import json
from datetime import datetime

for line in sys.stdin:
    try:
        log = json.loads(line.strip())
        campaign_id = log.get('campaign_id', 'unknown')
        event_type = log.get('event_type', 'unknown')
        cost = float(log.get('cost', 0))
        revenue = float(log.get('revenue', 0))
        
        # í‚¤: campaign_id, ê°’: event_type,cost,revenue
        key = campaign_id
        value = f"{event_type},{cost},{revenue}"
        print(f"{key}\\t{value}")
        
    except Exception as e:
        continue
"""
        
        # Reducer ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        reducer_script = """#!/usr/bin/env python3
import sys
from collections import defaultdict

current_campaign = None
stats = defaultdict(lambda: {'count': 0, 'cost': 0.0, 'revenue': 0.0})

for line in sys.stdin:
    try:
        campaign_id, value = line.strip().split('\\t')
        event_type, cost, revenue = value.split(',')
        cost = float(cost)
        revenue = float(revenue)
        
        if current_campaign != campaign_id:
            if current_campaign:
                # ì´ì „ ìº í˜ì¸ ê²°ê³¼ ì¶œë ¥
                output_stats(current_campaign, stats)
            
            current_campaign = campaign_id
            stats = defaultdict(lambda: {'count': 0, 'cost': 0.0, 'revenue': 0.0})
        
        stats[event_type]['count'] += 1
        stats[event_type]['cost'] += cost
        stats[event_type]['revenue'] += revenue
        
    except Exception as e:
        continue

# ë§ˆì§€ë§‰ ìº í˜ì¸ ì²˜ë¦¬
if current_campaign:
    output_stats(current_campaign, stats)

def output_stats(campaign_id, stats):
    impressions = stats['impression']['count']
    clicks = stats['click']['count'] 
    conversions = stats['conversion']['count']
    total_cost = sum(s['cost'] for s in stats.values())
    total_revenue = sum(s['revenue'] for s in stats.values())
    
    ctr = (clicks / impressions * 100) if impressions > 0 else 0
    cvr = (conversions / clicks * 100) if clicks > 0 else 0
    roas = (total_revenue / total_cost) if total_cost > 0 else 0
    
    result = f"{campaign_id},{impressions},{clicks},{conversions},{total_cost:.2f},{total_revenue:.2f},{ctr:.2f},{cvr:.2f},{roas:.2f}"
    print(result)
"""
        
        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì €ì¥
        with open('/tmp/mapper.py', 'w') as f:
            f.write(mapper_script)
        with open('/tmp/reducer.py', 'w') as f:
            f.write(reducer_script)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        os.chmod('/tmp/mapper.py', 0o755)
        os.chmod('/tmp/reducer.py', 0o755)
        
        # MapReduce ì‘ì—… ì‹¤í–‰
        cmd = [
            'hadoop', 'jar', self.streaming_jar,
            '-files', '/tmp/mapper.py,/tmp/reducer.py',
            '-mapper', 'mapper.py',
            '-reducer', 'reducer.py',
            '-input', input_path,
            '-output', output_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"MapReduce job completed successfully")
                logger.info(f"Output: {result.stdout}")
            else:
                logger.error(f"MapReduce job failed: {result.stderr}")
        except Exception as e:
            logger.error(f"Error running MapReduce job: {e}")

class SqoopDataImporter:
    """Sqoop ë°ì´í„° ì„í¬íŠ¸"""
    
    def __init__(self, mysql_host: str = 'localhost', 
                 mysql_user: str = 'root', mysql_password: str = 'password'):
        self.mysql_host = mysql_host
        self.mysql_user = mysql_user
        self.mysql_password = mysql_password
        
    def import_campaign_data(self, database: str, table: str, 
                           hdfs_target: str = '/ad_data/campaigns'):
        """ìº í˜ì¸ ë°ì´í„° ì„í¬íŠ¸"""
        cmd = [
            'sqoop', 'import',
            '--connect', f'jdbc:mysql://{self.mysql_host}/{database}',
            '--username', self.mysql_user,
            '--password', self.mysql_password,
            '--table', table,
            '--target-dir', hdfs_target,
            '--split-by', 'campaign_id',
            '--fields-terminated-by', '\\t',
            '--null-string', '\\\\N',
            '--null-non-string', '\\\\N'
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"Sqoop import completed: {table}")
            else:
                logger.error(f"Sqoop import failed: {result.stderr}")
        except Exception as e:
            logger.error(f"Error running Sqoop import: {e}")
    
    def export_to_mysql(self, hdfs_source: str, database: str, table: str):
        """HDFSì—ì„œ MySQLë¡œ ë°ì´í„° ìµìŠ¤í¬íŠ¸"""
        cmd = [
            'sqoop', 'export',
            '--connect', f'jdbc:mysql://{self.mysql_host}/{database}',
            '--username', self.mysql_user,
            '--password', self.mysql_password,
            '--table', table,
            '--export-dir', hdfs_source,
            '--fields-terminated-by', '\\t',
            '--update-mode', 'allowinsert',
            '--update-key', 'campaign_id,date'
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"Sqoop export completed: {table}")
            else:
                logger.error(f"Sqoop export failed: {result.stderr}")
        except Exception as e:
            logger.error(f"Error running Sqoop export: {e}")

# ì‚¬ìš© ì˜ˆì‹œ ë° í†µí•© íŒŒì´í”„ë¼ì¸
def example_hadoop_ad_pipeline():
    """Hadoop ê´‘ê³  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ"""
    print("=== Hadoop ê´‘ê³  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
    
    # 1. ìƒ˜í”Œ ê´‘ê³  ë¡œê·¸ ë°ì´í„° ìƒì„±
    sample_logs = []
    campaigns = ['camp_001', 'camp_002', 'camp_003']
    events = ['impression', 'click', 'conversion']
    devices = ['mobile', 'desktop', 'tablet']
    countries = ['KR', 'US', 'JP', 'CN']
    
    np.random.seed(42)
    
    for i in range(10000):
        log = AdLog(
            timestamp=datetime.now() - timedelta(days=np.random.randint(0, 30)),
            user_id=f'user_{np.random.randint(1, 1000):05d}',
            campaign_id=np.random.choice(campaigns),
            ad_id=f'ad_{np.random.randint(1, 100):03d}',
            event_type=np.random.choice(events, p=[0.7, 0.25, 0.05]),
            device_type=np.random.choice(devices),
            os=np.random.choice(['iOS', 'Android', 'Windows', 'MacOS']),
            browser=np.random.choice(['Chrome', 'Safari', 'Firefox', 'Edge']),
            ip_address=f'{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}',
            country=np.random.choice(countries),
            city=np.random.choice(['Seoul', 'New York', 'Tokyo', 'Beijing']),
            cost=np.random.uniform(0.1, 5.0),
            revenue=np.random.uniform(0, 50) if np.random.random() < 0.1 else 0.0
        )
        sample_logs.append(log)
    
    try:
        # 2. HDFS ë°ì´í„° ê´€ë¦¬
        print("\n--- HDFS ë°ì´í„° ì—…ë¡œë“œ ---")
        hdfs_manager = HDFSAdDataManager()
        hdfs_manager.create_partitioned_table_data(sample_logs)
        
        partitions = hdfs_manager.list_partitions('processed')
        print(f"ìƒì„±ëœ íŒŒí‹°ì…˜: {len(partitions)}ê°œ")
        
    except Exception as e:
        print(f"HDFS ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
    
    try:
        # 3. Hive ë¶„ì„
        print("\n--- Hive ë°ì´í„° ë¶„ì„ ---")
        hive_analyzer = HiveAdAnalyzer()
        
        # ìµœê·¼ 7ì¼ê°„ ìº í˜ì¸ ì„±ê³¼ ë¶„ì„
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        end_date = datetime.now().strftime('%Y-%m-%d')
        
        performance_results = hive_analyzer.analyze_campaign_performance(start_date, end_date)
        print(f"ìº í˜ì¸ ì„±ê³¼ ë¶„ì„ ê²°ê³¼: {len(performance_results)}ê°œ ìº í˜ì¸")
        
        # ì‚¬ìš©ì í–‰ë™ ë¶„ì„
        user_behavior = hive_analyzer.analyze_user_behavior(30)
        print(f"ì‚¬ìš©ì í–‰ë™ ë¶„ì„ ê²°ê³¼: {len(user_behavior)}ëª…")
        
    except Exception as e:
        print(f"Hive ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    try:
        # 4. HBase ì‹¤ì‹œê°„ ë°ì´í„°
        print("\n--- HBase ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ ---")
        hbase_system = HBaseAdServingSystem()
        
        # ìƒ˜í”Œ ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥
        sample_profile = {
            'demographics': {'age': 28, 'gender': 'M', 'income': 'high'},
            'interests': ['technology', 'gaming', 'sports'],
            'behavior': {'session_duration': 300, 'pages_per_session': 5},
            'segment': 'high_value'
        }
        
        hbase_system.store_user_profile('user_00001', sample_profile)
        
        # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
        for i in range(100):
            campaign_id = np.random.choice(campaigns)
            event_type = np.random.choice(events, p=[0.7, 0.25, 0.05])
            hbase_system.update_real_time_stats(campaign_id, event_type)
        
        # í†µê³„ ì¡°íšŒ
        stats = hbase_system.get_campaign_stats('camp_001')
        print(f"ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(stats)}ê°œ ì§€í‘œ")
        
    except Exception as e:
        print(f"HBase ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
    
    # 5. ê²°ê³¼ ìš”ì•½
    print("\n=== íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
    print("âœ… HDFSì— íŒŒí‹°ì…˜ëœ ë°ì´í„° ì €ì¥")
    print("âœ… Hiveë¡œ ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰") 
    print("âœ… HBaseì— ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥")
    print("âœ… í†µí•© ê´‘ê³  ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ")
    
    return {
        'logs_processed': len(sample_logs),
        'partitions_created': len(partitions) if 'partitions' in locals() else 0,
        'campaigns_analyzed': len(performance_results) if 'performance_results' in locals() else 0,
        'users_analyzed': len(user_behavior) if 'user_behavior' in locals() else 0
    }

if __name__ == "__main__":
    results = example_hadoop_ad_pipeline()
    print(f"Hadoop ê´‘ê³  ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì²˜ë¦¬ëœ ë¡œê·¸: {results['logs_processed']}ê°œ")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ëŒ€ìš©ëŸ‰ ê´‘ê³  ë¡œê·¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ**
2. **ì‹¤ì‹œê°„ ê´‘ê³  ì„œë¹™ í”Œë«í¼**
3. **Hive ê¸°ë°˜ ê´‘ê³  ë¶„ì„ ëŒ€ì‹œë³´ë“œ**
4. **í†µí•© ê´‘ê³  ë°ì´í„° íŒŒì´í”„ë¼ì¸**