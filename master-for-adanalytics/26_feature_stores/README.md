# 26. Feature Stores - í”¼ì²˜ ìŠ¤í† ì–´

## ğŸ“š ê³¼ì • ì†Œê°œ
ê´‘ê³  ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ í”¼ì²˜ ìŠ¤í† ì–´ ì•„í‚¤í…ì²˜ë¥¼ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ íŠ¹ì„± ì„œë¹™, íŠ¹ì„± íŒŒì´í”„ë¼ì¸, ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ê¹Œì§€ í¬ê´„ì ì¸ MLOps í”¼ì²˜ ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ëŒ€ê·œëª¨ í”¼ì²˜ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë¹™ ì‹œìŠ¤í…œ
- í”¼ì²˜ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
- A/B í…ŒìŠ¤íŠ¸ìš© í”¼ì²˜ ê´€ë¦¬

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê´‘ê³  í”¼ì²˜ ìŠ¤í† ì–´ ì‹œìŠ¤í…œ
```python
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json
import logging
import redis
import sqlite3
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
import time
from collections import defaultdict
import pickle

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

Base = declarative_base()

@dataclass
class FeatureDefinition:
    """í”¼ì²˜ ì •ì˜ í´ë˜ìŠ¤"""
    name: str
    description: str
    feature_type: str  # categorical, numerical, binary, embedding
    data_source: str
    computation_logic: str
    update_frequency: str  # real_time, hourly, daily, weekly
    version: str
    tags: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    validation_rules: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

@dataclass
class FeatureValue:
    """í”¼ì²˜ ê°’ í´ë˜ìŠ¤"""
    entity_id: str
    feature_name: str
    value: Union[float, int, str, List]
    timestamp: datetime
    version: str
    metadata: Dict[str, Any] = field(default_factory=dict)

class FeatureRegistry(Base):
    """í”¼ì²˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í…Œì´ë¸”"""
    __tablename__ = 'feature_registry'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(255), unique=True, nullable=False)
    description = Column(Text)
    feature_type = Column(String(50))
    data_source = Column(String(255))
    computation_logic = Column(Text)
    update_frequency = Column(String(50))
    version = Column(String(50))
    tags = Column(Text)  # JSON string
    dependencies = Column(Text)  # JSON string
    validation_rules = Column(Text)  # JSON string
    created_at = Column(DateTime, default=datetime.now)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now)
    is_active = Column(Boolean, default=True)

class FeatureStore:
    """í”¼ì²˜ ìŠ¤í† ì–´ ê´€ë¦¬ì"""
    
    def __init__(self, database_url: str = "sqlite:///feature_store.db",
                 redis_host: str = "localhost", redis_port: int = 6379):
        self.engine = create_engine(database_url)
        Base.metadata.create_all(self.engine)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        
        # Redis ì—°ê²° (ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë¹™ìš©)
        try:
            self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis_client.ping()
            logger.info("Redis ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            self.redis_client = None
        
        # í”¼ì²˜ ìºì‹œ
        self.feature_cache = {}
        self.feature_definitions = {}
        
        # ì‹¤í–‰ì í’€
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        self.load_feature_definitions()
    
    def register_feature(self, feature_def: FeatureDefinition) -> bool:
        """í”¼ì²˜ ë“±ë¡"""
        try:
            feature_registry = FeatureRegistry(
                name=feature_def.name,
                description=feature_def.description,
                feature_type=feature_def.feature_type,
                data_source=feature_def.data_source,
                computation_logic=feature_def.computation_logic,
                update_frequency=feature_def.update_frequency,
                version=feature_def.version,
                tags=json.dumps(feature_def.tags),
                dependencies=json.dumps(feature_def.dependencies),
                validation_rules=json.dumps(feature_def.validation_rules)
            )
            
            # ê¸°ì¡´ í”¼ì²˜ ë¹„í™œì„±í™”
            existing = self.session.query(FeatureRegistry).filter_by(name=feature_def.name).first()
            if existing:
                existing.is_active = False
            
            self.session.add(feature_registry)
            self.session.commit()
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
            self.feature_definitions[feature_def.name] = feature_def
            
            logger.info(f"í”¼ì²˜ ë“±ë¡ ì™„ë£Œ: {feature_def.name}")
            return True
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ë“±ë¡ ì‹¤íŒ¨ {feature_def.name}: {e}")
            self.session.rollback()
            return False
    
    def load_feature_definitions(self):
        """í”¼ì²˜ ì •ì˜ ë¡œë“œ"""
        try:
            features = self.session.query(FeatureRegistry).filter_by(is_active=True).all()
            
            for feature in features:
                feature_def = FeatureDefinition(
                    name=feature.name,
                    description=feature.description,
                    feature_type=feature.feature_type,
                    data_source=feature.data_source,
                    computation_logic=feature.computation_logic,
                    update_frequency=feature.update_frequency,
                    version=feature.version,
                    tags=json.loads(feature.tags) if feature.tags else [],
                    dependencies=json.loads(feature.dependencies) if feature.dependencies else [],
                    validation_rules=json.loads(feature.validation_rules) if feature.validation_rules else {},
                    created_at=feature.created_at,
                    updated_at=feature.updated_at
                )
                
                self.feature_definitions[feature.name] = feature_def
            
            logger.info(f"í”¼ì²˜ ì •ì˜ ë¡œë“œ ì™„ë£Œ: {len(self.feature_definitions)}ê°œ")
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ì •ì˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_feature_definition(self, feature_name: str) -> Optional[FeatureDefinition]:
        """í”¼ì²˜ ì •ì˜ ì¡°íšŒ"""
        return self.feature_definitions.get(feature_name)
    
    def list_features(self, tags: List[str] = None) -> List[FeatureDefinition]:
        """í”¼ì²˜ ëª©ë¡ ì¡°íšŒ"""
        features = list(self.feature_definitions.values())
        
        if tags:
            features = [f for f in features if any(tag in f.tags for tag in tags)]
        
        return features

class AdFeatureComputer:
    """ê´‘ê³  í”¼ì²˜ ê³„ì‚°ê¸°"""
    
    def __init__(self, feature_store: FeatureStore):
        self.feature_store = feature_store
        self.computation_functions = {
            'user_ctr': self._compute_user_ctr,
            'user_cvr': self._compute_user_cvr,
            'user_ltv': self._compute_user_ltv,
            'user_engagement_score': self._compute_user_engagement,
            'user_recency': self._compute_user_recency,
            'user_frequency': self._compute_user_frequency,
            'user_monetary': self._compute_user_monetary,
            'campaign_performance': self._compute_campaign_performance,
            'ad_quality_score': self._compute_ad_quality_score,
            'audience_similarity': self._compute_audience_similarity,
            'seasonal_factor': self._compute_seasonal_factor,
            'device_preference': self._compute_device_preference,
            'time_of_day_factor': self._compute_time_factor,
            'keyword_relevance': self._compute_keyword_relevance,
            'competitor_pressure': self._compute_competitor_pressure
        }
    
    def compute_feature(self, feature_name: str, entity_id: str, 
                       context: Dict[str, Any] = None) -> Optional[FeatureValue]:
        """ê°œë³„ í”¼ì²˜ ê³„ì‚°"""
        if feature_name not in self.computation_functions:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” í”¼ì²˜: {feature_name}")
            return None
        
        try:
            feature_def = self.feature_store.get_feature_definition(feature_name)
            if not feature_def:
                logger.error(f"í”¼ì²˜ ì •ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {feature_name}")
                return None
            
            # í”¼ì²˜ ê³„ì‚°
            computation_func = self.computation_functions[feature_name]
            value = computation_func(entity_id, context or {})
            
            feature_value = FeatureValue(
                entity_id=entity_id,
                feature_name=feature_name,
                value=value,
                timestamp=datetime.now(),
                version=feature_def.version,
                metadata={'computation_context': context}
            )
            
            return feature_value
            
        except Exception as e:
            logger.error(f"í”¼ì²˜ ê³„ì‚° ì‹¤íŒ¨ {feature_name}: {e}")
            return None
    
    def compute_feature_set(self, feature_names: List[str], entity_id: str,
                          context: Dict[str, Any] = None) -> Dict[str, FeatureValue]:
        """í”¼ì²˜ ì…‹ ì¼ê´„ ê³„ì‚°"""
        results = {}
        
        # ì˜ì¡´ì„± ì •ë ¬
        sorted_features = self._sort_by_dependencies(feature_names)
        
        for feature_name in sorted_features:
            feature_value = self.compute_feature(feature_name, entity_id, context)
            if feature_value:
                results[feature_name] = feature_value
                # ì»¨í…ìŠ¤íŠ¸ì— ê³„ì‚°ëœ í”¼ì²˜ ì¶”ê°€ (ì˜ì¡´ì„± ì²˜ë¦¬ìš©)
                if context is None:
                    context = {}
                context[f"computed_{feature_name}"] = feature_value.value
        
        return results
    
    def _sort_by_dependencies(self, feature_names: List[str]) -> List[str]:
        """ì˜ì¡´ì„±ì— ë”°ë¥¸ í”¼ì²˜ ì •ë ¬"""
        # ê°„ë‹¨í•œ í† í´ë¡œì§€ ì •ë ¬ êµ¬í˜„
        sorted_features = []
        remaining = feature_names.copy()
        
        while remaining:
            # ì˜ì¡´ì„±ì´ ì—†ê±°ë‚˜ ëª¨ë“  ì˜ì¡´ì„±ì´ í•´ê²°ëœ í”¼ì²˜ ì°¾ê¸°
            ready_features = []
            for feature_name in remaining:
                feature_def = self.feature_store.get_feature_definition(feature_name)
                if not feature_def or not feature_def.dependencies:
                    ready_features.append(feature_name)
                else:
                    # ëª¨ë“  ì˜ì¡´ì„±ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    deps_resolved = all(dep in sorted_features for dep in feature_def.dependencies 
                                      if dep in feature_names)
                    if deps_resolved:
                        ready_features.append(feature_name)
            
            if not ready_features:
                # ìˆœí™˜ ì˜ì¡´ì„±ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‚˜ë¨¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€
                sorted_features.extend(remaining)
                break
            
            sorted_features.extend(ready_features)
            remaining = [f for f in remaining if f not in ready_features]
        
        return sorted_features
    
    # ê°œë³„ í”¼ì²˜ ê³„ì‚° í•¨ìˆ˜ë“¤
    def _compute_user_ctr(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì CTR ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ìì˜ ê³¼ê±° í´ë¦­/ë…¸ì¶œ ë°ì´í„°ë¥¼ ì¡°íšŒ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(hash(user_id) % 2**32)
        base_ctr = np.random.uniform(0.01, 0.15)
        
        # ì»¨í…ìŠ¤íŠ¸ ìš”ì¸ ë°˜ì˜
        if context.get('device_type') == 'mobile':
            base_ctr *= 1.2
        if context.get('time_of_day', 12) in [19, 20, 21]:  # ì €ë… ì‹œê°„ëŒ€
            base_ctr *= 1.1
        
        return round(base_ctr, 4)
    
    def _compute_user_cvr(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì CVR ê³„ì‚°"""
        np.random.seed(hash(user_id + "cvr") % 2**32)
        base_cvr = np.random.uniform(0.005, 0.08)
        
        # êµ¬ë§¤ë ¥ ì§€í‘œ ë°˜ì˜
        if context.get('income_level') == 'high':
            base_cvr *= 1.5
        elif context.get('income_level') == 'low':
            base_cvr *= 0.7
        
        return round(base_cvr, 4)
    
    def _compute_user_ltv(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì ìƒì• ê°€ì¹˜ ê³„ì‚°"""
        np.random.seed(hash(user_id + "ltv") % 2**32)
        base_ltv = np.random.uniform(50, 2000)
        
        # ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë°˜ì˜
        engagement_score = context.get('computed_user_engagement_score', 0.5)
        base_ltv *= (1 + engagement_score)
        
        return round(base_ltv, 2)
    
    def _compute_user_engagement(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°"""
        np.random.seed(hash(user_id + "engagement") % 2**32)
        
        # ë‹¤ì–‘í•œ ì°¸ì—¬ ì§€í‘œ ì¢…í•©
        factors = {
            'session_duration': np.random.uniform(0, 1),
            'page_views': np.random.uniform(0, 1),  
            'interaction_rate': np.random.uniform(0, 1),
            'return_frequency': np.random.uniform(0, 1)
        }
        
        engagement_score = sum(factors.values()) / len(factors)
        return round(engagement_score, 3)
    
    def _compute_user_recency(self, user_id: str, context: Dict[str, Any]) -> int:
        """ì‚¬ìš©ì ìµœê·¼ì„± (ë§ˆì§€ë§‰ í™œë™ ì´í›„ ì¼ìˆ˜)"""
        np.random.seed(hash(user_id + "recency") % 2**32)
        return np.random.randint(0, 365)  # 0-365ì¼
    
    def _compute_user_frequency(self, user_id: str, context: Dict[str, Any]) -> int:
        """ì‚¬ìš©ì ë¹ˆë„ (30ì¼ê°„ í™œë™ ì¼ìˆ˜)"""
        np.random.seed(hash(user_id + "frequency") % 2**32)
        return np.random.randint(1, 31)  # 1-30ì¼
    
    def _compute_user_monetary(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì êµ¬ë§¤ë ¥ (30ì¼ê°„ êµ¬ë§¤ ê¸ˆì•¡)"""
        np.random.seed(hash(user_id + "monetary") % 2**32)
        return round(np.random.uniform(0, 5000), 2)
    
    def _compute_campaign_performance(self, campaign_id: str, context: Dict[str, Any]) -> Dict[str, float]:
        """ìº í˜ì¸ ì„±ê³¼ ì§€í‘œ"""
        np.random.seed(hash(campaign_id) % 2**32)
        return {
            'ctr': round(np.random.uniform(0.01, 0.1), 4),
            'cvr': round(np.random.uniform(0.01, 0.05), 4),
            'roas': round(np.random.uniform(1.0, 8.0), 2),
            'quality_score': round(np.random.uniform(1, 10), 1)
        }
    
    def _compute_ad_quality_score(self, ad_id: str, context: Dict[str, Any]) -> float:
        """ê´‘ê³  í’ˆì§ˆ ì ìˆ˜"""
        np.random.seed(hash(ad_id) % 2**32)
        return round(np.random.uniform(1, 10), 1)
    
    def _compute_audience_similarity(self, user_id: str, context: Dict[str, Any]) -> float:
        """ì˜¤ë””ì–¸ìŠ¤ ìœ ì‚¬ë„"""
        target_audience = context.get('target_audience', {})
        user_profile = context.get('user_profile', {})
        
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_score = 0.5  # ê¸°ë³¸ê°’
        
        if target_audience.get('age_group') == user_profile.get('age_group'):
            similarity_score += 0.2
        if target_audience.get('gender') == user_profile.get('gender'):
            similarity_score += 0.1
        if any(interest in user_profile.get('interests', []) 
               for interest in target_audience.get('interests', [])):
            similarity_score += 0.2
        
        return round(min(similarity_score, 1.0), 3)
    
    def _compute_seasonal_factor(self, entity_id: str, context: Dict[str, Any]) -> float:
        """ê³„ì ˆì„± ìš”ì¸"""
        current_month = datetime.now().month
        
        # ì›”ë³„ ê³„ì ˆì„± íŒ©í„° (ì˜ˆ: 11-12ì›” ë†’ìŒ)
        seasonal_factors = {
            1: 0.8, 2: 0.7, 3: 0.9, 4: 1.0, 5: 1.1, 6: 1.2,
            7: 1.1, 8: 1.0, 9: 0.9, 10: 1.0, 11: 1.3, 12: 1.4
        }
        
        return seasonal_factors.get(current_month, 1.0)
    
    def _compute_device_preference(self, user_id: str, context: Dict[str, Any]) -> str:
        """ë””ë°”ì´ìŠ¤ ì„ í˜¸ë„"""
        np.random.seed(hash(user_id + "device") % 2**32)
        return np.random.choice(['mobile', 'desktop', 'tablet'], p=[0.6, 0.3, 0.1])
    
    def _compute_time_factor(self, entity_id: str, context: Dict[str, Any]) -> float:
        """ì‹œê°„ëŒ€ë³„ ìš”ì¸"""
        current_hour = datetime.now().hour
        
        # ì‹œê°„ëŒ€ë³„ í™œë™ íŒ©í„°
        if 9 <= current_hour <= 12:  # ì˜¤ì „
            return 1.1
        elif 13 <= current_hour <= 18:  # ì˜¤í›„
            return 1.2
        elif 19 <= current_hour <= 22:  # ì €ë…
            return 1.3
        else:  # ìƒˆë²½/ë°¤
            return 0.8
    
    def _compute_keyword_relevance(self, entity_id: str, context: Dict[str, Any]) -> float:
        """í‚¤ì›Œë“œ ê´€ë ¨ì„±"""
        keywords = context.get('keywords', [])
        user_interests = context.get('user_interests', [])
        
        if not keywords or not user_interests:
            return 0.5
        
        # í‚¤ì›Œë“œì™€ ê´€ì‹¬ì‚¬ ë§¤ì¹­
        matches = sum(1 for keyword in keywords if keyword in user_interests)
        relevance = matches / len(keywords)
        
        return round(relevance, 3)
    
    def _compute_competitor_pressure(self, entity_id: str, context: Dict[str, Any]) -> float:
        """ê²½ìŸ ì••ë°•ë„"""
        # ì¹´í…Œê³ ë¦¬ë³„ ê²½ìŸ ì••ë°•ë„ ì‹œë®¬ë ˆì´ì…˜
        category = context.get('category', 'general')
        
        pressure_map = {
            'electronics': 0.8,
            'fashion': 0.7,
            'automotive': 0.6,
            'finance': 0.9,
            'general': 0.5
        }
        
        return pressure_map.get(category, 0.5)

class RealTimeFeatureServer:
    """ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë¹™ ì„œë²„"""
    
    def __init__(self, feature_store: FeatureStore, feature_computer: AdFeatureComputer):
        self.feature_store = feature_store
        self.feature_computer = feature_computer
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ
    
    def get_features(self, entity_id: str, feature_names: List[str],
                    context: Dict[str, Any] = None) -> Dict[str, Any]:
        """í”¼ì²˜ ì¡°íšŒ (ìºì‹œ ìš°ì„ )"""
        results = {}
        missing_features = []
        
        # Redis ìºì‹œì—ì„œ ì¡°íšŒ
        if self.feature_store.redis_client:
            for feature_name in feature_names:
                cache_key = f"feature:{entity_id}:{feature_name}"
                cached_value = self.feature_store.redis_client.get(cache_key)
                
                if cached_value:
                    try:
                        feature_value = pickle.loads(cached_value.encode('latin1'))
                        results[feature_name] = feature_value
                    except Exception as e:
                        logger.warning(f"ìºì‹œ ë””ì‹œë¦¬ì–¼ë¼ì´ì œì´ì…˜ ì‹¤íŒ¨ {cache_key}: {e}")
                        missing_features.append(feature_name)
                else:
                    missing_features.append(feature_name)
        else:
            missing_features = feature_names
        
        # ìºì‹œì— ì—†ëŠ” í”¼ì²˜ë“¤ ê³„ì‚°
        if missing_features:
            computed_features = self.feature_computer.compute_feature_set(
                missing_features, entity_id, context
            )
            
            # ê²°ê³¼ ë³‘í•© ë° ìºì‹œ ì €ì¥
            for feature_name, feature_value in computed_features.items():
                results[feature_name] = feature_value
                
                # Redisì— ìºì‹œ
                if self.feature_store.redis_client:
                    cache_key = f"feature:{entity_id}:{feature_name}"
                    try:
                        serialized_value = pickle.dumps(feature_value).decode('latin1')
                        self.feature_store.redis_client.setex(
                            cache_key, self.cache_ttl, serialized_value
                        )
                    except Exception as e:
                        logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {cache_key}: {e}")
        
        return results
    
    async def get_features_async(self, entity_id: str, feature_names: List[str],
                               context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë¹„ë™ê¸° í”¼ì²˜ ì¡°íšŒ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.feature_store.executor,
            self.get_features,
            entity_id,
            feature_names,
            context
        )
    
    def batch_get_features(self, entity_ids: List[str], feature_names: List[str],
                          contexts: List[Dict[str, Any]] = None) -> Dict[str, Dict[str, Any]]:
        """ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ"""
        if contexts is None:
            contexts = [{}] * len(entity_ids)
        
        results = {}
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_entity = {
                executor.submit(self.get_features, entity_id, feature_names, context): entity_id
                for entity_id, context in zip(entity_ids, contexts)
            }
            
            for future in as_completed(future_to_entity):
                entity_id = future_to_entity[future]
                try:
                    entity_features = future.result()
                    results[entity_id] = entity_features
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ ì‹¤íŒ¨ {entity_id}: {e}")
                    results[entity_id] = {}
        
        return results

class FeatureValidator:
    """í”¼ì²˜ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.validation_rules = {
            'range': self._validate_range,
            'type': self._validate_type,
            'not_null': self._validate_not_null,
            'enum': self._validate_enum,
            'regex': self._validate_regex,
            'custom': self._validate_custom
        }
    
    def validate_feature(self, feature_value: FeatureValue,
                        validation_rules: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """í”¼ì²˜ ê°’ ê²€ì¦"""
        errors = []
        
        for rule_type, rule_config in validation_rules.items():
            if rule_type not in self.validation_rules:
                continue
            
            validator = self.validation_rules[rule_type]
            is_valid, error_msg = validator(feature_value.value, rule_config)
            
            if not is_valid:
                errors.append(f"{feature_value.feature_name}: {error_msg}")
        
        return len(errors) == 0, errors
    
    def _validate_range(self, value, config) -> Tuple[bool, str]:
        """ë²”ìœ„ ê²€ì¦"""
        if not isinstance(value, (int, float)):
            return False, f"ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤: {value}"
        
        min_val = config.get('min')
        max_val = config.get('max')
        
        if min_val is not None and value < min_val:
            return False, f"ìµœì†Ÿê°’ {min_val}ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤: {value}"
        
        if max_val is not None and value > max_val:
            return False, f"ìµœëŒ“ê°’ {max_val}ë³´ë‹¤ í½ë‹ˆë‹¤: {value}"
        
        return True, ""
    
    def _validate_type(self, value, config) -> Tuple[bool, str]:
        """íƒ€ì… ê²€ì¦"""
        expected_type = config.get('expected_type')
        
        if expected_type == 'string' and not isinstance(value, str):
            return False, f"ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(value)}"
        elif expected_type == 'number' and not isinstance(value, (int, float)):
            return False, f"ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤: {type(value)}"
        elif expected_type == 'list' and not isinstance(value, list):
            return False, f"ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(value)}"
        
        return True, ""
    
    def _validate_not_null(self, value, config) -> Tuple[bool, str]:
        """Not Null ê²€ì¦"""
        if value is None or (isinstance(value, str) and len(value.strip()) == 0):
            return False, "ê°’ì´ ì—†ìŠµë‹ˆë‹¤"
        return True, ""
    
    def _validate_enum(self, value, config) -> Tuple[bool, str]:
        """ì—´ê±°í˜• ê²€ì¦"""
        allowed_values = config.get('allowed_values', [])
        if value not in allowed_values:
            return False, f"í—ˆìš©ë˜ì§€ ì•ŠëŠ” ê°’ì…ë‹ˆë‹¤. í—ˆìš©ê°’: {allowed_values}"
        return True, ""
    
    def _validate_regex(self, value, config) -> Tuple[bool, str]:
        """ì •ê·œì‹ ê²€ì¦"""
        import re
        pattern = config.get('pattern')
        if pattern and not re.match(pattern, str(value)):
            return False, f"íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pattern}"
        return True, ""
    
    def _validate_custom(self, value, config) -> Tuple[bool, str]:
        """ì»¤ìŠ¤í…€ ê²€ì¦"""
        # ì»¤ìŠ¤í…€ ê²€ì¦ ë¡œì§ ì‹¤í–‰
        validation_function = config.get('function')
        if validation_function and callable(validation_function):
            try:
                is_valid = validation_function(value)
                if not is_valid:
                    return False, "ì»¤ìŠ¤í…€ ê²€ì¦ ì‹¤íŒ¨"
            except Exception as e:
                return False, f"ì»¤ìŠ¤í…€ ê²€ì¦ ì˜¤ë¥˜: {e}"
        
        return True, ""

class FeatureMonitor:
    """í”¼ì²˜ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, feature_store: FeatureStore):
        self.feature_store = feature_store
        self.drift_thresholds = {
            'statistical': 0.05,  # p-value ì„ê³„ê°’
            'distribution': 0.1,   # KL divergence ì„ê³„ê°’
            'mean_shift': 0.2      # í‰ê·  ë³€í™” ì„ê³„ê°’
        }
    
    def detect_feature_drift(self, feature_name: str, 
                           baseline_period_days: int = 30,
                           current_period_days: int = 7) -> Dict[str, Any]:
        """í”¼ì²˜ ë“œë¦¬í”„íŠ¸ íƒì§€"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ë¥¼ ì¡°íšŒ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        
        np.random.seed(42)
        
        # ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° (ê³¼ê±° 30ì¼)
        baseline_data = np.random.normal(0.5, 0.1, 1000)
        
        # í˜„ì¬ ë°ì´í„° (ìµœê·¼ 7ì¼) - ì•½ê°„ì˜ ë“œë¦¬í”„íŠ¸ í¬í•¨
        current_data = np.random.normal(0.55, 0.12, 200)  # í‰ê· ì´ ì¦ê°€
        
        # í†µê³„ì  ê²€ì •
        from scipy import stats
        statistic, p_value = stats.ks_2samp(baseline_data, current_data)
        
        # ë¶„í¬ ë³€í™” ê³„ì‚°
        mean_baseline = np.mean(baseline_data)
        mean_current = np.mean(current_data)
        mean_shift = abs(mean_current - mean_baseline) / mean_baseline
        
        std_baseline = np.std(baseline_data)
        std_current = np.std(current_data)
        std_shift = abs(std_current - std_baseline) / std_baseline
        
        # ë“œë¦¬í”„íŠ¸ íŒì •
        drift_detected = (
            p_value < self.drift_thresholds['statistical'] or
            mean_shift > self.drift_thresholds['mean_shift']
        )
        
        return {
            'feature_name': feature_name,
            'drift_detected': drift_detected,
            'statistical_test': {
                'ks_statistic': statistic,
                'p_value': p_value,
                'significant': p_value < self.drift_thresholds['statistical']
            },
            'distribution_changes': {
                'mean_baseline': mean_baseline,
                'mean_current': mean_current,
                'mean_shift_ratio': mean_shift,
                'std_baseline': std_baseline,
                'std_current': std_current,
                'std_shift_ratio': std_shift
            },
            'drift_severity': self._calculate_drift_severity(mean_shift, std_shift, p_value),
            'recommendations': self._generate_drift_recommendations(drift_detected, mean_shift, p_value)
        }
    
    def _calculate_drift_severity(self, mean_shift: float, std_shift: float, p_value: float) -> str:
        """ë“œë¦¬í”„íŠ¸ ì‹¬ê°ë„ ê³„ì‚°"""
        if mean_shift > 0.5 or std_shift > 0.5 or p_value < 0.001:
            return 'high'
        elif mean_shift > 0.2 or std_shift > 0.2 or p_value < 0.01:
            return 'medium'
        elif mean_shift > 0.1 or std_shift > 0.1 or p_value < 0.05:
            return 'low'
        else:
            return 'none'
    
    def _generate_drift_recommendations(self, drift_detected: bool, 
                                      mean_shift: float, p_value: float) -> List[str]:
        """ë“œë¦¬í”„íŠ¸ ëŒ€ì‘ ì¶”ì²œì‚¬í•­"""
        recommendations = []
        
        if not drift_detected:
            recommendations.append("í˜„ì¬ í”¼ì²˜ëŠ” ì•ˆì •ì ì…ë‹ˆë‹¤.")
        else:
            recommendations.append("í”¼ì²˜ ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            if mean_shift > 0.3:
                recommendations.append("í”¼ì²˜ ì¬ê³„ì‚° ë¡œì§ì„ ê²€í† í•˜ì„¸ìš”.")
                recommendations.append("ë°ì´í„° ì†ŒìŠ¤ì— ë³€í™”ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            
            if p_value < 0.001:
                recommendations.append("ëª¨ë¸ ì¬í•™ìŠµì„ ê³ ë ¤í•˜ì„¸ìš”.")
                recommendations.append("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì¬ê²€í† í•˜ì„¸ìš”.")
            
            recommendations.append("ì•Œë¦¼ ì„¤ì •ì„ í†µí•´ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
        
        return recommendations
    
    def monitor_feature_freshness(self) -> Dict[str, Any]:
        """í”¼ì²˜ ì‹ ì„ ë„ ëª¨ë‹ˆí„°ë§"""
        freshness_report = {}
        
        for feature_name, feature_def in self.feature_store.feature_definitions.items():
            expected_update_freq = feature_def.update_frequency
            
            # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            np.random.seed(hash(feature_name) % 2**32)
            hours_since_update = np.random.randint(0, 48)
            last_update = datetime.now() - timedelta(hours=hours_since_update)
            
            # ì‹ ì„ ë„ íŒì •
            freshness_status = self._evaluate_freshness(expected_update_freq, hours_since_update)
            
            freshness_report[feature_name] = {
                'last_update': last_update.isoformat(),
                'hours_since_update': hours_since_update,
                'expected_frequency': expected_update_freq,
                'freshness_status': freshness_status,
                'is_stale': freshness_status in ['stale', 'very_stale']
            }
        
        return freshness_report
    
    def _evaluate_freshness(self, expected_freq: str, hours_since_update: int) -> str:
        """ì‹ ì„ ë„ í‰ê°€"""
        thresholds = {
            'real_time': {'fresh': 1, 'stale': 6, 'very_stale': 24},
            'hourly': {'fresh': 2, 'stale': 12, 'very_stale': 48},
            'daily': {'fresh': 25, 'stale': 72, 'very_stale': 168},
            'weekly': {'fresh': 168, 'stale': 336, 'very_stale': 672}
        }
        
        threshold = thresholds.get(expected_freq, thresholds['daily'])
        
        if hours_since_update <= threshold['fresh']:
            return 'fresh'
        elif hours_since_update <= threshold['stale']:
            return 'stale'
        else:
            return 'very_stale'

# ì‚¬ìš© ì˜ˆì‹œ ë° í†µí•© ì‹œìŠ¤í…œ
def setup_ad_feature_store():
    """ê´‘ê³  í”¼ì²˜ ìŠ¤í† ì–´ ì„¤ì •"""
    print("=== ê´‘ê³  í”¼ì²˜ ìŠ¤í† ì–´ ì„¤ì • ===")
    
    # í”¼ì²˜ ìŠ¤í† ì–´ ì´ˆê¸°í™”
    feature_store = FeatureStore()
    
    # ê´‘ê³  ê´€ë ¨ í”¼ì²˜ ì •ì˜ë“¤
    ad_features = [
        FeatureDefinition(
            name="user_ctr",
            description="ì‚¬ìš©ì í´ë¦­ë¥ ",
            feature_type="numerical",
            data_source="ad_interactions",
            computation_logic="clicks / impressions for user",
            update_frequency="hourly",
            version="1.0",
            tags=["user", "performance", "ctr"],
            validation_rules={
                "range": {"min": 0, "max": 1},
                "type": {"expected_type": "number"}
            }
        ),
        FeatureDefinition(
            name="user_cvr",
            description="ì‚¬ìš©ì ì „í™˜ìœ¨",
            feature_type="numerical",
            data_source="ad_interactions",
            computation_logic="conversions / clicks for user",
            update_frequency="hourly",
            version="1.0",
            tags=["user", "performance", "conversion"],
            validation_rules={
                "range": {"min": 0, "max": 1},
                "type": {"expected_type": "number"}
            }
        ),
        FeatureDefinition(
            name="user_ltv",
            description="ì‚¬ìš©ì ìƒì• ê°€ì¹˜",
            feature_type="numerical",
            data_source="transactions",
            computation_logic="sum of user transactions with time decay",
            update_frequency="daily",
            version="1.0",
            tags=["user", "value", "ltv"],
            dependencies=["user_engagement_score"],
            validation_rules={
                "range": {"min": 0, "max": 50000},
                "type": {"expected_type": "number"}
            }
        ),
        FeatureDefinition(
            name="user_engagement_score",
            description="ì‚¬ìš©ì ì°¸ì—¬ë„ ì ìˆ˜",
            feature_type="numerical",
            data_source="user_activities",
            computation_logic="weighted combination of user activities",
            update_frequency="hourly",
            version="1.0",
            tags=["user", "engagement", "behavior"],
            validation_rules={
                "range": {"min": 0, "max": 1},
                "type": {"expected_type": "number"}
            }
        ),
        FeatureDefinition(
            name="seasonal_factor",
            description="ê³„ì ˆì„± ìš”ì¸",
            feature_type="numerical",
            data_source="calendar",
            computation_logic="seasonal adjustment based on month/week",
            update_frequency="daily",
            version="1.0",
            tags=["temporal", "seasonal", "external"],
            validation_rules={
                "range": {"min": 0.1, "max": 2.0},
                "type": {"expected_type": "number"}
            }
        )
    ]
    
    # í”¼ì²˜ ë“±ë¡
    for feature_def in ad_features:
        success = feature_store.register_feature(feature_def)
        if success:
            print(f"âœ… í”¼ì²˜ ë“±ë¡ ì„±ê³µ: {feature_def.name}")
        else:
            print(f"âŒ í”¼ì²˜ ë“±ë¡ ì‹¤íŒ¨: {feature_def.name}")
    
    return feature_store

def example_feature_store_system():
    """í”¼ì²˜ ìŠ¤í† ì–´ ì‹œìŠ¤í…œ ì˜ˆì‹œ"""
    print("=== ê´‘ê³  í”¼ì²˜ ìŠ¤í† ì–´ ì‹œìŠ¤í…œ ===")
    
    # 1. í”¼ì²˜ ìŠ¤í† ì–´ ì„¤ì •
    feature_store = setup_ad_feature_store()
    
    # 2. í”¼ì²˜ ê³„ì‚°ê¸° ì´ˆê¸°í™”
    feature_computer = AdFeatureComputer(feature_store)
    
    # 3. ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë²„ ì´ˆê¸°í™”
    feature_server = RealTimeFeatureServer(feature_store, feature_computer)
    
    # 4. í”¼ì²˜ ê²€ì¦ê¸° ë° ëª¨ë‹ˆí„°ë§
    validator = FeatureValidator()
    monitor = FeatureMonitor(feature_store)
    
    # 5. ìƒ˜í”Œ ì‚¬ìš©ìì— ëŒ€í•œ í”¼ì²˜ ê³„ì‚°
    print("\n=== í”¼ì²˜ ê³„ì‚° í…ŒìŠ¤íŠ¸ ===")
    sample_user_id = "user_12345"
    feature_names = ["user_ctr", "user_cvr", "user_ltv", "user_engagement_score", "seasonal_factor"]
    
    context = {
        'device_type': 'mobile',
        'time_of_day': 20,
        'income_level': 'high',
        'user_interests': ['technology', 'gaming'],
        'keywords': ['smartphone', 'gaming'],
        'category': 'electronics'
    }
    
    # í”¼ì²˜ ì¡°íšŒ
    features = feature_server.get_features(sample_user_id, feature_names, context)
    
    print(f"ì‚¬ìš©ì {sample_user_id}ì˜ í”¼ì²˜:")
    for feature_name, feature_value in features.items():
        print(f"  {feature_name}: {feature_value.value}")
    
    # 6. ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ í…ŒìŠ¤íŠ¸
    print("\n=== ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ===")
    user_ids = [f"user_{i:05d}" for i in range(10)]
    contexts = [context] * len(user_ids)
    
    batch_features = feature_server.batch_get_features(user_ids, ["user_ctr", "user_cvr"], contexts)
    print(f"ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: {len(batch_features)}ëª…ì˜ ì‚¬ìš©ì")
    
    # 7. í”¼ì²˜ ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("\n=== í”¼ì²˜ ê²€ì¦ í…ŒìŠ¤íŠ¸ ===")
    if features:
        sample_feature = list(features.values())[0]
        feature_def = feature_store.get_feature_definition(sample_feature.feature_name)
        
        is_valid, errors = validator.validate_feature(sample_feature, feature_def.validation_rules)
        print(f"í”¼ì²˜ ê²€ì¦ ê²°ê³¼: {'âœ… í†µê³¼' if is_valid else 'âŒ ì‹¤íŒ¨'}")
        if errors:
            for error in errors:
                print(f"  ì˜¤ë¥˜: {error}")
    
    # 8. ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
    print("\n=== ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ===")
    drift_report = monitor.detect_feature_drift("user_ctr")
    print(f"ë“œë¦¬í”„íŠ¸ ê°ì§€: {'ğŸš¨ ìˆìŒ' if drift_report['drift_detected'] else 'âœ… ì—†ìŒ'}")
    print(f"ì‹¬ê°ë„: {drift_report['drift_severity']}")
    
    # 9. ì‹ ì„ ë„ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
    print("\n=== ì‹ ì„ ë„ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ===")
    freshness_report = monitor.monitor_feature_freshness()
    stale_features = [name for name, info in freshness_report.items() if info['is_stale']]
    print(f"ì‹ ì„ í•˜ì§€ ì•Šì€ í”¼ì²˜: {len(stale_features)}ê°œ")
    
    # 10. í”¼ì²˜ ëª©ë¡ ì¡°íšŒ
    print("\n=== ë“±ë¡ëœ í”¼ì²˜ ëª©ë¡ ===")
    all_features = feature_store.list_features()
    print(f"ì´ {len(all_features)}ê°œ í”¼ì²˜ ë“±ë¡ë¨:")
    for feature_def in all_features:
        print(f"  - {feature_def.name} ({feature_def.feature_type}, {feature_def.update_frequency})")
    
    # ì„±ëŠ¥ í†µê³„
    performance_stats = {
        'total_features': len(all_features),
        'users_processed': len(batch_features),
        'features_computed': len(features),
        'drift_detected_features': sum(1 for report in [drift_report] if report['drift_detected']),
        'stale_features': len(stale_features)
    }
    
    return performance_stats

if __name__ == "__main__":
    results = example_feature_store_system()
    print(f"\n=== í”¼ì²˜ ìŠ¤í† ì–´ ì‹œìŠ¤í…œ ì™„ë£Œ ===")
    print(f"ì´ í”¼ì²˜ ìˆ˜: {results['total_features']}")
    print(f"ì²˜ë¦¬ëœ ì‚¬ìš©ì: {results['users_processed']}ëª…")
    print(f"ê³„ì‚°ëœ í”¼ì²˜: {results['features_computed']}ê°œ")
    print(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ í”¼ì²˜: {results['drift_detected_features']}ê°œ")
    print(f"ì‹ ì„ í•˜ì§€ ì•Šì€ í”¼ì²˜: {results['stale_features']}ê°œ")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë¹™ í”Œë«í¼**
2. **í”¼ì²˜ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**
3. **MLOps í”¼ì²˜ íŒŒì´í”„ë¼ì¸**
4. **A/B í…ŒìŠ¤íŠ¸ í”¼ì²˜ ê´€ë¦¬ ë„êµ¬**