# 64. Advertising Measurement Theory - Í¥ëÍ≥† Ï∏°Ï†ï Ïù¥Î°†

## üìö Í≥ºÏ†ï ÏÜåÍ∞ú
Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ïÏùò Í≥ºÌïôÏ†Å Ï†ëÍ∑ºÎ≤ïÏùÑ ÎßàÏä§ÌÑ∞Ìï©ÎãàÎã§. Adstock Î™®Îç∏Î∂ÄÌÑ∞ Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ÍπåÏßÄ Ï†ïÌôïÌïú Í¥ëÍ≥† ROI Ï∏°Ï†ïÍ≥º ÏµúÏ†ÅÌôî Î∞©Î≤ïÏùÑ ÌïôÏäµÌï©ÎãàÎã§.

## üéØ ÌïôÏäµ Î™©Ìëú
- AdstockÍ≥º Í¥ëÍ≥† ÏßÄÏó∞ Ìö®Í≥º Î™®Îç∏ÎßÅ
- Î∏åÎûúÎìú Ïù∏ÏßÄÎèÑÏôÄ Í¥ëÍ≥† ÎßàÎ™® Ïù¥Î°† Ï†ÅÏö©
- Ïû•Îã®Í∏∞ Í¥ëÍ≥† Ìö®Í≥º Î∂ÑÌï¥ÏôÄ Ï∏°Ï†ï
- Î©ÄÌã∞ ÌÑ∞Ïπò Ïñ¥Ìä∏Î¶¨Î∑∞ÏÖòÍ≥º Ï¶ùÎ∂Ñ Î¶¨ÌîÑÌä∏ Î∂ÑÏÑù

## üìñ Ï£ºÏöî ÎÇ¥Ïö©

### Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ï ÏãúÏä§ÌÖú
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, optimize
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings('ignore')

class AdvertisingMeasurementSystem:
    """Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ïÏùÑ ÏúÑÌïú Ï¢ÖÌï© ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        self.models = {}
        self.adstock_parameters = {}
        self.saturation_curves = {}
        
    def adstock_modeling(self, media_data, sales_data, max_lag=12):
        """Adstock Î™®Îç∏ÎßÅ - Í¥ëÍ≥†Ïùò ÏßÄÏó∞ Î∞è ÎàÑÏ†Å Ìö®Í≥º"""
        
        def geometric_adstock(media_series, decay_rate):
            """Í∏∞ÌïòÍ∏âÏàòÏ†Å Í∞êÏÜå Adstock"""
            adstock_series = media_series.copy()
            
            for i in range(1, len(media_series)):
                adstock_series.iloc[i] += adstock_series.iloc[i-1] * decay_rate
            
            return adstock_series
        
        def convoluted_adstock(media_series, decay_params):
            """Ìï©ÏÑ±Í≥± Adstock (Îçî Ïú†Ïó∞Ìïú Í∞êÏÜå Ìå®ÌÑ¥)"""
            adstock_series = np.zeros(len(media_series))
            
            for i in range(len(media_series)):
                for lag in range(min(i+1, len(decay_params))):
                    adstock_series[i] += media_series.iloc[i-lag] * decay_params[lag]
            
            return pd.Series(adstock_series, index=media_series.index)
        
        # Í∞Å ÎØ∏ÎîîÏñ¥ Ï±ÑÎÑêÎ≥Ñ ÏµúÏ†Å Adstock ÌååÎùºÎØ∏ÌÑ∞ Ï∞æÍ∏∞
        optimal_adstock = {}
        
        for channel in media_data.columns:
            # Í≤©Ïûê ÌÉêÏÉâÏúºÎ°ú ÏµúÏ†Å decay rate Ï∞æÍ∏∞
            best_r2 = -np.inf
            best_params = {}
            
            for decay_rate in np.arange(0.1, 0.9, 0.1):
                # Adstock Ï†ÅÏö©
                adstock_media = geometric_adstock(media_data[channel], decay_rate)
                
                # Í∞ÑÎã®Ìïú ÌöåÍ∑Ä Î™®Îç∏Î°ú ÏÑ±Í≥º ÏòàÏ∏°
                X = adstock_media.values.reshape(-1, 1)
                y = sales_data.values
                
                model = LinearRegression()
                model.fit(X, y)
                
                r2 = model.score(X, y)
                
                if r2 > best_r2:
                    best_r2 = r2
                    best_params = {
                        'decay_rate': decay_rate,
                        'r2_score': r2,
                        'adstock_series': adstock_media
                    }
            
            optimal_adstock[channel] = best_params
            self.adstock_parameters[channel] = best_params['decay_rate']
        
        # Í≥†Í∏â Ìï©ÏÑ±Í≥± Adstock Î™®Îç∏ÎßÅ
        advanced_adstock = {}
        for channel in media_data.columns:
            # Îçî Ï†ïÍµêÌïú ÏßÄÏó∞ Ìö®Í≥º Ìå®ÌÑ¥ ÌïôÏäµ
            decay_params = self._optimize_convoluted_adstock(
                media_data[channel], sales_data, max_lag
            )
            
            advanced_adstock[channel] = {
                'decay_params': decay_params,
                'adstock_series': convoluted_adstock(media_data[channel], decay_params)
            }
        
        return {
            'geometric_adstock': optimal_adstock,
            'convoluted_adstock': advanced_adstock,
            'adstock_visualization': self._visualize_adstock_effects(optimal_adstock)
        }
    
    def saturation_curve_modeling(self, media_spend, response_data):
        """Ìè¨Ìôî Í≥°ÏÑ† Î™®Îç∏ÎßÅ - ÏàòÌôï Ï≤¥Í∞ê Î≤ïÏπô"""
        
        def hill_saturation(x, alpha, gamma):
            """Hill Î≥ÄÌôò (S-curve)"""
            return alpha * (x ** gamma) / (x ** gamma + 1)
        
        def michaelis_menten(x, vmax, km):
            """Michaelis-Menten Ìè¨Ìôî Í≥°ÏÑ†"""
            return (vmax * x) / (km + x)
        
        def exponential_saturation(x, a, b):
            """ÏßÄÏàòÏ†Å Ìè¨Ìôî Í≥°ÏÑ†"""
            return a * (1 - np.exp(-b * x))
        
        saturation_models = {}
        
        for channel in media_spend.columns:
            x_data = media_spend[channel].values
            y_data = response_data.values
            
            # Í∞Å Ìè¨Ìôî Î™®Îç∏ ÌîºÌåÖ
            models = {
                'hill': (hill_saturation, [1.0, 1.0]),
                'michaelis_menten': (michaelis_menten, [max(y_data), np.median(x_data)]),
                'exponential': (exponential_saturation, [max(y_data), 0.001])
            }
            
            best_model = None
            best_r2 = -np.inf
            
            for model_name, (func, initial_params) in models.items():
                try:
                    # Í≥°ÏÑ† ÌîºÌåÖ
                    popt, _ = optimize.curve_fit(func, x_data, y_data, p0=initial_params)
                    
                    # ÏòàÏ∏° Î∞è ÌèâÍ∞Ä
                    y_pred = func(x_data, *popt)
                    r2 = r2_score(y_data, y_pred)
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = {
                            'name': model_name,
                            'function': func,
                            'parameters': popt,
                            'r2_score': r2,
                            'predictions': y_pred
                        }
                
                except Exception as e:
                    continue
            
            saturation_models[channel] = best_model
            self.saturation_curves[channel] = best_model
        
        # ÏµúÏ†Å ÏòàÏÇ∞ Ìè¨Ïù∏Ìä∏ Í≥ÑÏÇ∞
        optimal_spend_points = self._calculate_optimal_spend_points(saturation_models)
        
        return {
            'saturation_models': saturation_models,
            'optimal_spend_points': optimal_spend_points,
            'saturation_visualization': self._visualize_saturation_curves(saturation_models, media_spend)
        }
    
    def brand_lift_measurement(self, exposed_group, control_group, pre_metrics, post_metrics):
        """Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ Ï∏°Ï†ï"""
        
        # ÏÇ¨Ï†Ñ-ÏÇ¨ÌõÑ ÎπÑÍµê Î∂ÑÏÑù
        def calculate_lift(exposed_pre, exposed_post, control_pre, control_post):
            """Î¶¨ÌîÑÌä∏ Í≥ÑÏÇ∞ Í≥µÏãù"""
            exposed_change = (exposed_post - exposed_pre) / exposed_pre
            control_change = (control_post - control_pre) / control_pre
            
            return exposed_change - control_change
        
        lift_metrics = {}
        
        # Í∞Å Î∏åÎûúÎìú Î©îÌä∏Î¶≠Î≥Ñ Î¶¨ÌîÑÌä∏ Í≥ÑÏÇ∞
        for metric in pre_metrics.columns:
            # ÎÖ∏Ï∂ú Í∑∏Î£π Î¶¨ÌîÑÌä∏
            exposed_lift = calculate_lift(
                pre_metrics[pre_metrics['group'] == 'exposed'][metric].mean(),
                post_metrics[post_metrics['group'] == 'exposed'][metric].mean(),
                pre_metrics[pre_metrics['group'] == 'control'][metric].mean(),
                post_metrics[post_metrics['group'] == 'control'][metric].mean()
            )
            
            # ÌÜµÍ≥ÑÏ†Å Ïú†ÏùòÏÑ± Í≤ÄÏ†ï
            exposed_post_values = post_metrics[post_metrics['group'] == 'exposed'][metric]
            control_post_values = post_metrics[post_metrics['group'] == 'control'][metric]
            
            t_stat, p_value = stats.ttest_ind(exposed_post_values, control_post_values)
            
            # Ìö®Í≥º ÌÅ¨Í∏∞ (Cohen's d)
            pooled_std = np.sqrt(((len(exposed_post_values)-1) * exposed_post_values.std()**2 + 
                                 (len(control_post_values)-1) * control_post_values.std()**2) / 
                                (len(exposed_post_values) + len(control_post_values) - 2))
            
            cohens_d = (exposed_post_values.mean() - control_post_values.mean()) / pooled_std
            
            lift_metrics[metric] = {
                'lift_percentage': exposed_lift * 100,
                't_statistic': t_stat,
                'p_value': p_value,
                'cohens_d': cohens_d,
                'significance': 'Significant' if p_value < 0.05 else 'Not Significant',
                'effect_size': self._interpret_effect_size(abs(cohens_d))
            }
        
        # Ï¢ÖÌï© Î∏åÎûúÎìú Í±¥Í∞ïÎèÑ Ï†êÏàò
        brand_health_score = self._calculate_brand_health_score(lift_metrics)
        
        # ÏãúÍ∞ÅÌôî
        self._visualize_brand_lift(lift_metrics, pre_metrics, post_metrics)
        
        return {
            'lift_metrics': lift_metrics,
            'brand_health_score': brand_health_score,
            'statistical_power': self._calculate_statistical_power(exposed_group, control_group)
        }
    
    def incrementality_testing(self, test_data, control_data, experiment_period):
        """Ï¶ùÎ∂Ñ Ìö®Í≥º ÌÖåÏä§Ìä∏"""
        
        # Difference-in-Differences Î∂ÑÏÑù
        def did_analysis(test_before, test_after, control_before, control_after):
            """Ïù¥Ï§ëÏ∞®Î∂ÑÎ≤ï Î∂ÑÏÑù"""
            test_change = test_after - test_before
            control_change = control_after - control_before
            
            incremental_effect = test_change - control_change
            
            # ÌëúÏ§Ä Ïò§Ï∞® Í≥ÑÏÇ∞
            test_se = np.sqrt(test_before.var() + test_after.var())
            control_se = np.sqrt(control_before.var() + control_after.var())
            combined_se = np.sqrt(test_se**2 + control_se**2)
            
            # t-ÌÜµÍ≥ÑÎüâÍ≥º p-Í∞í
            t_stat = incremental_effect / combined_se
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=len(test_before)+len(control_before)-2))
            
            return {
                'incremental_effect': incremental_effect,
                'standard_error': combined_se,
                't_statistic': t_stat,
                'p_value': p_value
            }
        
        # Ïã§Ìóò Í∏∞Í∞Ñ Ï†ÑÌõÑ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨
        pre_period = experiment_period['start'] - pd.Timedelta(days=30)
        
        test_before = test_data[test_data['date'] < experiment_period['start']]['metric']
        test_after = test_data[test_data['date'] >= experiment_period['start']]['metric']
        control_before = control_data[control_data['date'] < experiment_period['start']]['metric']
        control_after = control_data[control_data['date'] >= experiment_period['start']]['metric']
        
        # DID Î∂ÑÏÑù Ïã§Ìñâ
        did_results = did_analysis(test_before, test_after, control_before, control_after)
        
        # Ìï©ÏÑ± ÎåÄÏ°∞Íµ∞ Î∞©Î≤ï (Synthetic Control)
        synthetic_control = self._create_synthetic_control(test_data, control_data, experiment_period)
        
        # Ïù∏Í≥º Ï∂îÎ°† Î™®Îç∏
        causal_impact = self._causal_impact_analysis(test_data, control_data, experiment_period)
        
        return {
            'did_results': did_results,
            'synthetic_control': synthetic_control,
            'causal_impact': causal_impact,
            'incremental_roi': self._calculate_incremental_roi(did_results, experiment_period)
        }
    
    def attribution_decay_modeling(self, touchpoint_data, conversion_data):
        """Ïñ¥Ìä∏Î¶¨Î∑∞ÏÖò Í∞êÏá† Î™®Îç∏ÎßÅ"""
        
        # ÏãúÍ∞Ñ Í∏∞Î∞ò Í∞êÏá† Ìï®ÏàòÎì§
        def linear_decay(days_since_touch, max_days=30):
            """ÏÑ†Ìòï Í∞êÏá†"""
            return max(0, 1 - days_since_touch / max_days)
        
        def exponential_decay(days_since_touch, decay_rate=0.1):
            """ÏßÄÏàò Í∞êÏá†"""
            return np.exp(-decay_rate * days_since_touch)
        
        def u_shaped_decay(days_since_touch, max_days=30):
            """UÏûêÌòï Í∞êÏá† (ÏµúÍ∑ºÏÑ± + Ï≤´Ïù∏ÏÉÅ Ìö®Í≥º)"""
            normalized_days = days_since_touch / max_days
            return 0.5 * (1 - normalized_days) + 0.5 * np.exp(-5 * normalized_days)
        
        # ÌÑ∞ÏπòÌè¨Ïù∏Ìä∏Î≥Ñ Í∏∞Ïó¨ÎèÑ Í≥ÑÏÇ∞
        attribution_weights = {}
        
        for conversion_id in conversion_data['conversion_id'].unique():
            # Ìï¥Îãπ Ï†ÑÌôòÏùò ÌÑ∞ÏπòÌè¨Ïù∏Ìä∏Îì§
            touchpoints = touchpoint_data[
                touchpoint_data['conversion_id'] == conversion_id
            ].sort_values('timestamp')
            
            if len(touchpoints) == 0:
                continue
            
            # Ï†ÑÌôò ÏãúÏ†êÎ∂ÄÌÑ∞Ïùò Í≤ΩÍ≥º ÏùºÏàò Í≥ÑÏÇ∞
            conversion_time = conversion_data[
                conversion_data['conversion_id'] == conversion_id
            ]['conversion_time'].iloc[0]
            
            touchpoints['days_since'] = (
                conversion_time - touchpoints['timestamp']
            ).dt.days
            
            # Í∞Å Í∞êÏá† Î™®Îç∏ Ï†ÅÏö©
            decay_models = {
                'linear': linear_decay,
                'exponential': exponential_decay,
                'u_shaped': u_shaped_decay
            }
            
            for model_name, decay_func in decay_models.items():
                weights = touchpoints['days_since'].apply(decay_func)
                normalized_weights = weights / weights.sum()
                
                if conversion_id not in attribution_weights:
                    attribution_weights[conversion_id] = {}
                
                attribution_weights[conversion_id][model_name] = dict(
                    zip(touchpoints['touchpoint_id'], normalized_weights)
                )
        
        # Ï±ÑÎÑêÎ≥Ñ Ï¥ù Í∏∞Ïó¨ÎèÑ ÏßëÍ≥Ñ
        channel_attribution = self._aggregate_channel_attribution(
            attribution_weights, touchpoint_data
        )
        
        # Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê
        model_performance = self._compare_attribution_models(
            channel_attribution, conversion_data
        )
        
        return {
            'attribution_weights': attribution_weights,
            'channel_attribution': channel_attribution,
            'model_performance': model_performance,
            'recommended_model': max(model_performance.items(), key=lambda x: x[1]['auc'])[0]
        }
    
    def advertising_wearout_analysis(self, creative_data, performance_data):
        """Í¥ëÍ≥† ÎßàÎ™® Î∂ÑÏÑù"""
        
        # Í¥ëÍ≥†Î≥Ñ ÏÑ±Í≥º Ï∂îÏù¥ Î∂ÑÏÑù
        wearout_patterns = {}
        
        for creative_id in creative_data['creative_id'].unique():
            # Ìï¥Îãπ Í¥ëÍ≥†Ïùò ÏãúÍ≥ÑÏó¥ ÏÑ±Í≥º Îç∞Ïù¥ÌÑ∞
            creative_performance = performance_data[
                performance_data['creative_id'] == creative_id
            ].sort_values('date')
            
            if len(creative_performance) < 10:  # ÏµúÏÜå Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏
                continue
            
            # ÏÑ±Í≥º ÏßÄÌëúÎ≥Ñ ÎßàÎ™® Ìå®ÌÑ¥ Î∂ÑÏÑù
            metrics = ['ctr', 'conversion_rate', 'cpm']
            
            for metric in metrics:
                if metric not in creative_performance.columns:
                    continue
                
                # Ìä∏Î†åÎìú Î∂ÑÏÑù
                x = np.arange(len(creative_performance))
                y = creative_performance[metric].values
                
                # ÏÑ†Ìòï ÌöåÍ∑ÄÎ°ú Ï†ÑÎ∞òÏ†Å Ìä∏Î†åÎìú ÌååÏïÖ
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                
                # Î≥ÄÍ≥°Ï†ê ÌÉêÏßÄ (ÏÑ±Í≥º Í∏âÎùΩ ÏãúÏ†ê)
                change_points = self._detect_change_points(y)
                
                # ÎßàÎ™®Ïú® Í≥ÑÏÇ∞
                wearout_rate = -slope if slope < 0 else 0
                
                wearout_patterns[creative_id] = {
                    'metric': metric,
                    'slope': slope,
                    'r_squared': r_value**2,
                    'p_value': p_value,
                    'wearout_rate': wearout_rate,
                    'change_points': change_points,
                    'days_to_wearout': self._estimate_days_to_wearout(slope, y),
                    'performance_data': creative_performance
                }
        
        # Í¥ëÍ≥† ÍµêÏ≤¥ Í∂åÍ≥†ÏÇ¨Ìï≠
        replacement_recommendations = self._generate_replacement_recommendations(wearout_patterns)
        
        # ÎßàÎ™® Ìå®ÌÑ¥ ÏãúÍ∞ÅÌôî
        self._visualize_wearout_patterns(wearout_patterns)
        
        return {
            'wearout_patterns': wearout_patterns,
            'replacement_recommendations': replacement_recommendations,
            'average_wearout_days': np.mean([
                p['days_to_wearout'] for p in wearout_patterns.values() 
                if p['days_to_wearout'] is not None
            ])
        }

# Î≥¥Ï°∞ Î©îÏÑúÎìúÎì§
    def _optimize_convoluted_adstock(self, media_series, sales_series, max_lag):
        """Ìï©ÏÑ±Í≥± Adstock ÏµúÏ†ÅÌôî"""
        best_params = None
        best_r2 = -np.inf
        
        # Î≤†Ïù¥ÏßÄÏïà ÏµúÏ†ÅÌôî ÎòêÎäî Í≤©Ïûê ÌÉêÏÉâÏúºÎ°ú ÏµúÏ†Å Í∞êÏá† Ìå®ÌÑ¥ Ï∞æÍ∏∞
        for _ in range(50):  # ÎûúÎç§ ÏÑúÏπò
            decay_params = np.random.exponential(0.5, max_lag)
            decay_params = decay_params / decay_params.sum()  # Ï†ïÍ∑úÌôî
            
            # Adstock Ï†ÅÏö©
            adstock_series = np.zeros(len(media_series))
            for i in range(len(media_series)):
                for lag in range(min(i+1, len(decay_params))):
                    adstock_series[i] += media_series.iloc[i-lag] * decay_params[lag]
            
            # ÏÑ±Í≥º ÏòàÏ∏°
            X = adstock_series.reshape(-1, 1)
            y = sales_series.values
            
            model = LinearRegression()
            model.fit(X, y)
            r2 = model.score(X, y)
            
            if r2 > best_r2:
                best_r2 = r2
                best_params = decay_params
        
        return best_params
    
    def _calculate_optimal_spend_points(self, saturation_models):
        """ÏµúÏ†Å ÏßÄÏ∂ú Ìè¨Ïù∏Ìä∏ Í≥ÑÏÇ∞"""
        optimal_points = {}
        
        for channel, model in saturation_models.items():
            if model is None:
                continue
            
            func = model['function']
            params = model['parameters']
            
            # ÌïúÍ≥Ñ Ìö®Ïú®ÏÑ±Ïù¥ ÌäπÏ†ï ÏûÑÍ≥ÑÍ∞í Ïù¥ÌïòÎ°ú Îñ®Ïñ¥ÏßÄÎäî ÏßÄÏ†ê Ï∞æÍ∏∞
            spend_range = np.linspace(0, 1000000, 10000)
            responses = func(spend_range, *params)
            
            # 1Ï∞® ÎØ∏Î∂Ñ (ÌïúÍ≥Ñ Ìö®Ïú®ÏÑ±)
            marginal_efficiency = np.gradient(responses, spend_range)
            
            # ÌïúÍ≥Ñ Ìö®Ïú®ÏÑ±Ïù¥ Ï¥àÍ∏∞Í∞íÏùò 50% Ïù¥ÌïòÎ°ú Îñ®Ïñ¥ÏßÄÎäî ÏßÄÏ†ê
            threshold = marginal_efficiency[1] * 0.5
            optimal_idx = np.where(marginal_efficiency < threshold)[0]
            
            if len(optimal_idx) > 0:
                optimal_points[channel] = {
                    'optimal_spend': spend_range[optimal_idx[0]],
                    'expected_response': responses[optimal_idx[0]],
                    'marginal_efficiency': marginal_efficiency[optimal_idx[0]]
                }
        
        return optimal_points
    
    def _visualize_adstock_effects(self, adstock_results):
        """Adstock Ìö®Í≥º ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()
        
        for i, (channel, result) in enumerate(adstock_results.items()):
            if i >= 4:  # ÏµúÎåÄ 4Í∞ú Ï±ÑÎÑêÎßå ÌëúÏãú
                break
            
            # ÏõêÎ≥∏ vs Adstock Ï†ÅÏö© Îç∞Ïù¥ÌÑ∞
            axes[i].plot(result['adstock_series'].index, 
                        result['adstock_series'].values, 
                        label='With Adstock', alpha=0.7)
            axes[i].set_title(f'{channel} - Decay Rate: {result["decay_rate"]:.2f}')
            axes[i].set_xlabel('Time Period')
            axes[i].set_ylabel('Media Effect')
            axes[i].legend()
        
        plt.tight_layout()
        return fig

# Ïã§Ïäµ ÏòàÏ†ú: Ï¢ÖÌï© Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ï
def comprehensive_advertising_measurement():
    """Ï¢ÖÌï©Ï†ÅÏù∏ Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ï Î∂ÑÏÑù"""
    
    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=365, freq='D')
    
    # ÎØ∏ÎîîÏñ¥ ÏßÄÏ∂ú Îç∞Ïù¥ÌÑ∞
    media_spend = pd.DataFrame({
        'TV': np.random.uniform(10000, 50000, 365),
        'Digital': np.random.uniform(5000, 30000, 365),
        'Radio': np.random.uniform(2000, 15000, 365),
        'Print': np.random.uniform(1000, 10000, 365)
    }, index=dates)
    
    # Îß§Ï∂ú Îç∞Ïù¥ÌÑ∞ (ÎØ∏ÎîîÏñ¥ Ìö®Í≥º + ÎÖ∏Ïù¥Ï¶à)
    base_sales = 100000
    media_effect = (
        media_spend['TV'] * 0.3 + 
        media_spend['Digital'] * 0.5 + 
        media_spend['Radio'] * 0.2 + 
        media_spend['Print'] * 0.1
    )
    
    sales_data = pd.Series(
        base_sales + media_effect + np.random.normal(0, 5000, 365),
        index=dates
    )
    
    # Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞
    pre_metrics = pd.DataFrame({
        'group': ['exposed'] * 500 + ['control'] * 500,
        'brand_awareness': np.concatenate([
            np.random.normal(0.3, 0.1, 500),  # ÎÖ∏Ï∂ú Í∑∏Î£π
            np.random.normal(0.25, 0.1, 500)  # ÎåÄÏ°∞ Í∑∏Î£π
        ]),
        'purchase_intent': np.concatenate([
            np.random.normal(0.2, 0.08, 500),
            np.random.normal(0.18, 0.08, 500)
        ])
    })
    
    post_metrics = pd.DataFrame({
        'group': ['exposed'] * 500 + ['control'] * 500,
        'brand_awareness': np.concatenate([
            np.random.normal(0.45, 0.12, 500),  # Î¶¨ÌîÑÌä∏ Ï†ÅÏö©
            np.random.normal(0.27, 0.1, 500)
        ]),
        'purchase_intent': np.concatenate([
            np.random.normal(0.32, 0.1, 500),
            np.random.normal(0.19, 0.08, 500)
        ])
    })
    
    # Ï∏°Ï†ï ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    measurement_system = AdvertisingMeasurementSystem()
    
    # 1. Adstock Î™®Îç∏ÎßÅ
    adstock_results = measurement_system.adstock_modeling(media_spend, sales_data)
    
    # 2. Ìè¨Ìôî Í≥°ÏÑ† Î™®Îç∏ÎßÅ
    saturation_results = measurement_system.saturation_curve_modeling(
        media_spend, sales_data
    )
    
    # 3. Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ Ï∏°Ï†ï
    brand_lift_results = measurement_system.brand_lift_measurement(
        pre_metrics[pre_metrics['group'] == 'exposed'],
        pre_metrics[pre_metrics['group'] == 'control'],
        pre_metrics,
        post_metrics
    )
    
    # Í≤∞Í≥º Ï∂úÎ†•
    print("=== Í¥ëÍ≥† Ìö®Í≥º Ï∏°Ï†ï Í≤∞Í≥º ===")
    print("Adstock ÌååÎùºÎØ∏ÌÑ∞:")
    for channel, params in adstock_results['geometric_adstock'].items():
        print(f"  {channel}: {params['decay_rate']:.3f} (R¬≤ = {params['r2_score']:.3f})")
    
    print("\nÎ∏åÎûúÎìú Î¶¨ÌîÑÌä∏:")
    for metric, lift in brand_lift_results['lift_metrics'].items():
        print(f"  {metric}: {lift['lift_percentage']:.1f}% ({lift['significance']})")
    
    # ÏãúÍ∞ÅÌôî
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Adstock Ìö®Í≥º ÏòàÏãú (Ï≤´ Î≤àÏß∏ Ï±ÑÎÑê)
    first_channel = list(adstock_results['geometric_adstock'].keys())[0]
    adstock_data = adstock_results['geometric_adstock'][first_channel]['adstock_series']
    
    axes[0, 0].plot(media_spend.index, media_spend[first_channel], 
                   label='Original Spend', alpha=0.7)
    axes[0, 0].plot(media_spend.index, adstock_data, 
                   label='With Adstock', alpha=0.7)
    axes[0, 0].set_title(f'{first_channel} - Adstock Effect')
    axes[0, 0].legend()
    
    # Ìè¨Ìôî Í≥°ÏÑ† ÏòàÏãú
    if first_channel in saturation_results['saturation_models']:
        model = saturation_results['saturation_models'][first_channel]
        if model:
            x_range = np.linspace(0, media_spend[first_channel].max(), 100)
            y_pred = model['function'](x_range, *model['parameters'])
            
            axes[0, 1].scatter(media_spend[first_channel], sales_data, alpha=0.5)
            axes[0, 1].plot(x_range, y_pred, 'r-', label=f'{model["name"]} fit')
            axes[0, 1].set_xlabel(f'{first_channel} Spend')
            axes[0, 1].set_ylabel('Sales')
            axes[0, 1].set_title('Saturation Curve')
            axes[0, 1].legend()
    
    # Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ ÏãúÍ∞ÅÌôî
    metrics = ['brand_awareness', 'purchase_intent']
    x_pos = np.arange(len(metrics))
    
    pre_exposed = [pre_metrics[pre_metrics['group'] == 'exposed'][m].mean() for m in metrics]
    post_exposed = [post_metrics[post_metrics['group'] == 'exposed'][m].mean() for m in metrics]
    pre_control = [pre_metrics[pre_metrics['group'] == 'control'][m].mean() for m in metrics]
    post_control = [post_metrics[post_metrics['group'] == 'control'][m].mean() for m in metrics]
    
    width = 0.2
    axes[1, 0].bar(x_pos - width*1.5, pre_exposed, width, label='Pre-Exposed', alpha=0.7)
    axes[1, 0].bar(x_pos - width*0.5, post_exposed, width, label='Post-Exposed', alpha=0.7)
    axes[1, 0].bar(x_pos + width*0.5, pre_control, width, label='Pre-Control', alpha=0.7)
    axes[1, 0].bar(x_pos + width*1.5, post_control, width, label='Post-Control', alpha=0.7)
    
    axes[1, 0].set_xlabel('Metrics')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].set_title('Brand Lift Analysis')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels(metrics)
    axes[1, 0].legend()
    
    # Îß§Ï∂ú vs ÏòàÏ∏° ÎπÑÍµê
    # Í∞ÑÎã®Ìïú Adstock Ï†ÅÏö© ÌõÑ ÏòàÏ∏°
    adstock_media = pd.DataFrame()
    for channel in media_spend.columns:
        decay_rate = adstock_results['geometric_adstock'][channel]['decay_rate']
        adstock_series = media_spend[channel].copy()
        for i in range(1, len(adstock_series)):
            adstock_series.iloc[i] += adstock_series.iloc[i-1] * decay_rate
        adstock_media[channel] = adstock_series
    
    # Í∞ÑÎã®Ìïú ÏÑ†Ìòï Î™®Îç∏Î°ú ÏòàÏ∏°
    X = adstock_media.values
    y = sales_data.values
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    
    axes[1, 1].scatter(sales_data, y_pred, alpha=0.5)
    axes[1, 1].plot([sales_data.min(), sales_data.max()], 
                   [sales_data.min(), sales_data.max()], 'r--')
    axes[1, 1].set_xlabel('Actual Sales')
    axes[1, 1].set_ylabel('Predicted Sales')
    axes[1, 1].set_title(f'Model Performance (R¬≤ = {model.score(X, y):.3f})')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'adstock_results': adstock_results,
        'saturation_results': saturation_results,
        'brand_lift_results': brand_lift_results,
        'model_performance': model.score(X, y)
    }

# Î©îÏù∏ Ïã§Ìñâ
if __name__ == "__main__":
    print("=== Í¥ëÍ≥† Ï∏°Ï†ï Ïù¥Î°† Ïã§Ïäµ ===")
    print("Adstock Î™®Îç∏ÎßÅÍ≥º Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ Ï∏°Ï†ï")
    
    results = comprehensive_advertising_measurement()
    
    print(f"\nÏ∏°Ï†ï ÏôÑÎ£å:")
    print(f"- Î™®Îç∏ ÏÑ±Îä• (R¬≤): {results['model_performance']:.3f}")
    print(f"- Î∏åÎûúÎìú Í±¥Í∞ïÎèÑ: {results['brand_lift_results']['brand_health_score']:.3f}")
```

## üöÄ ÌîÑÎ°úÏ†ùÌä∏
1. **MMM ÏûêÎèôÌôî ÌîåÎû´Ìèº** - AdstockÍ≥º Ìè¨ÌôîÍ≥°ÏÑ† ÏûêÎèô ÏµúÏ†ÅÌôî
2. **Î∏åÎûúÎìú Î¶¨ÌîÑÌä∏ Ï∏°Ï†ï ÎèÑÍµ¨** - Ïã§ÏãúÍ∞Ñ A/B ÌÖåÏä§Ìä∏ Î∂ÑÏÑù
3. **Ï¶ùÎ∂Ñ Ìö®Í≥º ÎåÄÏãúÎ≥¥Îìú** - Ïù∏Í≥º Ï∂îÎ°† Í∏∞Î∞ò ROI Ï∏°Ï†ï
4. **Í¥ëÍ≥† ÎßàÎ™® ÏòàÏ∏° ÏãúÏä§ÌÖú** - ÌÅ¨Î¶¨ÏóêÏù¥Ìã∞Î∏å ÍµêÏ≤¥ ÏãúÏ†ê ÏïåÎ¶º