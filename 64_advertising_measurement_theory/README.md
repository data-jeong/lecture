# 64. Advertising Measurement Theory - ê´‘ê³  ì¸¡ì • ì´ë¡ 

## ğŸ“š ê³¼ì • ì†Œê°œ
ê´‘ê³  íš¨ê³¼ ì¸¡ì •ì˜ ê³¼í•™ì  ì ‘ê·¼ë²•ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. Adstock ëª¨ë¸ë¶€í„° ë¸Œëœë“œ ë¦¬í”„íŠ¸ê¹Œì§€ ì •í™•í•œ ê´‘ê³  ROI ì¸¡ì •ê³¼ ìµœì í™” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- Adstockê³¼ ê´‘ê³  ì§€ì—° íš¨ê³¼ ëª¨ë¸ë§
- ë¸Œëœë“œ ì¸ì§€ë„ì™€ ê´‘ê³  ë§ˆëª¨ ì´ë¡  ì ìš©
- ì¥ë‹¨ê¸° ê´‘ê³  íš¨ê³¼ ë¶„í•´ì™€ ì¸¡ì •
- ë©€í‹° í„°ì¹˜ ì–´íŠ¸ë¦¬ë·°ì…˜ê³¼ ì¦ë¶„ ë¦¬í”„íŠ¸ ë¶„ì„

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê´‘ê³  íš¨ê³¼ ì¸¡ì • ì‹œìŠ¤í…œ
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, optimize
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings('ignore')

class AdvertisingMeasurementSystem:
    """ê´‘ê³  íš¨ê³¼ ì¸¡ì •ì„ ìœ„í•œ ì¢…í•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.adstock_parameters = {}
        self.saturation_curves = {}
        
    def adstock_modeling(self, media_data, sales_data, max_lag=12):
        """Adstock ëª¨ë¸ë§ - ê´‘ê³ ì˜ ì§€ì—° ë° ëˆ„ì  íš¨ê³¼"""
        
        def geometric_adstock(media_series, decay_rate):
            """ê¸°í•˜ê¸‰ìˆ˜ì  ê°ì†Œ Adstock"""
            adstock_series = media_series.copy()
            
            for i in range(1, len(media_series)):
                adstock_series.iloc[i] += adstock_series.iloc[i-1] * decay_rate
            
            return adstock_series
        
        def convoluted_adstock(media_series, decay_params):
            """í•©ì„±ê³± Adstock (ë” ìœ ì—°í•œ ê°ì†Œ íŒ¨í„´)"""
            adstock_series = np.zeros(len(media_series))
            
            for i in range(len(media_series)):
                for lag in range(min(i+1, len(decay_params))):
                    adstock_series[i] += media_series.iloc[i-lag] * decay_params[lag]
            
            return pd.Series(adstock_series, index=media_series.index)
        
        # ê° ë¯¸ë””ì–´ ì±„ë„ë³„ ìµœì  Adstock íŒŒë¼ë¯¸í„° ì°¾ê¸°
        optimal_adstock = {}
        
        for channel in media_data.columns:
            # ê²©ì íƒìƒ‰ìœ¼ë¡œ ìµœì  decay rate ì°¾ê¸°
            best_r2 = -np.inf
            best_params = {}
            
            for decay_rate in np.arange(0.1, 0.9, 0.1):
                # Adstock ì ìš©
                adstock_media = geometric_adstock(media_data[channel], decay_rate)
                
                # ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ë¡œ ì„±ê³¼ ì˜ˆì¸¡
                X = adstock_media.values.reshape(-1, 1)
                y = sales_data.values
                
                model = LinearRegression()
                model.fit(X, y)
                
                r2 = model.score(X, y)
                
                if r2 > best_r2:
                    best_r2 = r2
                    best_params = {
                        'decay_rate': decay_rate,
                        'r2_score': r2,
                        'adstock_series': adstock_media
                    }
            
            optimal_adstock[channel] = best_params
            self.adstock_parameters[channel] = best_params['decay_rate']
        
        # ê³ ê¸‰ í•©ì„±ê³± Adstock ëª¨ë¸ë§
        advanced_adstock = {}
        for channel in media_data.columns:
            # ë” ì •êµí•œ ì§€ì—° íš¨ê³¼ íŒ¨í„´ í•™ìŠµ
            decay_params = self._optimize_convoluted_adstock(
                media_data[channel], sales_data, max_lag
            )
            
            advanced_adstock[channel] = {
                'decay_params': decay_params,
                'adstock_series': convoluted_adstock(media_data[channel], decay_params)
            }
        
        return {
            'geometric_adstock': optimal_adstock,
            'convoluted_adstock': advanced_adstock,
            'adstock_visualization': self._visualize_adstock_effects(optimal_adstock)
        }
    
    def saturation_curve_modeling(self, media_spend, response_data):
        """í¬í™” ê³¡ì„  ëª¨ë¸ë§ - ìˆ˜í™• ì²´ê° ë²•ì¹™"""
        
        def hill_saturation(x, alpha, gamma):
            """Hill ë³€í™˜ (S-curve)"""
            return alpha * (x ** gamma) / (x ** gamma + 1)
        
        def michaelis_menten(x, vmax, km):
            """Michaelis-Menten í¬í™” ê³¡ì„ """
            return (vmax * x) / (km + x)
        
        def exponential_saturation(x, a, b):
            """ì§€ìˆ˜ì  í¬í™” ê³¡ì„ """
            return a * (1 - np.exp(-b * x))
        
        saturation_models = {}
        
        for channel in media_spend.columns:
            x_data = media_spend[channel].values
            y_data = response_data.values
            
            # ê° í¬í™” ëª¨ë¸ í”¼íŒ…
            models = {
                'hill': (hill_saturation, [1.0, 1.0]),
                'michaelis_menten': (michaelis_menten, [max(y_data), np.median(x_data)]),
                'exponential': (exponential_saturation, [max(y_data), 0.001])
            }
            
            best_model = None
            best_r2 = -np.inf
            
            for model_name, (func, initial_params) in models.items():
                try:
                    # ê³¡ì„  í”¼íŒ…
                    popt, _ = optimize.curve_fit(func, x_data, y_data, p0=initial_params)
                    
                    # ì˜ˆì¸¡ ë° í‰ê°€
                    y_pred = func(x_data, *popt)
                    r2 = r2_score(y_data, y_pred)
                    
                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = {
                            'name': model_name,
                            'function': func,
                            'parameters': popt,
                            'r2_score': r2,
                            'predictions': y_pred
                        }
                
                except Exception as e:
                    continue
            
            saturation_models[channel] = best_model
            self.saturation_curves[channel] = best_model
        
        # ìµœì  ì˜ˆì‚° í¬ì¸íŠ¸ ê³„ì‚°
        optimal_spend_points = self._calculate_optimal_spend_points(saturation_models)
        
        return {
            'saturation_models': saturation_models,
            'optimal_spend_points': optimal_spend_points,
            'saturation_visualization': self._visualize_saturation_curves(saturation_models, media_spend)
        }
    
    def brand_lift_measurement(self, exposed_group, control_group, pre_metrics, post_metrics):
        """ë¸Œëœë“œ ë¦¬í”„íŠ¸ ì¸¡ì •"""
        
        # ì‚¬ì „-ì‚¬í›„ ë¹„êµ ë¶„ì„
        def calculate_lift(exposed_pre, exposed_post, control_pre, control_post):
            """ë¦¬í”„íŠ¸ ê³„ì‚° ê³µì‹"""
            exposed_change = (exposed_post - exposed_pre) / exposed_pre
            control_change = (control_post - control_pre) / control_pre
            
            return exposed_change - control_change
        
        lift_metrics = {}
        
        # ê° ë¸Œëœë“œ ë©”íŠ¸ë¦­ë³„ ë¦¬í”„íŠ¸ ê³„ì‚°
        for metric in pre_metrics.columns:
            # ë…¸ì¶œ ê·¸ë£¹ ë¦¬í”„íŠ¸
            exposed_lift = calculate_lift(
                pre_metrics[pre_metrics['group'] == 'exposed'][metric].mean(),
                post_metrics[post_metrics['group'] == 'exposed'][metric].mean(),
                pre_metrics[pre_metrics['group'] == 'control'][metric].mean(),
                post_metrics[post_metrics['group'] == 'control'][metric].mean()
            )
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
            exposed_post_values = post_metrics[post_metrics['group'] == 'exposed'][metric]
            control_post_values = post_metrics[post_metrics['group'] == 'control'][metric]
            
            t_stat, p_value = stats.ttest_ind(exposed_post_values, control_post_values)
            
            # íš¨ê³¼ í¬ê¸° (Cohen's d)
            pooled_std = np.sqrt(((len(exposed_post_values)-1) * exposed_post_values.std()**2 + 
                                 (len(control_post_values)-1) * control_post_values.std()**2) / 
                                (len(exposed_post_values) + len(control_post_values) - 2))
            
            cohens_d = (exposed_post_values.mean() - control_post_values.mean()) / pooled_std
            
            lift_metrics[metric] = {
                'lift_percentage': exposed_lift * 100,
                't_statistic': t_stat,
                'p_value': p_value,
                'cohens_d': cohens_d,
                'significance': 'Significant' if p_value < 0.05 else 'Not Significant',
                'effect_size': self._interpret_effect_size(abs(cohens_d))
            }
        
        # ì¢…í•© ë¸Œëœë“œ ê±´ê°•ë„ ì ìˆ˜
        brand_health_score = self._calculate_brand_health_score(lift_metrics)
        
        # ì‹œê°í™”
        self._visualize_brand_lift(lift_metrics, pre_metrics, post_metrics)
        
        return {
            'lift_metrics': lift_metrics,
            'brand_health_score': brand_health_score,
            'statistical_power': self._calculate_statistical_power(exposed_group, control_group)
        }
    
    def incrementality_testing(self, test_data, control_data, experiment_period):
        """ì¦ë¶„ íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        
        # Difference-in-Differences ë¶„ì„
        def did_analysis(test_before, test_after, control_before, control_after):
            """ì´ì¤‘ì°¨ë¶„ë²• ë¶„ì„"""
            test_change = test_after - test_before
            control_change = control_after - control_before
            
            incremental_effect = test_change - control_change
            
            # í‘œì¤€ ì˜¤ì°¨ ê³„ì‚°
            test_se = np.sqrt(test_before.var() + test_after.var())
            control_se = np.sqrt(control_before.var() + control_after.var())
            combined_se = np.sqrt(test_se**2 + control_se**2)
            
            # t-í†µê³„ëŸ‰ê³¼ p-ê°’
            t_stat = incremental_effect / combined_se
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=len(test_before)+len(control_before)-2))
            
            return {
                'incremental_effect': incremental_effect,
                'standard_error': combined_se,
                't_statistic': t_stat,
                'p_value': p_value
            }
        
        # ì‹¤í—˜ ê¸°ê°„ ì „í›„ ë°ì´í„° ë¶„ë¦¬
        pre_period = experiment_period['start'] - pd.Timedelta(days=30)
        
        test_before = test_data[test_data['date'] < experiment_period['start']]['metric']
        test_after = test_data[test_data['date'] >= experiment_period['start']]['metric']
        control_before = control_data[control_data['date'] < experiment_period['start']]['metric']
        control_after = control_data[control_data['date'] >= experiment_period['start']]['metric']
        
        # DID ë¶„ì„ ì‹¤í–‰
        did_results = did_analysis(test_before, test_after, control_before, control_after)
        
        # í•©ì„± ëŒ€ì¡°êµ° ë°©ë²• (Synthetic Control)
        synthetic_control = self._create_synthetic_control(test_data, control_data, experiment_period)
        
        # ì¸ê³¼ ì¶”ë¡  ëª¨ë¸
        causal_impact = self._causal_impact_analysis(test_data, control_data, experiment_period)
        
        return {
            'did_results': did_results,
            'synthetic_control': synthetic_control,
            'causal_impact': causal_impact,
            'incremental_roi': self._calculate_incremental_roi(did_results, experiment_period)
        }
    
    def attribution_decay_modeling(self, touchpoint_data, conversion_data):
        """ì–´íŠ¸ë¦¬ë·°ì…˜ ê°ì‡  ëª¨ë¸ë§"""
        
        # ì‹œê°„ ê¸°ë°˜ ê°ì‡  í•¨ìˆ˜ë“¤
        def linear_decay(days_since_touch, max_days=30):
            """ì„ í˜• ê°ì‡ """
            return max(0, 1 - days_since_touch / max_days)
        
        def exponential_decay(days_since_touch, decay_rate=0.1):
            """ì§€ìˆ˜ ê°ì‡ """
            return np.exp(-decay_rate * days_since_touch)
        
        def u_shaped_decay(days_since_touch, max_days=30):
            """Uìí˜• ê°ì‡  (ìµœê·¼ì„± + ì²«ì¸ìƒ íš¨ê³¼)"""
            normalized_days = days_since_touch / max_days
            return 0.5 * (1 - normalized_days) + 0.5 * np.exp(-5 * normalized_days)
        
        # í„°ì¹˜í¬ì¸íŠ¸ë³„ ê¸°ì—¬ë„ ê³„ì‚°
        attribution_weights = {}
        
        for conversion_id in conversion_data['conversion_id'].unique():
            # í•´ë‹¹ ì „í™˜ì˜ í„°ì¹˜í¬ì¸íŠ¸ë“¤
            touchpoints = touchpoint_data[
                touchpoint_data['conversion_id'] == conversion_id
            ].sort_values('timestamp')
            
            if len(touchpoints) == 0:
                continue
            
            # ì „í™˜ ì‹œì ë¶€í„°ì˜ ê²½ê³¼ ì¼ìˆ˜ ê³„ì‚°
            conversion_time = conversion_data[
                conversion_data['conversion_id'] == conversion_id
            ]['conversion_time'].iloc[0]
            
            touchpoints['days_since'] = (
                conversion_time - touchpoints['timestamp']
            ).dt.days
            
            # ê° ê°ì‡  ëª¨ë¸ ì ìš©
            decay_models = {
                'linear': linear_decay,
                'exponential': exponential_decay,
                'u_shaped': u_shaped_decay
            }
            
            for model_name, decay_func in decay_models.items():
                weights = touchpoints['days_since'].apply(decay_func)
                normalized_weights = weights / weights.sum()
                
                if conversion_id not in attribution_weights:
                    attribution_weights[conversion_id] = {}
                
                attribution_weights[conversion_id][model_name] = dict(
                    zip(touchpoints['touchpoint_id'], normalized_weights)
                )
        
        # ì±„ë„ë³„ ì´ ê¸°ì—¬ë„ ì§‘ê³„
        channel_attribution = self._aggregate_channel_attribution(
            attribution_weights, touchpoint_data
        )
        
        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        model_performance = self._compare_attribution_models(
            channel_attribution, conversion_data
        )
        
        return {
            'attribution_weights': attribution_weights,
            'channel_attribution': channel_attribution,
            'model_performance': model_performance,
            'recommended_model': max(model_performance.items(), key=lambda x: x[1]['auc'])[0]
        }
    
    def advertising_wearout_analysis(self, creative_data, performance_data):
        """ê´‘ê³  ë§ˆëª¨ ë¶„ì„"""
        
        # ê´‘ê³ ë³„ ì„±ê³¼ ì¶”ì´ ë¶„ì„
        wearout_patterns = {}
        
        for creative_id in creative_data['creative_id'].unique():
            # í•´ë‹¹ ê´‘ê³ ì˜ ì‹œê³„ì—´ ì„±ê³¼ ë°ì´í„°
            creative_performance = performance_data[
                performance_data['creative_id'] == creative_id
            ].sort_values('date')
            
            if len(creative_performance) < 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
                continue
            
            # ì„±ê³¼ ì§€í‘œë³„ ë§ˆëª¨ íŒ¨í„´ ë¶„ì„
            metrics = ['ctr', 'conversion_rate', 'cpm']
            
            for metric in metrics:
                if metric not in creative_performance.columns:
                    continue
                
                # íŠ¸ë Œë“œ ë¶„ì„
                x = np.arange(len(creative_performance))
                y = creative_performance[metric].values
                
                # ì„ í˜• íšŒê·€ë¡œ ì „ë°˜ì  íŠ¸ë Œë“œ íŒŒì•…
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                
                # ë³€ê³¡ì  íƒì§€ (ì„±ê³¼ ê¸‰ë½ ì‹œì )
                change_points = self._detect_change_points(y)
                
                # ë§ˆëª¨ìœ¨ ê³„ì‚°
                wearout_rate = -slope if slope < 0 else 0
                
                wearout_patterns[creative_id] = {
                    'metric': metric,
                    'slope': slope,
                    'r_squared': r_value**2,
                    'p_value': p_value,
                    'wearout_rate': wearout_rate,
                    'change_points': change_points,
                    'days_to_wearout': self._estimate_days_to_wearout(slope, y),
                    'performance_data': creative_performance
                }
        
        # ê´‘ê³  êµì²´ ê¶Œê³ ì‚¬í•­
        replacement_recommendations = self._generate_replacement_recommendations(wearout_patterns)
        
        # ë§ˆëª¨ íŒ¨í„´ ì‹œê°í™”
        self._visualize_wearout_patterns(wearout_patterns)
        
        return {
            'wearout_patterns': wearout_patterns,
            'replacement_recommendations': replacement_recommendations,
            'average_wearout_days': np.mean([
                p['days_to_wearout'] for p in wearout_patterns.values() 
                if p['days_to_wearout'] is not None
            ])
        }

# ë³´ì¡° ë©”ì„œë“œë“¤
    def _optimize_convoluted_adstock(self, media_series, sales_series, max_lag):
        """í•©ì„±ê³± Adstock ìµœì í™”"""
        best_params = None
        best_r2 = -np.inf
        
        # ë² ì´ì§€ì•ˆ ìµœì í™” ë˜ëŠ” ê²©ì íƒìƒ‰ìœ¼ë¡œ ìµœì  ê°ì‡  íŒ¨í„´ ì°¾ê¸°
        for _ in range(50):  # ëœë¤ ì„œì¹˜
            decay_params = np.random.exponential(0.5, max_lag)
            decay_params = decay_params / decay_params.sum()  # ì •ê·œí™”
            
            # Adstock ì ìš©
            adstock_series = np.zeros(len(media_series))
            for i in range(len(media_series)):
                for lag in range(min(i+1, len(decay_params))):
                    adstock_series[i] += media_series.iloc[i-lag] * decay_params[lag]
            
            # ì„±ê³¼ ì˜ˆì¸¡
            X = adstock_series.reshape(-1, 1)
            y = sales_series.values
            
            model = LinearRegression()
            model.fit(X, y)
            r2 = model.score(X, y)
            
            if r2 > best_r2:
                best_r2 = r2
                best_params = decay_params
        
        return best_params
    
    def _calculate_optimal_spend_points(self, saturation_models):
        """ìµœì  ì§€ì¶œ í¬ì¸íŠ¸ ê³„ì‚°"""
        optimal_points = {}
        
        for channel, model in saturation_models.items():
            if model is None:
                continue
            
            func = model['function']
            params = model['parameters']
            
            # í•œê³„ íš¨ìœ¨ì„±ì´ íŠ¹ì • ì„ê³„ê°’ ì´í•˜ë¡œ ë–¨ì–´ì§€ëŠ” ì§€ì  ì°¾ê¸°
            spend_range = np.linspace(0, 1000000, 10000)
            responses = func(spend_range, *params)
            
            # 1ì°¨ ë¯¸ë¶„ (í•œê³„ íš¨ìœ¨ì„±)
            marginal_efficiency = np.gradient(responses, spend_range)
            
            # í•œê³„ íš¨ìœ¨ì„±ì´ ì´ˆê¸°ê°’ì˜ 50% ì´í•˜ë¡œ ë–¨ì–´ì§€ëŠ” ì§€ì 
            threshold = marginal_efficiency[1] * 0.5
            optimal_idx = np.where(marginal_efficiency < threshold)[0]
            
            if len(optimal_idx) > 0:
                optimal_points[channel] = {
                    'optimal_spend': spend_range[optimal_idx[0]],
                    'expected_response': responses[optimal_idx[0]],
                    'marginal_efficiency': marginal_efficiency[optimal_idx[0]]
                }
        
        return optimal_points
    
    def _visualize_adstock_effects(self, adstock_results):
        """Adstock íš¨ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()
        
        for i, (channel, result) in enumerate(adstock_results.items()):
            if i >= 4:  # ìµœëŒ€ 4ê°œ ì±„ë„ë§Œ í‘œì‹œ
                break
            
            # ì›ë³¸ vs Adstock ì ìš© ë°ì´í„°
            axes[i].plot(result['adstock_series'].index, 
                        result['adstock_series'].values, 
                        label='With Adstock', alpha=0.7)
            axes[i].set_title(f'{channel} - Decay Rate: {result["decay_rate"]:.2f}')
            axes[i].set_xlabel('Time Period')
            axes[i].set_ylabel('Media Effect')
            axes[i].legend()
        
        plt.tight_layout()
        return fig

# ì‹¤ìŠµ ì˜ˆì œ: ì¢…í•© ê´‘ê³  íš¨ê³¼ ì¸¡ì •
def comprehensive_advertising_measurement():
    """ì¢…í•©ì ì¸ ê´‘ê³  íš¨ê³¼ ì¸¡ì • ë¶„ì„"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=365, freq='D')
    
    # ë¯¸ë””ì–´ ì§€ì¶œ ë°ì´í„°
    media_spend = pd.DataFrame({
        'TV': np.random.uniform(10000, 50000, 365),
        'Digital': np.random.uniform(5000, 30000, 365),
        'Radio': np.random.uniform(2000, 15000, 365),
        'Print': np.random.uniform(1000, 10000, 365)
    }, index=dates)
    
    # ë§¤ì¶œ ë°ì´í„° (ë¯¸ë””ì–´ íš¨ê³¼ + ë…¸ì´ì¦ˆ)
    base_sales = 100000
    media_effect = (
        media_spend['TV'] * 0.3 + 
        media_spend['Digital'] * 0.5 + 
        media_spend['Radio'] * 0.2 + 
        media_spend['Print'] * 0.1
    )
    
    sales_data = pd.Series(
        base_sales + media_effect + np.random.normal(0, 5000, 365),
        index=dates
    )
    
    # ë¸Œëœë“œ ë¦¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    pre_metrics = pd.DataFrame({
        'group': ['exposed'] * 500 + ['control'] * 500,
        'brand_awareness': np.concatenate([
            np.random.normal(0.3, 0.1, 500),  # ë…¸ì¶œ ê·¸ë£¹
            np.random.normal(0.25, 0.1, 500)  # ëŒ€ì¡° ê·¸ë£¹
        ]),
        'purchase_intent': np.concatenate([
            np.random.normal(0.2, 0.08, 500),
            np.random.normal(0.18, 0.08, 500)
        ])
    })
    
    post_metrics = pd.DataFrame({
        'group': ['exposed'] * 500 + ['control'] * 500,
        'brand_awareness': np.concatenate([
            np.random.normal(0.45, 0.12, 500),  # ë¦¬í”„íŠ¸ ì ìš©
            np.random.normal(0.27, 0.1, 500)
        ]),
        'purchase_intent': np.concatenate([
            np.random.normal(0.32, 0.1, 500),
            np.random.normal(0.19, 0.08, 500)
        ])
    })
    
    # ì¸¡ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    measurement_system = AdvertisingMeasurementSystem()
    
    # 1. Adstock ëª¨ë¸ë§
    adstock_results = measurement_system.adstock_modeling(media_spend, sales_data)
    
    # 2. í¬í™” ê³¡ì„  ëª¨ë¸ë§
    saturation_results = measurement_system.saturation_curve_modeling(
        media_spend, sales_data
    )
    
    # 3. ë¸Œëœë“œ ë¦¬í”„íŠ¸ ì¸¡ì •
    brand_lift_results = measurement_system.brand_lift_measurement(
        pre_metrics[pre_metrics['group'] == 'exposed'],
        pre_metrics[pre_metrics['group'] == 'control'],
        pre_metrics,
        post_metrics
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("=== ê´‘ê³  íš¨ê³¼ ì¸¡ì • ê²°ê³¼ ===")
    print("Adstock íŒŒë¼ë¯¸í„°:")
    for channel, params in adstock_results['geometric_adstock'].items():
        print(f"  {channel}: {params['decay_rate']:.3f} (RÂ² = {params['r2_score']:.3f})")
    
    print("\në¸Œëœë“œ ë¦¬í”„íŠ¸:")
    for metric, lift in brand_lift_results['lift_metrics'].items():
        print(f"  {metric}: {lift['lift_percentage']:.1f}% ({lift['significance']})")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Adstock íš¨ê³¼ ì˜ˆì‹œ (ì²« ë²ˆì§¸ ì±„ë„)
    first_channel = list(adstock_results['geometric_adstock'].keys())[0]
    adstock_data = adstock_results['geometric_adstock'][first_channel]['adstock_series']
    
    axes[0, 0].plot(media_spend.index, media_spend[first_channel], 
                   label='Original Spend', alpha=0.7)
    axes[0, 0].plot(media_spend.index, adstock_data, 
                   label='With Adstock', alpha=0.7)
    axes[0, 0].set_title(f'{first_channel} - Adstock Effect')
    axes[0, 0].legend()
    
    # í¬í™” ê³¡ì„  ì˜ˆì‹œ
    if first_channel in saturation_results['saturation_models']:
        model = saturation_results['saturation_models'][first_channel]
        if model:
            x_range = np.linspace(0, media_spend[first_channel].max(), 100)
            y_pred = model['function'](x_range, *model['parameters'])
            
            axes[0, 1].scatter(media_spend[first_channel], sales_data, alpha=0.5)
            axes[0, 1].plot(x_range, y_pred, 'r-', label=f'{model["name"]} fit')
            axes[0, 1].set_xlabel(f'{first_channel} Spend')
            axes[0, 1].set_ylabel('Sales')
            axes[0, 1].set_title('Saturation Curve')
            axes[0, 1].legend()
    
    # ë¸Œëœë“œ ë¦¬í”„íŠ¸ ì‹œê°í™”
    metrics = ['brand_awareness', 'purchase_intent']
    x_pos = np.arange(len(metrics))
    
    pre_exposed = [pre_metrics[pre_metrics['group'] == 'exposed'][m].mean() for m in metrics]
    post_exposed = [post_metrics[post_metrics['group'] == 'exposed'][m].mean() for m in metrics]
    pre_control = [pre_metrics[pre_metrics['group'] == 'control'][m].mean() for m in metrics]
    post_control = [post_metrics[post_metrics['group'] == 'control'][m].mean() for m in metrics]
    
    width = 0.2
    axes[1, 0].bar(x_pos - width*1.5, pre_exposed, width, label='Pre-Exposed', alpha=0.7)
    axes[1, 0].bar(x_pos - width*0.5, post_exposed, width, label='Post-Exposed', alpha=0.7)
    axes[1, 0].bar(x_pos + width*0.5, pre_control, width, label='Pre-Control', alpha=0.7)
    axes[1, 0].bar(x_pos + width*1.5, post_control, width, label='Post-Control', alpha=0.7)
    
    axes[1, 0].set_xlabel('Metrics')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].set_title('Brand Lift Analysis')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels(metrics)
    axes[1, 0].legend()
    
    # ë§¤ì¶œ vs ì˜ˆì¸¡ ë¹„êµ
    # ê°„ë‹¨í•œ Adstock ì ìš© í›„ ì˜ˆì¸¡
    adstock_media = pd.DataFrame()
    for channel in media_spend.columns:
        decay_rate = adstock_results['geometric_adstock'][channel]['decay_rate']
        adstock_series = media_spend[channel].copy()
        for i in range(1, len(adstock_series)):
            adstock_series.iloc[i] += adstock_series.iloc[i-1] * decay_rate
        adstock_media[channel] = adstock_series
    
    # ê°„ë‹¨í•œ ì„ í˜• ëª¨ë¸ë¡œ ì˜ˆì¸¡
    X = adstock_media.values
    y = sales_data.values
    model = LinearRegression()
    model.fit(X, y)
    y_pred = model.predict(X)
    
    axes[1, 1].scatter(sales_data, y_pred, alpha=0.5)
    axes[1, 1].plot([sales_data.min(), sales_data.max()], 
                   [sales_data.min(), sales_data.max()], 'r--')
    axes[1, 1].set_xlabel('Actual Sales')
    axes[1, 1].set_ylabel('Predicted Sales')
    axes[1, 1].set_title(f'Model Performance (RÂ² = {model.score(X, y):.3f})')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'adstock_results': adstock_results,
        'saturation_results': saturation_results,
        'brand_lift_results': brand_lift_results,
        'model_performance': model.score(X, y)
    }

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("=== ê´‘ê³  ì¸¡ì • ì´ë¡  ì‹¤ìŠµ ===")
    print("Adstock ëª¨ë¸ë§ê³¼ ë¸Œëœë“œ ë¦¬í”„íŠ¸ ì¸¡ì •")
    
    results = comprehensive_advertising_measurement()
    
    print(f"\nì¸¡ì • ì™„ë£Œ:")
    print(f"- ëª¨ë¸ ì„±ëŠ¥ (RÂ²): {results['model_performance']:.3f}")
    print(f"- ë¸Œëœë“œ ê±´ê°•ë„: {results['brand_lift_results']['brand_health_score']:.3f}")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **MMM ìë™í™” í”Œë«í¼** - Adstockê³¼ í¬í™”ê³¡ì„  ìë™ ìµœì í™”
2. **ë¸Œëœë“œ ë¦¬í”„íŠ¸ ì¸¡ì • ë„êµ¬** - ì‹¤ì‹œê°„ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„
3. **ì¦ë¶„ íš¨ê³¼ ëŒ€ì‹œë³´ë“œ** - ì¸ê³¼ ì¶”ë¡  ê¸°ë°˜ ROI ì¸¡ì •
4. **ê´‘ê³  ë§ˆëª¨ ì˜ˆì¸¡ ì‹œìŠ¤í…œ** - í¬ë¦¬ì—ì´í‹°ë¸Œ êµì²´ ì‹œì  ì•Œë¦¼