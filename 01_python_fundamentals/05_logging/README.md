# Chapter 05: Logging - ë¡œê¹…

## ğŸ“š í•™ìŠµ ëª©í‘œ
- íš¨ê³¼ì ì¸ ë¡œê¹… ì „ëµ ìˆ˜ë¦½
- êµ¬ì¡°í™”ëœ ë¡œê¹… êµ¬í˜„
- ê´‘ê³  ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•
- ë¡œê·¸ ë¶„ì„ì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

## ğŸ“– ì´ë¡ : ë¡œê¹…ì˜ í•µì‹¬ ê°œë…

### 1. ë¡œê¹… ë ˆë²¨ê³¼ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

```python
import logging
import json
from datetime import datetime
from typing import Dict, Any
import traceback

# ê´‘ê³  ì‹œìŠ¤í…œì„ ìœ„í•œ ë¡œê¹… ì„¤ì •
class AdSystemLogger:
    """ê´‘ê³  ì‹œìŠ¤í…œ ì „ìš© ë¡œê±°"""
    
    @staticmethod
    def setup_logging(app_name: str = "ad_system") -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        
        # ë¡œê±° ìƒì„±
        logger = logging.getLogger(app_name)
        logger.setLevel(logging.DEBUG)
        
        # í¬ë§·í„° ìƒì„±
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - '
            '%(filename)s:%(lineno)d - %(message)s'
        )
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¼ë³„ ë¡œí…Œì´ì…˜)
        from logging.handlers import TimedRotatingFileHandler
        file_handler = TimedRotatingFileHandler(
            f'logs/{app_name}.log',
            when='midnight',
            interval=1,
            backupCount=30
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        
        # ì—ëŸ¬ ì „ìš© í•¸ë“¤ëŸ¬
        error_handler = logging.FileHandler(f'logs/{app_name}_errors.log')
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(formatter)
        
        # í•¸ë“¤ëŸ¬ ì¶”ê°€
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        logger.addHandler(error_handler)
        
        return logger

# ë¡œê¹… ë ˆë²¨ë³„ ì‚¬ìš© ì˜ˆì‹œ
logger = AdSystemLogger.setup_logging()

# DEBUG: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
logger.debug(f"ìº í˜ì¸ ë°ì´í„° ì¡°íšŒ: campaign_id={campaign_id}, filters={filters}")

# INFO: ì¼ë°˜ì ì¸ ì •ë³´ì„± ë©”ì‹œì§€
logger.info(f"ê´‘ê³  ìº í˜ì¸ ìƒì„± ì™„ë£Œ: {campaign_name} (ID: {campaign_id})")

# WARNING: ì£¼ì˜ê°€ í•„ìš”í•œ ìƒí™©
logger.warning(f"ì¼ì¼ ì˜ˆì‚°ì˜ 80% ì†Œì§„: campaign_id={campaign_id}, spent={spent}")

# ERROR: ì˜¤ë¥˜ ë°œìƒ (ë³µêµ¬ ê°€ëŠ¥)
logger.error(f"ê´‘ê³  API í˜¸ì¶œ ì‹¤íŒ¨: {api_error}", exc_info=True)

# CRITICAL: ì‹¬ê°í•œ ì˜¤ë¥˜ (ì‹œìŠ¤í…œ ì¤‘ë‹¨ ê°€ëŠ¥)
logger.critical(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {db_error}")
```

### 2. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
import json
import logging
from typing import Dict, Any, Optional
from functools import wraps
import time
import uuid

class StructuredLogger:
    """êµ¬ì¡°í™”ëœ ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ë¡œê±°"""
    
    def __init__(self, logger_name: str):
        self.logger = logging.getLogger(logger_name)
        self.context = {}
    
    def set_context(self, **kwargs):
        """ë¡œê¹… ì»¨í…ìŠ¤íŠ¸ ì„¤ì •"""
        self.context.update(kwargs)
    
    def clear_context(self):
        """ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        self.context = {}
    
    def _create_log_entry(self, 
                         level: str,
                         message: str,
                         **kwargs) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level,
            'message': message,
            'context': self.context.copy()
        }
        
        # ì¶”ê°€ í•„ë“œ
        log_entry.update(kwargs)
        
        return log_entry
    
    def debug(self, message: str, **kwargs):
        log_entry = self._create_log_entry('DEBUG', message, **kwargs)
        self.logger.debug(json.dumps(log_entry))
    
    def info(self, message: str, **kwargs):
        log_entry = self._create_log_entry('INFO', message, **kwargs)
        self.logger.info(json.dumps(log_entry))
    
    def warning(self, message: str, **kwargs):
        log_entry = self._create_log_entry('WARNING', message, **kwargs)
        self.logger.warning(json.dumps(log_entry))
    
    def error(self, message: str, exception: Optional[Exception] = None, **kwargs):
        log_entry = self._create_log_entry('ERROR', message, **kwargs)
        
        if exception:
            log_entry['exception'] = {
                'type': type(exception).__name__,
                'message': str(exception),
                'traceback': traceback.format_exc()
            }
        
        self.logger.error(json.dumps(log_entry))
    
    def critical(self, message: str, **kwargs):
        log_entry = self._create_log_entry('CRITICAL', message, **kwargs)
        self.logger.critical(json.dumps(log_entry))

# ê´‘ê³  ì´ë²¤íŠ¸ ë¡œê¹…
class AdEventLogger(StructuredLogger):
    """ê´‘ê³  ì´ë²¤íŠ¸ ì „ìš© ë¡œê±°"""
    
    def log_impression(self, ad_id: str, user_id: str, **kwargs):
        """ë…¸ì¶œ ë¡œê¹…"""
        self.info(
            "Ad impression recorded",
            event_type="impression",
            ad_id=ad_id,
            user_id=user_id,
            **kwargs
        )
    
    def log_click(self, ad_id: str, user_id: str, **kwargs):
        """í´ë¦­ ë¡œê¹…"""
        self.info(
            "Ad click recorded",
            event_type="click",
            ad_id=ad_id,
            user_id=user_id,
            **kwargs
        )
    
    def log_conversion(self, ad_id: str, user_id: str, value: float, **kwargs):
        """ì „í™˜ ë¡œê¹…"""
        self.info(
            "Conversion recorded",
            event_type="conversion",
            ad_id=ad_id,
            user_id=user_id,
            conversion_value=value,
            **kwargs
        )
    
    def log_bid_request(self, request_id: str, **kwargs):
        """ì…ì°° ìš”ì²­ ë¡œê¹…"""
        self.debug(
            "Bid request received",
            event_type="bid_request",
            request_id=request_id,
            **kwargs
        )
    
    def log_bid_response(self, request_id: str, bid_price: float, 
                        win: bool, **kwargs):
        """ì…ì°° ì‘ë‹µ ë¡œê¹…"""
        self.info(
            "Bid response sent",
            event_type="bid_response",
            request_id=request_id,
            bid_price=bid_price,
            win=win,
            **kwargs
        )
```

### 3. ì„±ëŠ¥ ë¡œê¹…ê³¼ ë©”íŠ¸ë¦­

```python
import time
from contextlib import contextmanager
from functools import wraps
import statistics

class PerformanceLogger:
    """ì„±ëŠ¥ ì¸¡ì • ë° ë¡œê¹…"""
    
    def __init__(self, logger: StructuredLogger):
        self.logger = logger
        self.metrics = {}
    
    @contextmanager
    def measure_time(self, operation: str, **tags):
        """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        start_time = time.time()
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            self.logger.info(
                f"Operation completed: {operation}",
                operation=operation,
                duration_ms=duration * 1000,
                **tags
            )
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            if operation not in self.metrics:
                self.metrics[operation] = []
            self.metrics[operation].append(duration)
    
    def log_api_call(self, func):
        """API í˜¸ì¶œ ë¡œê¹… ë°ì½”ë ˆì´í„°"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            request_id = str(uuid.uuid4())
            start_time = time.time()
            
            self.logger.info(
                f"API call started: {func.__name__}",
                api_method=func.__name__,
                request_id=request_id,
                args=str(args)[:100],  # ì¸ì ì¼ë¶€ë§Œ ë¡œê¹…
                kwargs=str(kwargs)[:100]
            )
            
            try:
                result = func(*args, **kwargs)
                
                duration = time.time() - start_time
                self.logger.info(
                    f"API call succeeded: {func.__name__}",
                    api_method=func.__name__,
                    request_id=request_id,
                    duration_ms=duration * 1000,
                    status="success"
                )
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                self.logger.error(
                    f"API call failed: {func.__name__}",
                    exception=e,
                    api_method=func.__name__,
                    request_id=request_id,
                    duration_ms=duration * 1000,
                    status="error"
                )
                raise
        
        return wrapper
    
    def get_performance_summary(self) -> Dict[str, Dict[str, float]]:
        """ì„±ëŠ¥ ìš”ì•½ í†µê³„"""
        summary = {}
        
        for operation, durations in self.metrics.items():
            if durations:
                summary[operation] = {
                    'count': len(durations),
                    'mean_ms': statistics.mean(durations) * 1000,
                    'median_ms': statistics.median(durations) * 1000,
                    'min_ms': min(durations) * 1000,
                    'max_ms': max(durations) * 1000,
                    'stdev_ms': statistics.stdev(durations) * 1000 if len(durations) > 1 else 0
                }
        
        return summary

# ì‚¬ìš© ì˜ˆì‹œ
logger = StructuredLogger("ad_api")
perf_logger = PerformanceLogger(logger)

class AdAPI:
    """ê´‘ê³  API í´ë˜ìŠ¤"""
    
    @perf_logger.log_api_call
    def create_campaign(self, name: str, budget: float) -> Dict:
        """ìº í˜ì¸ ìƒì„± API"""
        with perf_logger.measure_time("db_insert", table="campaigns"):
            # DB ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.1)
        
        return {"campaign_id": "camp_123", "status": "created"}
    
    @perf_logger.log_api_call
    def get_campaign_metrics(self, campaign_id: str) -> Dict:
        """ìº í˜ì¸ ë©”íŠ¸ë¦­ ì¡°íšŒ API"""
        with perf_logger.measure_time("metrics_calculation", campaign_id=campaign_id):
            # ë³µì¡í•œ ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.2)
        
        return {
            "impressions": 10000,
            "clicks": 500,
            "conversions": 50
        }
```

## ğŸ› ï¸ ì‹¤ìŠµ: ê´‘ê³  ì‹œìŠ¤í…œ ë¡œê¹… êµ¬í˜„

### ì‹¤ìŠµ 1: ê´‘ê³  ìš”ì²­ ì¶”ì  ì‹œìŠ¤í…œ

```python
import logging
import json
from typing import Dict, Optional
import hashlib
from datetime import datetime, timedelta

class AdRequestTracker:
    """ê´‘ê³  ìš”ì²­ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.request_cache = {}
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("ad_request_tracker")
        logger.setLevel(logging.DEBUG)
        
        # JSON í¬ë§·í„°
        class JsonFormatter(logging.Formatter):
            def format(self, record):
                log_obj = {
                    'timestamp': datetime.utcnow().isoformat(),
                    'level': record.levelname,
                    'logger': record.name,
                    'message': record.getMessage(),
                    'module': record.module,
                    'function': record.funcName,
                    'line': record.lineno
                }
                
                # ì¶”ê°€ ì†ì„±
                if hasattr(record, 'request_id'):
                    log_obj['request_id'] = record.request_id
                if hasattr(record, 'user_id'):
                    log_obj['user_id'] = record.user_id
                if hasattr(record, 'ad_id'):
                    log_obj['ad_id'] = record.ad_id
                
                return json.dumps(log_obj)
        
        # í•¸ë“¤ëŸ¬ ì„¤ì •
        handler = logging.StreamHandler()
        handler.setFormatter(JsonFormatter())
        logger.addHandler(handler)
        
        return logger
    
    def generate_request_id(self, user_id: str, timestamp: float) -> str:
        """ìš”ì²­ ID ìƒì„±"""
        data = f"{user_id}:{timestamp}".encode()
        return hashlib.sha256(data).hexdigest()[:16]
    
    def track_ad_request(self, user_id: str, 
                        user_context: Dict,
                        available_ads: List[Dict]) -> str:
        """ê´‘ê³  ìš”ì²­ ì¶”ì """
        timestamp = datetime.now().timestamp()
        request_id = self.generate_request_id(user_id, timestamp)
        
        # ìš”ì²­ ì •ë³´ ë¡œê¹…
        self.logger.info(
            "Ad request received",
            extra={
                'request_id': request_id,
                'user_id': user_id,
                'user_context': user_context,
                'available_ads_count': len(available_ads)
            }
        )
        
        # ìºì‹œì— ì €ì¥
        self.request_cache[request_id] = {
            'user_id': user_id,
            'timestamp': timestamp,
            'context': user_context,
            'status': 'pending'
        }
        
        return request_id
    
    def track_ad_selection(self, request_id: str, 
                         selected_ad: Dict,
                         selection_reason: str):
        """ê´‘ê³  ì„ íƒ ì¶”ì """
        if request_id not in self.request_cache:
            self.logger.warning(
                "Unknown request ID",
                extra={'request_id': request_id}
            )
            return
        
        self.logger.info(
            "Ad selected",
            extra={
                'request_id': request_id,
                'ad_id': selected_ad['id'],
                'selection_reason': selection_reason,
                'bid_price': selected_ad.get('bid_price')
            }
        )
        
        self.request_cache[request_id]['selected_ad'] = selected_ad['id']
        self.request_cache[request_id]['status'] = 'served'
    
    def track_ad_impression(self, request_id: str, ad_id: str):
        """ë…¸ì¶œ ì¶”ì """
        self.logger.info(
            "Ad impression",
            extra={
                'request_id': request_id,
                'ad_id': ad_id,
                'event_type': 'impression'
            }
        )
    
    def track_ad_click(self, request_id: str, ad_id: str, 
                      click_position: Dict):
        """í´ë¦­ ì¶”ì """
        self.logger.info(
            "Ad click",
            extra={
                'request_id': request_id,
                'ad_id': ad_id,
                'event_type': 'click',
                'click_x': click_position.get('x'),
                'click_y': click_position.get('y')
            }
        )
    
    def track_conversion(self, request_id: str, ad_id: str, 
                        conversion_value: float,
                        conversion_type: str):
        """ì „í™˜ ì¶”ì """
        self.logger.info(
            "Conversion tracked",
            extra={
                'request_id': request_id,
                'ad_id': ad_id,
                'event_type': 'conversion',
                'conversion_value': conversion_value,
                'conversion_type': conversion_type
            }
        )
    
    def get_request_journey(self, request_id: str) -> Dict:
        """ìš”ì²­ ì—¬ì • ì¡°íšŒ"""
        if request_id not in self.request_cache:
            return None
        
        journey = self.request_cache[request_id].copy()
        
        # ë¡œê·¸ì—ì„œ ê´€ë ¨ ì´ë²¤íŠ¸ ìˆ˜ì§‘ (ì‹¤ì œë¡œëŠ” ë¡œê·¸ ì €ì¥ì†Œì—ì„œ ì¡°íšŒ)
        journey['events'] = [
            {'type': 'request', 'timestamp': journey['timestamp']},
            # ì¶”ê°€ ì´ë²¤íŠ¸ë“¤...
        ]
        
        return journey
```

### ì‹¤ìŠµ 2: ì—ëŸ¬ ì¶”ì  ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
import logging
from logging.handlers import SMTPHandler, HTTPHandler
import requests
from typing import List, Dict
import json

class AdSystemErrorTracker:
    """ê´‘ê³  ì‹œìŠ¤í…œ ì—ëŸ¬ ì¶”ì ê¸°"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = self._setup_error_logging()
        self.error_stats = {}
    
    def _setup_error_logging(self) -> logging.Logger:
        """ì—ëŸ¬ ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger("ad_system_errors")
        logger.setLevel(logging.ERROR)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        error_handler = logging.FileHandler('logs/errors.log')
        error_handler.setLevel(logging.ERROR)
        
        # ì´ë©”ì¼ í•¸ë“¤ëŸ¬ (ì‹¬ê°í•œ ì—ëŸ¬)
        if self.config.get('email_alerts'):
            email_handler = SMTPHandler(
                mailhost=self.config['smtp_host'],
                fromaddr=self.config['from_email'],
                toaddrs=self.config['alert_emails'],
                subject='ê´‘ê³  ì‹œìŠ¤í…œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ'
            )
            email_handler.setLevel(logging.CRITICAL)
            logger.addHandler(email_handler)
        
        # Slack í•¸ë“¤ëŸ¬ (ì»¤ìŠ¤í…€)
        if self.config.get('slack_webhook'):
            slack_handler = SlackHandler(self.config['slack_webhook'])
            slack_handler.setLevel(logging.ERROR)
            logger.addHandler(slack_handler)
        
        # í¬ë§·í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s\n'
            'File: %(filename)s - Line: %(lineno)d - Function: %(funcName)s\n'
            '%(exc_info)s'
        )
        error_handler.setFormatter(formatter)
        
        logger.addHandler(error_handler)
        return logger
    
    def track_error(self, error_type: str, error_message: str, 
                   context: Dict = None, exception: Exception = None):
        """ì—ëŸ¬ ì¶”ì """
        # ì—ëŸ¬ í†µê³„ ì—…ë°ì´íŠ¸
        if error_type not in self.error_stats:
            self.error_stats[error_type] = {
                'count': 0,
                'first_seen': datetime.now(),
                'last_seen': None
            }
        
        self.error_stats[error_type]['count'] += 1
        self.error_stats[error_type]['last_seen'] = datetime.now()
        
        # ì—ëŸ¬ ë¡œê¹…
        error_data = {
            'error_type': error_type,
            'message': error_message,
            'context': context or {},
            'timestamp': datetime.now().isoformat()
        }
        
        if exception:
            error_data['exception'] = {
                'type': type(exception).__name__,
                'message': str(exception),
                'traceback': traceback.format_exc()
            }
            self.logger.error(
                f"{error_type}: {error_message}",
                exc_info=exception,
                extra=error_data
            )
        else:
            self.logger.error(
                f"{error_type}: {error_message}",
                extra=error_data
            )
        
        # ì—ëŸ¬ ë¹ˆë„ ì²´í¬
        self._check_error_frequency(error_type)
    
    def _check_error_frequency(self, error_type: str):
        """ì—ëŸ¬ ë¹ˆë„ ì²´í¬ ë° ì•Œë¦¼"""
        stats = self.error_stats[error_type]
        
        # 5ë¶„ ë‚´ 10íšŒ ì´ìƒ ë°œìƒ ì‹œ ì•Œë¦¼
        if stats['count'] >= 10:
            time_window = datetime.now() - stats['first_seen']
            if time_window.total_seconds() <= 300:  # 5ë¶„
                self.logger.critical(
                    f"ë†’ì€ ì—ëŸ¬ ë¹ˆë„ ê°ì§€: {error_type}",
                    extra={
                        'error_type': error_type,
                        'count': stats['count'],
                        'time_window_seconds': time_window.total_seconds()
                    }
                )
                
                # í†µê³„ ë¦¬ì…‹
                self.error_stats[error_type]['count'] = 0
                self.error_stats[error_type]['first_seen'] = datetime.now()

class SlackHandler(logging.Handler):
    """Slack ì•Œë¦¼ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, webhook_url: str):
        super().__init__()
        self.webhook_url = webhook_url
    
    def emit(self, record):
        try:
            # Slack ë©”ì‹œì§€ í¬ë§·
            message = {
                'text': f':warning: *{record.levelname}* in Ad System',
                'attachments': [{
                    'color': 'danger' if record.levelname == 'CRITICAL' else 'warning',
                    'fields': [
                        {
                            'title': 'Error',
                            'value': record.getMessage(),
                            'short': False
                        },
                        {
                            'title': 'Location',
                            'value': f'{record.filename}:{record.lineno}',
                            'short': True
                        },
                        {
                            'title': 'Function',
                            'value': record.funcName,
                            'short': True
                        },
                        {
                            'title': 'Time',
                            'value': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'short': True
                        }
                    ]
                }]
            }
            
            # Webhook ì „ì†¡
            requests.post(self.webhook_url, json=message)
            
        except Exception as e:
            self.handleError(record)
```

### ì‹¤ìŠµ 3: ë¡œê·¸ ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§

```python
import re
from collections import defaultdict, Counter
from datetime import datetime, timedelta
import pandas as pd

class AdLogAnalyzer:
    """ê´‘ê³  ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(self, log_path: str):
        self.log_path = log_path
        self.patterns = {
            'timestamp': r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            'level': r'"level":\s*"(\w+)"',
            'event_type': r'"event_type":\s*"(\w+)"',
            'ad_id': r'"ad_id":\s*"([^"]+)"',
            'user_id': r'"user_id":\s*"([^"]+)"',
            'error_type': r'"error_type":\s*"([^"]+)"'
        }
    
    def parse_log_line(self, line: str) -> Dict:
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        try:
            # JSON ë¡œê·¸ íŒŒì‹±
            if line.strip().startswith('{'):
                return json.loads(line)
            
            # ì¼ë°˜ ë¡œê·¸ íŒŒì‹±
            parsed = {}
            for key, pattern in self.patterns.items():
                match = re.search(pattern, line)
                if match:
                    parsed[key] = match.group(1)
            
            return parsed
            
        except Exception:
            return {}
    
    def analyze_logs(self, start_time: datetime, end_time: datetime) -> Dict:
        """ë¡œê·¸ ë¶„ì„"""
        results = {
            'summary': {},
            'events': defaultdict(int),
            'errors': defaultdict(int),
            'performance': {},
            'anomalies': []
        }
        
        with open(self.log_path, 'r') as f:
            for line in f:
                log_entry = self.parse_log_line(line)
                
                if not log_entry:
                    continue
                
                # ì‹œê°„ í•„í„°ë§
                if 'timestamp' in log_entry:
                    log_time = datetime.fromisoformat(
                        log_entry['timestamp'].replace('Z', '+00:00')
                    )
                    if not (start_time <= log_time <= end_time):
                        continue
                
                # ì´ë²¤íŠ¸ ì§‘ê³„
                if 'event_type' in log_entry:
                    results['events'][log_entry['event_type']] += 1
                
                # ì—ëŸ¬ ì§‘ê³„
                if log_entry.get('level') == 'ERROR':
                    error_type = log_entry.get('error_type', 'unknown')
                    results['errors'][error_type] += 1
                
                # ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
                if 'duration_ms' in log_entry:
                    operation = log_entry.get('operation', 'unknown')
                    if operation not in results['performance']:
                        results['performance'][operation] = []
                    results['performance'][operation].append(
                        float(log_entry['duration_ms'])
                    )
        
        # ìš”ì•½ í†µê³„ ê³„ì‚°
        results['summary'] = self._calculate_summary(results)
        
        # ì´ìƒ íƒì§€
        results['anomalies'] = self._detect_anomalies(results)
        
        return results
    
    def _calculate_summary(self, results: Dict) -> Dict:
        """ìš”ì•½ í†µê³„ ê³„ì‚°"""
        summary = {
            'total_events': sum(results['events'].values()),
            'total_errors': sum(results['errors'].values()),
            'event_breakdown': dict(results['events']),
            'error_breakdown': dict(results['errors']),
            'performance_summary': {}
        }
        
        # ì„±ëŠ¥ ìš”ì•½
        for operation, durations in results['performance'].items():
            if durations:
                summary['performance_summary'][operation] = {
                    'avg_ms': sum(durations) / len(durations),
                    'min_ms': min(durations),
                    'max_ms': max(durations),
                    'p95_ms': pd.Series(durations).quantile(0.95)
                }
        
        return summary
    
    def _detect_anomalies(self, results: Dict) -> List[Dict]:
        """ì´ìƒ íƒì§€"""
        anomalies = []
        
        # ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€
        total_events = sum(results['events'].values())
        total_errors = sum(results['errors'].values())
        
        if total_events > 0:
            error_rate = total_errors / total_events
            if error_rate > 0.05:  # 5% ì´ìƒ
                anomalies.append({
                    'type': 'high_error_rate',
                    'severity': 'high',
                    'value': error_rate,
                    'threshold': 0.05
                })
        
        # ëŠë¦° ì‘ë‹µ ê°ì§€
        for operation, durations in results['performance'].items():
            if durations:
                p95 = pd.Series(durations).quantile(0.95)
                if p95 > 1000:  # 1ì´ˆ ì´ìƒ
                    anomalies.append({
                        'type': 'slow_response',
                        'severity': 'medium',
                        'operation': operation,
                        'p95_ms': p95,
                        'threshold_ms': 1000
                    })
        
        return anomalies
    
    def generate_report(self, analysis_results: Dict) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 50)
        report.append("ê´‘ê³  ì‹œìŠ¤í…œ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 50)
        
        # ìš”ì•½
        summary = analysis_results['summary']
        report.append(f"\nì´ ì´ë²¤íŠ¸ ìˆ˜: {summary['total_events']:,}")
        report.append(f"ì´ ì—ëŸ¬ ìˆ˜: {summary['total_errors']:,}")
        
        # ì´ë²¤íŠ¸ ë¶„ì„
        report.append("\nì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„í¬:")
        for event_type, count in summary['event_breakdown'].items():
            report.append(f"  - {event_type}: {count:,}")
        
        # ì—ëŸ¬ ë¶„ì„
        if summary['error_breakdown']:
            report.append("\nì—ëŸ¬ íƒ€ì…ë³„ ë¶„í¬:")
            for error_type, count in summary['error_breakdown'].items():
                report.append(f"  - {error_type}: {count:,}")
        
        # ì„±ëŠ¥ ë¶„ì„
        if summary['performance_summary']:
            report.append("\nì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            for op, metrics in summary['performance_summary'].items():
                report.append(f"  {op}:")
                report.append(f"    - í‰ê· : {metrics['avg_ms']:.2f}ms")
                report.append(f"    - P95: {metrics['p95_ms']:.2f}ms")
        
        # ì´ìƒ ê°ì§€
        if analysis_results['anomalies']:
            report.append("\nâš ï¸  ê°ì§€ëœ ì´ìƒ:")
            for anomaly in analysis_results['anomalies']:
                report.append(f"  - {anomaly['type']} ({anomaly['severity']})")
        
        return '\n'.join(report)
```

## ğŸš€ í”„ë¡œì íŠ¸: í†µí•© ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
ad_logging_system/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ handlers.py
â”‚   â””â”€â”€ formatters.py
â”œâ”€â”€ collectors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ event_collector.py
â”‚   â”œâ”€â”€ metric_collector.py
â”‚   â””â”€â”€ error_collector.py
â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ real_time_analyzer.py
â”‚   â”œâ”€â”€ batch_analyzer.py
â”‚   â””â”€â”€ anomaly_detector.py
â”œâ”€â”€ exporters/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ elasticsearch_exporter.py
â”‚   â”œâ”€â”€ prometheus_exporter.py
â”‚   â””â”€â”€ s3_exporter.py
â””â”€â”€ dashboard/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ api.py
    â””â”€â”€ visualizations.py
```

### í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ êµ¬í˜„

```python
# core/logger.py
import logging
import json
from typing import Dict, Any, Optional, List
from datetime import datetime
import asyncio
from contextlib import asynccontextmanager
import aioredis

class AdLoggingSystem:
    """ê´‘ê³  ì‹œìŠ¤í…œ í†µí•© ë¡œê¹…"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.loggers = {}
        self.handlers = []
        self.redis_client = None
        self._setup_logging()
    
    def _setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        
        # í•¸ë“¤ëŸ¬ ì¶”ê°€
        self._add_console_handler()
        self._add_file_handler()
        self._add_structured_handler()
        
        if self.config.get('enable_remote_logging'):
            self._add_remote_handler()
    
    def _add_structured_handler(self):
        """êµ¬ì¡°í™”ëœ ë¡œê¹… í•¸ë“¤ëŸ¬"""
        from pythonjsonlogger import jsonlogger
        
        handler = logging.StreamHandler()
        formatter = jsonlogger.JsonFormatter(
            '%(timestamp)s %(level)s %(name)s %(message)s',
            timestamp=True
        )
        handler.setFormatter(formatter)
        
        self.handlers.append(handler)
        logging.getLogger().addHandler(handler)
    
    def get_logger(self, name: str, **default_fields) -> 'ContextualLogger':
        """ì»¨í…ìŠ¤íŠ¸ ë¡œê±° ìƒì„±"""
        if name not in self.loggers:
            self.loggers[name] = ContextualLogger(name, default_fields)
        return self.loggers[name]
    
    async def setup_redis(self):
        """Redis ì—°ê²° ì„¤ì •"""
        self.redis_client = await aioredis.create_redis_pool(
            self.config['redis_url']
        )
    
    async def log_event_async(self, event_type: str, data: Dict[str, Any]):
        """ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë¡œê¹…"""
        if not self.redis_client:
            await self.setup_redis()
        
        event = {
            'timestamp': datetime.utcnow().isoformat(),
            'type': event_type,
            'data': data
        }
        
        # Redisì— ì´ë²¤íŠ¸ ì €ì¥
        await self.redis_client.lpush(
            f"logs:{event_type}",
            json.dumps(event)
        )
        
        # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°œí–‰
        await self.redis_client.xadd(
            'log_stream',
            {'event': json.dumps(event)}
        )

class ContextualLogger:
    """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë¡œê±°"""
    
    def __init__(self, name: str, default_fields: Dict[str, Any] = None):
        self.logger = logging.getLogger(name)
        self.default_fields = default_fields or {}
        self.context_stack = []
    
    @asynccontextmanager
    async def context(self, **fields):
        """ì„ì‹œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
        self.context_stack.append(fields)
        try:
            yield self
        finally:
            self.context_stack.pop()
    
    def _merge_context(self, **fields) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ë³‘í•©"""
        merged = self.default_fields.copy()
        
        # ìŠ¤íƒì˜ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ ë³‘í•©
        for context in self.context_stack:
            merged.update(context)
        
        # í˜„ì¬ í•„ë“œ ë³‘í•©
        merged.update(fields)
        
        return merged
    
    def debug(self, message: str, **fields):
        context = self._merge_context(**fields)
        self.logger.debug(message, extra={'context': context})
    
    def info(self, message: str, **fields):
        context = self._merge_context(**fields)
        self.logger.info(message, extra={'context': context})
    
    def warning(self, message: str, **fields):
        context = self._merge_context(**fields)
        self.logger.warning(message, extra={'context': context})
    
    def error(self, message: str, exception: Exception = None, **fields):
        context = self._merge_context(**fields)
        
        if exception:
            context['exception'] = {
                'type': type(exception).__name__,
                'message': str(exception),
                'traceback': traceback.format_exc()
            }
        
        self.logger.error(message, extra={'context': context}, exc_info=exception)

# ê´‘ê³  ìº í˜ì¸ ë¡œê¹… ì˜ˆì‹œ
class CampaignLogger:
    """ìº í˜ì¸ ì „ìš© ë¡œê±°"""
    
    def __init__(self, logging_system: AdLoggingSystem):
        self.logging_system = logging_system
        self.logger = logging_system.get_logger(
            'campaign',
            service='campaign_manager'
        )
    
    async def log_campaign_created(self, campaign_id: str, 
                                  campaign_data: Dict[str, Any]):
        """ìº í˜ì¸ ìƒì„± ë¡œê¹…"""
        async with self.logger.context(campaign_id=campaign_id):
            self.logger.info(
                "Campaign created",
                action='create_campaign',
                campaign_name=campaign_data['name'],
                budget=campaign_data['budget'],
                start_date=campaign_data['start_date'],
                end_date=campaign_data['end_date']
            )
            
            # ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì—ë„ ì „ì†¡
            await self.logging_system.log_event_async(
                'campaign_created',
                {
                    'campaign_id': campaign_id,
                    **campaign_data
                }
            )
    
    async def log_campaign_performance(self, campaign_id: str,
                                     metrics: Dict[str, Any]):
        """ìº í˜ì¸ ì„±ê³¼ ë¡œê¹…"""
        async with self.logger.context(
            campaign_id=campaign_id,
            metric_type='performance'
        ):
            self.logger.info(
                "Campaign performance update",
                impressions=metrics['impressions'],
                clicks=metrics['clicks'],
                conversions=metrics['conversions'],
                spend=metrics['spend'],
                ctr=metrics['ctr'],
                roas=metrics['roas']
            )
            
            # ì„±ê³¼ê°€ ëª©í‘œì¹˜ ì´í•˜ì¸ ê²½ìš° ê²½ê³ 
            if metrics['roas'] < 1.0:
                self.logger.warning(
                    "Campaign underperforming",
                    roas=metrics['roas'],
                    target_roas=1.0
                )

# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
class LogMonitor:
    """ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, logging_system: AdLoggingSystem):
        self.logging_system = logging_system
        self.monitors = []
        self.alerts = []
    
    def add_monitor(self, monitor_func, name: str):
        """ëª¨ë‹ˆí„° ì¶”ê°€"""
        self.monitors.append({
            'name': name,
            'func': monitor_func,
            'active': True
        })
    
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        redis = await aioredis.create_redis_pool(
            self.logging_system.config['redis_url']
        )
        
        try:
            # ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ êµ¬ë…
            stream_key = 'log_stream'
            last_id = '0'
            
            while True:
                # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë¡œê·¸ ì½ê¸°
                messages = await redis.xread(
                    [stream_key],
                    latest_ids=[last_id],
                    count=100,
                    timeout=1000
                )
                
                for stream, stream_messages in messages:
                    for message_id, data in stream_messages:
                        last_id = message_id
                        
                        # ê° ëª¨ë‹ˆí„° ì‹¤í–‰
                        event = json.loads(data[b'event'])
                        await self._process_event(event)
                
        finally:
            redis.close()
            await redis.wait_closed()
    
    async def _process_event(self, event: Dict[str, Any]):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        for monitor in self.monitors:
            if monitor['active']:
                try:
                    alert = await monitor['func'](event)
                    if alert:
                        await self._handle_alert(alert)
                except Exception as e:
                    print(f"Monitor error: {monitor['name']} - {e}")
    
    async def _handle_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        self.alerts.append(alert)
        
        # ì•Œë¦¼ ì „ì†¡ (Slack, Email ë“±)
        if alert['severity'] == 'critical':
            await self._send_critical_alert(alert)
    
    async def _send_critical_alert(self, alert: Dict[str, Any]):
        """ì‹¬ê°í•œ ì•Œë¦¼ ì „ì†¡"""
        # Slack ì›¹í›… í˜¸ì¶œ ë“±
        pass

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    config = {
        'redis_url': 'redis://localhost',
        'enable_remote_logging': True
    }
    
    logging_system = AdLoggingSystem(config)
    await logging_system.setup_redis()
    
    # ìº í˜ì¸ ë¡œê±° ìƒì„±
    campaign_logger = CampaignLogger(logging_system)
    
    # ìº í˜ì¸ ìƒì„± ë¡œê¹…
    await campaign_logger.log_campaign_created(
        'camp_123',
        {
            'name': 'ì—¬ë¦„ ì„¸ì¼ ìº í˜ì¸',
            'budget': 1000000,
            'start_date': '2024-06-01',
            'end_date': '2024-08-31'
        }
    )
    
    # ì„±ê³¼ ë¡œê¹…
    await campaign_logger.log_campaign_performance(
        'camp_123',
        {
            'impressions': 100000,
            'clicks': 2500,
            'conversions': 125,
            'spend': 500000,
            'ctr': 0.025,
            'roas': 1.5
        }
    )
    
    # ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = LogMonitor(logging_system)
    
    # ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„° ì¶”ê°€
    async def error_rate_monitor(event):
        if event['type'] == 'error':
            # ì—ëŸ¬ìœ¨ ê³„ì‚° ë¡œì§
            return {
                'type': 'high_error_rate',
                'severity': 'warning',
                'message': 'Error rate exceeded threshold'
            }
        return None
    
    monitor.add_monitor(error_rate_monitor, 'error_rate')
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    await monitor.start_monitoring()

if __name__ == '__main__':
    asyncio.run(main())
```

## ğŸ“ ê³¼ì œ

### ê³¼ì œ 1: ë¶„ì‚° ë¡œê¹… ì‹œìŠ¤í…œ
ì—¬ëŸ¬ ì„œë²„ì—ì„œ ìˆ˜ì§‘ëœ ë¡œê·¸ë¥¼ ì¤‘ì•™í™”í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„:
- Fluentd/Logstash ì—°ë™
- ë¡œê·¸ ì§‘ê³„ ë° ê²€ìƒ‰
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ

### ê³¼ì œ 2: ë¡œê·¸ ê¸°ë°˜ ì´ìƒ íƒì§€
ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ë¡œê·¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ:
- ì •ìƒ íŒ¨í„´ í•™ìŠµ
- ì‹¤ì‹œê°„ ì´ìƒ ê°ì§€
- ìë™ ì•Œë¦¼

### ê³¼ì œ 3: ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ
ìƒì„¸í•œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° ë¶„ì„:
- í•¨ìˆ˜ë³„ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
- ë³‘ëª© ì§€ì  ìë™ ê°ì§€
- ìµœì í™” ì œì•ˆ

### ê³¼ì œ 4: ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¡œê¹…
ê·œì • ì¤€ìˆ˜ë¥¼ ìœ„í•œ ê°ì‚¬ ë¡œê¹… ì‹œìŠ¤í…œ:
- ê°œì¸ì •ë³´ ì ‘ê·¼ ë¡œê¹…
- ë¡œê·¸ ë¬´ê²°ì„± ë³´ì¥
- ë³´ê´€ ì •ì±… ìë™í™”

## ğŸ”— ì°¸ê³  ìë£Œ
- [Python Logging Documentation](https://docs.python.org/3/library/logging.html)
- [Structured Logging in Python](https://www.structlog.org/)
- [ELK Stack Tutorial](https://www.elastic.co/guide/index.html)
- [Distributed Tracing with OpenTelemetry](https://opentelemetry.io/docs/python/)

---

ë‹¤ìŒ ì¥: [Chapter 06: Error Handling â†’](../06_error_handling/README.md)