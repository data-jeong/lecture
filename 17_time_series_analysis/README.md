# 17. Time Series Analysis - ì‹œê³„ì—´ ë¶„ì„

## ğŸ“š ê³¼ì • ì†Œê°œ
ê´‘ê³  ì„±ê³¼ ë°ì´í„°ì˜ ì‹œê³„ì—´ ë¶„ì„ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. íŠ¸ë Œë“œ ë¶„ì„, ê³„ì ˆì„± íƒì§€, ì˜ˆì¸¡ ëª¨ë¸ë§ì„ í†µí•´ ê´‘ê³  ìº í˜ì¸ ìµœì í™”ì™€ ì˜ˆì‚° ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- ê´‘ê³  ì„±ê³¼ ì‹œê³„ì—´ ë¶„ì„
- ê³„ì ˆì„± ë° íŠ¸ë Œë“œ ë¶„í•´
- ARIMA, Prophet ì˜ˆì¸¡ ëª¨ë¸
- ì´ìƒ íƒì§€ ë° ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ

## ğŸ“– ì£¼ìš” ë‚´ìš©

### ê´‘ê³  ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ì‹œê³„ì—´ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller, acf, pacf
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Prophet (Facebook)
try:
    from prophet import Prophet
    from prophet.diagnostics import cross_validation, performance_metrics
    from prophet.plot import plot_cross_validation_metric
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("Prophet not available. Install with: pip install prophet")

# ë¨¸ì‹ ëŸ¬ë‹
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

from typing import Dict, List, Tuple, Optional
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class AdTimeSeriesAnalyzer:
    """ê´‘ê³  ì‹œê³„ì—´ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.data = None
        self.decomposition = None
        self.models = {}
        
    def load_ad_performance_data(self, file_path: str = None) -> pd.DataFrame:
        """ê´‘ê³  ì„±ê³¼ ë°ì´í„° ë¡œë“œ"""
        if file_path:
            self.data = pd.read_csv(file_path, parse_dates=['date'])
        else:
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            self.data = self._generate_sample_data()
            
        self.data.set_index('date', inplace=True)
        return self.data
    
    def _generate_sample_data(self) -> pd.DataFrame:
        """ìƒ˜í”Œ ê´‘ê³  ì„±ê³¼ ë°ì´í„° ìƒì„±"""
        # 2ë…„ê°„ì˜ ì¼ë³„ ë°ì´í„°
        dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')
        
        # ê¸°ë³¸ íŠ¸ë Œë“œ + ê³„ì ˆì„± + ë…¸ì´ì¦ˆ
        np.random.seed(42)
        n_days = len(dates)
        
        # íŠ¸ë Œë“œ (ì ì§„ì  ì¦ê°€)
        trend = np.linspace(1000, 1500, n_days)
        
        # ì£¼ê°„ ê³„ì ˆì„± (ì£¼ë§ íš¨ê³¼)
        weekly_pattern = 100 * np.sin(2 * np.pi * np.arange(n_days) / 7)
        
        # ì—°ê°„ ê³„ì ˆì„± (ì—°ë§ ì‡¼í•‘ ì‹œì¦Œ)
        yearly_pattern = 200 * np.sin(2 * np.pi * np.arange(n_days) / 365.25 - np.pi/2)
        
        # íŠ¹ë³„ ì´ë²¤íŠ¸ (Black Friday, Christmas ë“±)
        special_events = np.zeros(n_days)
        for i, date in enumerate(dates):
            if date.month == 11 and date.day >= 20:  # Black Friday ì‹œì¦Œ
                special_events[i] = 300
            elif date.month == 12 and date.day >= 15:  # Christmas ì‹œì¦Œ
                special_events[i] = 500
            elif date.month == 3 and date.day <= 14:  # í™”ì´íŠ¸ë°ì´
                special_events[i] = 150
        
        # ë…¸ì´ì¦ˆ
        noise = np.random.normal(0, 50, n_days)
        
        # ìµœì¢… ì„í”„ë ˆì…˜ ìˆ˜
        impressions = trend + weekly_pattern + yearly_pattern + special_events + noise
        impressions = np.maximum(impressions, 0)  # ìŒìˆ˜ ë°©ì§€
        
        # í´ë¦­ìˆ˜ (CTR ê¸°ë°˜)
        base_ctr = 0.02
        ctr_noise = np.random.normal(0, 0.005, n_days)
        ctr = np.maximum(base_ctr + ctr_noise, 0.001)
        clicks = impressions * ctr
        
        # ë¹„ìš© (CPC ê¸°ë°˜)
        base_cpc = 0.5
        cpc_noise = np.random.normal(0, 0.1, n_days)
        cpc = np.maximum(base_cpc + cpc_noise, 0.1)
        cost = clicks * cpc
        
        # ì „í™˜ìˆ˜ (CVR ê¸°ë°˜)
        base_cvr = 0.05
        cvr_noise = np.random.normal(0, 0.01, n_days)
        cvr = np.maximum(base_cvr + cvr_noise, 0.001)
        conversions = clicks * cvr
        
        # ë§¤ì¶œ (í‰ê·  ì£¼ë¬¸ê°€ê²© ê¸°ë°˜)
        avg_order_value = 50
        aov_noise = np.random.normal(0, 10, n_days)
        aov = np.maximum(avg_order_value + aov_noise, 10)
        revenue = conversions * aov
        
        return pd.DataFrame({
            'date': dates,
            'impressions': impressions.astype(int),
            'clicks': clicks.astype(int),
            'cost': np.round(cost, 2),
            'conversions': conversions.astype(int),
            'revenue': np.round(revenue, 2),
            'ctr': np.round(clicks / impressions, 4),
            'cpc': np.round(cost / clicks, 2),
            'cvr': np.round(conversions / clicks, 4),
            'roas': np.round(revenue / cost, 2)
        })
    
    def decompose_time_series(self, column: str = 'impressions', 
                            model: str = 'additive') -> Dict:
        """ì‹œê³„ì—´ ë¶„í•´"""
        if self.data is None:
            raise ValueError("ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.")
        
        # ì‹œê³„ì—´ ë¶„í•´
        self.decomposition = seasonal_decompose(
            self.data[column], 
            model=model, 
            period=7  # ì£¼ê°„ ê³„ì ˆì„±
        )
        
        # ë¶„í•´ ê²°ê³¼ ì‹œê°í™”
        fig, axes = plt.subplots(4, 1, figsize=(15, 12))
        
        self.decomposition.observed.plot(ax=axes[0], title='Original')
        self.decomposition.trend.plot(ax=axes[1], title='Trend')
        self.decomposition.seasonal.plot(ax=axes[2], title='Seasonal')
        self.decomposition.resid.plot(ax=axes[3], title='Residual')
        
        plt.tight_layout()
        plt.show()
        
        # ê³„ì ˆì„± ê°•ë„ ê³„ì‚°
        seasonal_strength = self._calculate_seasonal_strength(column)
        trend_strength = self._calculate_trend_strength(column)
        
        return {
            'seasonal_strength': seasonal_strength,
            'trend_strength': trend_strength,
            'decomposition': self.decomposition
        }
    
    def _calculate_seasonal_strength(self, column: str) -> float:
        """ê³„ì ˆì„± ê°•ë„ ê³„ì‚°"""
        if self.decomposition is None:
            return 0.0
        
        var_residual = np.var(self.decomposition.resid.dropna())
        var_seasonal_residual = np.var(
            (self.decomposition.seasonal + self.decomposition.resid).dropna()
        )
        
        if var_seasonal_residual == 0:
            return 0.0
        
        return max(0, 1 - var_residual / var_seasonal_residual)
    
    def _calculate_trend_strength(self, column: str) -> float:
        """íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°"""
        if self.decomposition is None:
            return 0.0
        
        var_residual = np.var(self.decomposition.resid.dropna())
        var_trend_residual = np.var(
            (self.decomposition.trend + self.decomposition.resid).dropna()
        )
        
        if var_trend_residual == 0:
            return 0.0
        
        return max(0, 1 - var_residual / var_trend_residual)
    
    def stationarity_test(self, column: str = 'impressions') -> Dict:
        """ì •ìƒì„± ê²€ì •"""
        series = self.data[column].dropna()
        
        # Augmented Dickey-Fuller Test
        adf_result = adfuller(series)
        
        result = {
            'adf_statistic': adf_result[0],
            'p_value': adf_result[1],
            'critical_values': adf_result[4],
            'is_stationary': adf_result[1] < 0.05
        }
        
        print(f"ADF Test Results for {column}:")
        print(f"ADF Statistic: {result['adf_statistic']:.6f}")
        print(f"p-value: {result['p_value']:.6f}")
        print(f"Is Stationary: {result['is_stationary']}")
        
        for key, value in result['critical_values'].items():
            print(f"Critical Value ({key}): {value:.3f}")
        
        return result
    
    def make_stationary(self, column: str = 'impressions') -> pd.Series:
        """ì‹œê³„ì—´ ì •ìƒí™”"""
        series = self.data[column]
        
        # 1ì°¨ ì°¨ë¶„
        diff_series = series.diff().dropna()
        
        # ì •ìƒì„± í™•ì¸
        adf_result = adfuller(diff_series)
        
        if adf_result[1] < 0.05:
            print("1ì°¨ ì°¨ë¶„ìœ¼ë¡œ ì •ìƒì„± ë‹¬ì„±")
            return diff_series
        else:
            # 2ì°¨ ì°¨ë¶„
            diff2_series = diff_series.diff().dropna()
            print("2ì°¨ ì°¨ë¶„ ì ìš©")
            return diff2_series
    
    def find_arima_parameters(self, column: str = 'impressions', 
                            max_p: int = 5, max_q: int = 5) -> Tuple[int, int, int]:
        """ARIMA íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰"""
        series = self.data[column].dropna()
        
        best_aic = float('inf')
        best_params = (0, 0, 0)
        
        # Grid Search
        for p in range(max_p + 1):
            for d in range(2):  # ë³´í†µ 0, 1ì°¨ ì°¨ë¶„
                for q in range(max_q + 1):
                    try:
                        model = ARIMA(series, order=(p, d, q))
                        fitted_model = model.fit()
                        
                        if fitted_model.aic < best_aic:
                            best_aic = fitted_model.aic
                            best_params = (p, d, q)
                            
                    except Exception:
                        continue
        
        print(f"Best ARIMA parameters: {best_params} (AIC: {best_aic:.2f})")
        return best_params
    
    def fit_arima_model(self, column: str = 'impressions', 
                       order: Tuple[int, int, int] = None) -> Dict:
        """ARIMA ëª¨ë¸ í•™ìŠµ"""
        series = self.data[column].dropna()
        
        if order is None:
            order = self.find_arima_parameters(column)
        
        # ëª¨ë¸ í•™ìŠµ
        model = ARIMA(series, order=order)
        fitted_model = model.fit()
        
        # ëª¨ë¸ ì €ì¥
        self.models[f'arima_{column}'] = fitted_model
        
        # ëª¨ë¸ ì§„ë‹¨
        residuals = fitted_model.resid
        
        # ì§„ë‹¨ í”Œë¡¯
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ì”ì°¨ í”Œë¡¯
        residuals.plot(ax=axes[0, 0], title='Residuals')
        
        # Q-Q í”Œë¡¯
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Q-Q Plot')
        
        # ACF/PACF
        plot_acf(residuals, ax=axes[1, 0], lags=20)
        plot_pacf(residuals, ax=axes[1, 1], lags=20)
        
        plt.tight_layout()
        plt.show()
        
        return {
            'model': fitted_model,
            'aic': fitted_model.aic,
            'bic': fitted_model.bic,
            'order': order,
            'summary': fitted_model.summary()
        }
    
    def forecast_arima(self, column: str = 'impressions', 
                      steps: int = 30) -> Dict:
        """ARIMA ì˜ˆì¸¡"""
        model_key = f'arima_{column}'
        if model_key not in self.models:
            print(f"{column}ì— ëŒ€í•œ ARIMA ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
            return {}
        
        model = self.models[model_key]
        
        # ì˜ˆì¸¡
        forecast = model.forecast(steps=steps)
        conf_int = model.get_forecast(steps=steps).conf_int()
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
        plt.figure(figsize=(15, 6))
        
        # ì›ë³¸ ë°ì´í„° (ë§ˆì§€ë§‰ 100ì¼)
        original_data = self.data[column].tail(100)
        plt.plot(original_data.index, original_data.values, 
                label='Observed', color='blue')
        
        # ì˜ˆì¸¡ ë°ì´í„°
        future_dates = pd.date_range(
            start=self.data.index[-1] + timedelta(days=1), 
            periods=steps, 
            freq='D'
        )
        
        plt.plot(future_dates, forecast, label='Forecast', 
                color='red', linestyle='--')
        
        # ì‹ ë¢°êµ¬ê°„
        plt.fill_between(future_dates, 
                        conf_int.iloc[:, 0], 
                        conf_int.iloc[:, 1], 
                        color='red', alpha=0.3)
        
        plt.title(f'{column.capitalize()} ARIMA Forecast')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        return {
            'forecast': forecast,
            'confidence_interval': conf_int,
            'future_dates': future_dates
        }

class ProphetAnalyzer:
    """Prophetì„ í™œìš©í•œ ì‹œê³„ì—´ ë¶„ì„"""
    
    def __init__(self):
        if not PROPHET_AVAILABLE:
            raise ImportError("Prophetì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.models = {}
        
    def prepare_prophet_data(self, data: pd.DataFrame, column: str) -> pd.DataFrame:
        """Prophetìš© ë°ì´í„° ì¤€ë¹„"""
        prophet_data = data.reset_index()
        prophet_data = prophet_data[['date', column]].rename(
            columns={'date': 'ds', column: 'y'}
        )
        return prophet_data
    
    def add_holiday_effects(self, data: pd.DataFrame) -> pd.DataFrame:
        """í•œêµ­ ì‡¼í•‘ ì‹œì¦Œ íœ´ì¼ íš¨ê³¼ ì¶”ê°€"""
        holidays = pd.DataFrame({
            'holiday': 'shopping_season',
            'ds': pd.to_datetime([
                '2022-11-11', '2022-11-25', '2022-12-25',  # 2022ë…„
                '2023-03-14', '2023-11-11', '2023-11-24', '2023-12-25',  # 2023ë…„
                '2024-03-14', '2024-11-11', '2024-11-29', '2024-12-25'   # 2024ë…„
            ]),
            'lower_window': -3,
            'upper_window': 3,
        })
        
        return holidays
    
    def fit_prophet_model(self, data: pd.DataFrame, column: str, 
                         include_holidays: bool = True) -> Dict:
        """Prophet ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„° ì¤€ë¹„
        prophet_data = self.prepare_prophet_data(data, column)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model_params = {
            'daily_seasonality': True,
            'weekly_seasonality': True,
            'yearly_seasonality': True,
            'changepoint_prior_scale': 0.05,
            'seasonality_prior_scale': 10.0
        }
        
        if include_holidays:
            holidays = self.add_holiday_effects(prophet_data)
            model_params['holidays'] = holidays
        
        model = Prophet(**model_params)
        
        # ì»¤ìŠ¤í…€ ê³„ì ˆì„± ì¶”ê°€ (ì›”ë³„)
        model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(prophet_data)
        
        # ëª¨ë¸ ì €ì¥
        self.models[f'prophet_{column}'] = {
            'model': model,
            'data': prophet_data
        }
        
        return {
            'model': model,
            'training_data': prophet_data
        }
    
    def forecast_prophet(self, column: str, periods: int = 30) -> Dict:
        """Prophet ì˜ˆì¸¡"""
        model_info = self.models.get(f'prophet_{column}')
        if not model_info:
            raise ValueError(f"{column}ì— ëŒ€í•œ Prophet ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        model = model_info['model']
        
        # ë¯¸ë˜ ë‚ ì§œ ìƒì„±
        future = model.make_future_dataframe(periods=periods)
        
        # ì˜ˆì¸¡
        forecast = model.predict(future)
        
        # ì‹œê°í™”
        fig1 = model.plot(forecast, figsize=(15, 6))
        plt.title(f'{column.capitalize()} Prophet Forecast')
        plt.show()
        
        # êµ¬ì„±ìš”ì†Œ ë¶„í•´
        fig2 = model.plot_components(forecast, figsize=(15, 10))
        plt.show()
        
        return {
            'forecast': forecast,
            'future_dates': future,
            'model': model
        }
    
    def cross_validate_prophet(self, column: str, 
                             initial: str = '365 days',
                             period: str = '30 days', 
                             horizon: str = '30 days') -> Dict:
        """Prophet êµì°¨ ê²€ì¦"""
        model_info = self.models.get(f'prophet_{column}')
        if not model_info:
            raise ValueError(f"{column}ì— ëŒ€í•œ Prophet ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        model = model_info['model']
        
        # êµì°¨ ê²€ì¦
        df_cv = cross_validation(
            model, 
            initial=initial, 
            period=period, 
            horizon=horizon
        )
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        df_p = performance_metrics(df_cv)
        
        # ì„±ëŠ¥ ì‹œê°í™”
        fig = plot_cross_validation_metric(df_cv, metric='mape')
        plt.show()
        
        return {
            'cross_validation': df_cv,
            'performance_metrics': df_p
        }

class AnomalyDetector:
    """ì‹œê³„ì—´ ì´ìƒ íƒì§€"""
    
    def __init__(self):
        self.detectors = {}
        
    def statistical_anomaly_detection(self, data: pd.Series, 
                                   window: int = 30, 
                                   threshold: float = 3.0) -> Dict:
        """í†µê³„ì  ì´ìƒ íƒì§€"""
        # ì´ë™ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
        rolling_mean = data.rolling(window=window).mean()
        rolling_std = data.rolling(window=window).std()
        
        # Z-score ê³„ì‚°
        z_scores = np.abs((data - rolling_mean) / rolling_std)
        
        # ì´ìƒì¹˜ íƒì§€
        anomalies = z_scores > threshold
        anomaly_dates = data[anomalies].index
        anomaly_values = data[anomalies].values
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 6))
        plt.plot(data.index, data.values, label='Original Data', alpha=0.7)
        plt.plot(rolling_mean.index, rolling_mean.values, label='Rolling Mean', color='green')
        
        # ì‹ ë¢°êµ¬ê°„
        plt.fill_between(data.index, 
                        rolling_mean - threshold * rolling_std,
                        rolling_mean + threshold * rolling_std,
                        alpha=0.3, color='green', label='Confidence Interval')
        
        # ì´ìƒì¹˜ í‘œì‹œ
        plt.scatter(anomaly_dates, anomaly_values, 
                   color='red', s=50, label='Anomalies', zorder=5)
        
        plt.title('Statistical Anomaly Detection')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        return {
            'anomalies': anomalies,
            'anomaly_dates': anomaly_dates,
            'anomaly_values': anomaly_values,
            'z_scores': z_scores
        }
    
    def isolation_forest_detection(self, data: pd.DataFrame, 
                                 columns: List[str],
                                 contamination: float = 0.1) -> Dict:
        """Isolation Forestë¥¼ ì´ìš©í•œ ì´ìƒ íƒì§€"""
        # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
        feature_data = data[columns].fillna(method='ffill')
        
        # í‘œì¤€í™”
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(feature_data)
        
        # Isolation Forest
        iso_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        
        # ì´ìƒì¹˜ ì˜ˆì¸¡ (-1: ì´ìƒì¹˜, 1: ì •ìƒ)
        anomaly_labels = iso_forest.fit_predict(scaled_data)
        anomaly_scores = iso_forest.decision_function(scaled_data)
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
        results_df = data[columns].copy()
        results_df['anomaly'] = anomaly_labels
        results_df['anomaly_score'] = anomaly_scores
        
        # ì´ìƒì¹˜ë§Œ ì¶”ì¶œ
        anomalies = results_df[results_df['anomaly'] == -1]
        
        # ì‹œê°í™” (2D ì˜ˆì‹œ - ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì»¬ëŸ¼)
        if len(columns) >= 2:
            plt.figure(figsize=(12, 8))
            
            normal_points = results_df[results_df['anomaly'] == 1]
            anomaly_points = results_df[results_df['anomaly'] == -1]
            
            plt.scatter(normal_points[columns[0]], normal_points[columns[1]], 
                       c='blue', alpha=0.6, label='Normal')
            plt.scatter(anomaly_points[columns[0]], anomaly_points[columns[1]], 
                       c='red', alpha=0.8, label='Anomaly')
            
            plt.xlabel(columns[0])
            plt.ylabel(columns[1])
            plt.title('Isolation Forest Anomaly Detection')
            plt.legend()
            plt.show()
        
        return {
            'anomalies': anomalies,
            'anomaly_scores': anomaly_scores,
            'model': iso_forest,
            'scaler': scaler
        }
    
    def prophet_anomaly_detection(self, data: pd.DataFrame, column: str,
                                uncertainty_interval: float = 0.95) -> Dict:
        """Prophetì„ ì´ìš©í•œ ì´ìƒ íƒì§€"""
        # Prophet ëª¨ë¸ í•™ìŠµ
        prophet_analyzer = ProphetAnalyzer()
        model_result = prophet_analyzer.fit_prophet_model(data, column)
        model = model_result['model']
        
        # ì˜ˆì¸¡ (ê¸°ì¡´ ë°ì´í„°ì— ëŒ€í•´)
        future = model.make_future_dataframe(periods=0)
        forecast = model.predict(future)
        
        # ì‹ ë¢°êµ¬ê°„ ë°–ì˜ ë°ì´í„°ë¥¼ ì´ìƒì¹˜ë¡œ íŒë‹¨
        lower_bound = forecast['yhat_lower']
        upper_bound = forecast['yhat_upper']
        actual_values = data[column].values
        
        # ì´ìƒì¹˜ íƒì§€
        anomalies = (actual_values < lower_bound) | (actual_values > upper_bound)
        anomaly_dates = data.index[anomalies]
        anomaly_values = actual_values[anomalies]
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 6))
        plt.plot(data.index, actual_values, label='Actual', color='blue')
        plt.plot(forecast['ds'], forecast['yhat'], label='Predicted', color='orange')
        plt.fill_between(forecast['ds'], lower_bound, upper_bound, 
                        alpha=0.3, color='orange', label='Confidence Interval')
        plt.scatter(anomaly_dates, anomaly_values, 
                   color='red', s=50, label='Anomalies', zorder=5)
        
        plt.title(f'Prophet-based Anomaly Detection for {column}')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
        return {
            'anomalies': anomalies,
            'anomaly_dates': anomaly_dates,
            'anomaly_values': anomaly_values,
            'forecast': forecast,
            'model': model
        }

class CampaignPerformancePredictor:
    """ìº í˜ì¸ ì„±ê³¼ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self):
        self.models = {}
        
    def predict_campaign_metrics(self, historical_data: pd.DataFrame,
                               prediction_days: int = 30) -> Dict:
        """ìº í˜ì¸ ì§€í‘œ ì˜ˆì¸¡"""
        results = {}
        
        # ê° ì§€í‘œë³„ ì˜ˆì¸¡
        metrics = ['impressions', 'clicks', 'cost', 'conversions', 'revenue']
        
        for metric in metrics:
            if metric in historical_data.columns:
                # ARIMA ì˜ˆì¸¡
                arima_analyzer = AdTimeSeriesAnalyzer()
                arima_analyzer.data = historical_data
                
                try:
                    arima_result = arima_analyzer.fit_arima_model(metric)
                    forecast_result = arima_analyzer.forecast_arima(metric, prediction_days)
                    
                    results[f'{metric}_arima'] = {
                        'forecast': forecast_result['forecast'],
                        'confidence_interval': forecast_result['confidence_interval'],
                        'dates': forecast_result['future_dates']
                    }
                except Exception as e:
                    print(f"ARIMA ì˜ˆì¸¡ ì‹¤íŒ¨ for {metric}: {e}")
                
                # Prophet ì˜ˆì¸¡ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                if PROPHET_AVAILABLE:
                    try:
                        prophet_analyzer = ProphetAnalyzer()
                        prophet_result = prophet_analyzer.fit_prophet_model(historical_data, metric)
                        forecast_result = prophet_analyzer.forecast_prophet(metric, prediction_days)
                        
                        results[f'{metric}_prophet'] = {
                            'forecast': forecast_result['forecast'],
                            'model': forecast_result['model']
                        }
                    except Exception as e:
                        print(f"Prophet ì˜ˆì¸¡ ì‹¤íŒ¨ for {metric}: {e}")
        
        return results
    
    def calculate_prediction_accuracy(self, actual: pd.Series, 
                                    predicted: pd.Series) -> Dict:
        """ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°"""
        mae = mean_absolute_error(actual, predicted)
        mse = mean_squared_error(actual, predicted)
        rmse = np.sqrt(mse)
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((actual - predicted) / actual)) * 100
        
        return {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape
        }

# ì‚¬ìš© ì˜ˆì‹œ
def example_time_series_analysis():
    """ì‹œê³„ì—´ ë¶„ì„ ì˜ˆì‹œ"""
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = AdTimeSeriesAnalyzer()
    
    # ë°ì´í„° ë¡œë“œ
    print("ìƒ˜í”Œ ê´‘ê³  ë°ì´í„° ìƒì„± ì¤‘...")
    data = analyzer.load_ad_performance_data()
    print(f"ë°ì´í„° í˜•íƒœ: {data.shape}")
    print(data.head())
    
    # ì‹œê³„ì—´ ë¶„í•´
    print("\nì‹œê³„ì—´ ë¶„í•´ ì¤‘...")
    decomp_result = analyzer.decompose_time_series('impressions')
    print(f"ê³„ì ˆì„± ê°•ë„: {decomp_result['seasonal_strength']:.3f}")
    print(f"íŠ¸ë Œë“œ ê°•ë„: {decomp_result['trend_strength']:.3f}")
    
    # ì •ìƒì„± ê²€ì •
    print("\nì •ìƒì„± ê²€ì • ì¤‘...")
    stationarity_result = analyzer.stationarity_test('impressions')
    
    # ARIMA ëª¨ë¸
    print("\nARIMA ëª¨ë¸ í•™ìŠµ ì¤‘...")
    arima_result = analyzer.fit_arima_model('impressions')
    print(f"AIC: {arima_result['aic']:.2f}")
    
    # ARIMA ì˜ˆì¸¡
    print("\nARIMA ì˜ˆì¸¡ ì¤‘...")
    forecast_result = analyzer.forecast_arima('impressions', 30)
    
    # Prophet ë¶„ì„ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if PROPHET_AVAILABLE:
        print("\nProphet ëª¨ë¸ í•™ìŠµ ì¤‘...")
        prophet_analyzer = ProphetAnalyzer()
        prophet_result = prophet_analyzer.fit_prophet_model(data, 'impressions')
        
        print("\nProphet ì˜ˆì¸¡ ì¤‘...")
        prophet_forecast = prophet_analyzer.forecast_prophet('impressions', 30)
    
    # ì´ìƒ íƒì§€
    print("\nì´ìƒ íƒì§€ ì¤‘...")
    anomaly_detector = AnomalyDetector()
    
    # í†µê³„ì  ì´ìƒ íƒì§€
    stat_anomalies = anomaly_detector.statistical_anomaly_detection(
        data['impressions'], window=30, threshold=2.5
    )
    print(f"ë°œê²¬ëœ ì´ìƒì¹˜ ìˆ˜: {len(stat_anomalies['anomaly_dates'])}")
    
    # Isolation Forest ì´ìƒ íƒì§€
    iso_anomalies = anomaly_detector.isolation_forest_detection(
        data, ['impressions', 'clicks', 'cost'], contamination=0.05
    )
    print(f"Isolation Forest ì´ìƒì¹˜ ìˆ˜: {len(iso_anomalies['anomalies'])}")
    
    return {
        'data_shape': data.shape,
        'seasonal_strength': decomp_result['seasonal_strength'],
        'arima_aic': arima_result['aic'],
        'anomalies_count': len(stat_anomalies['anomaly_dates'])
    }

if __name__ == "__main__":
    results = example_time_series_analysis()
    print("ì‹œê³„ì—´ ë¶„ì„ ì™„ë£Œ!")
```

## ğŸš€ í”„ë¡œì íŠ¸
1. **ê´‘ê³  ì„±ê³¼ ì˜ˆì¸¡ ì‹œìŠ¤í…œ**
2. **ê³„ì ˆì„± ê¸°ë°˜ ì˜ˆì‚° ê³„íš**
3. **ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ì•Œë¦¼**
4. **ìº í˜ì¸ ìµœì í™” ëŒ€ì‹œë³´ë“œ**